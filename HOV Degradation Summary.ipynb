{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "final-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-diagnosis",
   "metadata": {},
   "source": [
    "# Tip for quick search\n",
    "\n",
    "* Needs attention: the place where needs update or better logic\n",
    "* question to be answered: the place where things are still not clear\n",
    "* Unit Test: Unit test where you can drill in to find the data that leads to the check results for a specific project and specific check\n",
    "* TODO: things needs to be done\n",
    "* bookmark: stop point from last visit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-hawaii",
   "metadata": {},
   "source": [
    "# Update Note: \n",
    "\n",
    "11-2-2021: group activities in internal project book output file  <br>\n",
    "12-15-2021: \n",
    "* bring back the live data reading option\n",
    "* remove duplicated project ID in the internal project book\n",
    "12-17-2021:\n",
    "* download all SHOPP/Minor/HM worksheets. Filter the dataframe before data checks\n",
    "* remove defaul na option when downloading data\n",
    "\n",
    "12-20-2021:\n",
    "* import projectbookcheck_utilityfunction as uf \n",
    "* skip safety checks for TYP=9999\n",
    "* add Data_HourMinute, change Data_TimeStamp to Data_Date\n",
    "* use TenYrShopp_RawData_ file create time as Data_HourMinute\n",
    "\n",
    "12-23-2021\n",
    "* delete Data_HourMinute, Data_Date , change to Data_TimeStamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-excess",
   "metadata": {},
   "source": [
    "# Admin Notes:\n",
    "\n",
    "\n",
    "1. The AMTool dataset is archived daily as csv files and used for the project book check. \n",
    "The csv files are located at: \n",
    "r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Tableau Dashboards\\DataLake'\n",
    "\n",
    "2. The excel input files are checked daily and archived with datestamp whenever it is modified.\n",
    "The continuously updated excel input files are located at: r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_WorkingFolder\\excel'\n",
    "The excel input file are archived at: r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Tableau Dashboards\\Data_MiscInput'\n",
    "To recover the archived excel file used in project book check for a target date, select the excel file with latest datestamp but is still earlier than the target date.\n",
    "\n",
    "3. The check summary export action is logged daily. It can be used for daily monitoring. \n",
    "The file export log is located at: \\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_WorkingFolder\\output_internal\\log\n",
    "\n",
    "4. The published data are at:\n",
    "\n",
    "    * csv files for district asset manager: http://svgcshopp.dot.ca.gov/DataLake/ProjectBookCheck/\n",
    "    * csv files for HQ AM: \\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_WorkingFolder\\output_internal\n",
    "    * tableau workbook with live data source: https://tableau.dot.ca.gov/#/site/AssetManagement/workbooks/1815/views\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-confidentiality",
   "metadata": {},
   "source": [
    "<a id='TableOfContents'></a>\n",
    "\n",
    "# Table Of Contents\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "### [Global Constants](#GlobalConstants)\n",
    "\n",
    "\n",
    "### [Load and cleanup source data](#Read_Data)\n",
    "\n",
    "* [Bridge_Inventory](#Bridge_Inventory)\n",
    "* [Bridge_Worksheet](#Bridge_Worksheet)\n",
    "* [Check_Exceptions](#Check_Exceptions)\n",
    "* [Counties](#Counties)\n",
    "* [Drainage_Worksheet](#Drainage_Worksheet)\n",
    "* [Minor_Raw_Data](#Minor_Raw_Data)\n",
    "* [Pavement_Worksheet](#Pavement_Worksheet)\n",
    "* [PID_Workload](#PID_Workload)\n",
    "* [Postmile_Check](#Postmile_Check)\n",
    "* [Programming_Summary](#Programming_Summary)\n",
    "* [ProgrammingList](#ProgrammingList)\n",
    "* [Project_Detail_Report](#Project_Detail_Report)\n",
    "* [Project_Obselete](#Project_Obselete)\n",
    "* [SHOPP_Candidates](#SHOPP_Candidates)\n",
    "* [SHOPP_Raw_Data](#SHOPP_Raw_Data)\n",
    "* [TenYrShopp_Perf_RawData](#TenYrShopp_Perf_RawData)\n",
    "* [TMS_Worksheet](#TMS_Worksheet)\n",
    "\n",
    "\n",
    "## Add fields to SHOPP raw data (calculate and join)\n",
    "* [Calculated Fields](#AddDataColumns)\n",
    "* [Join Tables](#DataJoining)\n",
    "\n",
    "\n",
    "\n",
    "## Data Check and Export\n",
    "\n",
    "\n",
    "## [Data Check List](#Issue_Table1)\n",
    "The main table of check issues, \n",
    "one issue per row, \n",
    "\n",
    "\n",
    "* [Will_this_project_be_included_in_the_Project_Book](#Will_this_project_be_included_in_the_Project_Book)\n",
    "* [Does_project_cost_exceed_Minor_Program_limits](#Does_project_cost_exceed_Minor_Program_limits)\n",
    "* [Is_Major_Damage_or_Mobility_Subcategory_Identified (Obselete)](#Is_Major_Damage_or_Mobility_Subcategory_Identified)\n",
    "* [Is_Planned_Project_RTL_in_a_FY_that_can_be_programmed_in_future_SHOPP_cycles](#Is_Planned_Project_RTL_in_a_FY_that_can_be_programmed_in_future_SHOPP_cycles)\n",
    "* [Is_the_PID_cycle_consistent_with_the_project_status_and_RTL](#Is_the_PID_cycle_consistent_with_the_project_status_and_RTL)\n",
    "* [Is_PIP_uploaded_(Active_and_Complete_PIDs)](#Is_PIP_uploaded_(Active_and_Complete_PIDs))\n",
    "* [Is_District_Director_Approval_Date_in_the_Future](#Is_District_Director_Approval_Date_in_the_Future)\n",
    "* [Is_the_EA_or_Project_ID_repeated_in_the_AM_tool](#Is_the_EA_or_Project_ID_repeated_in_the_AM_tool)\n",
    "* [Does_project_include_performance_related_to_each_location](#Does_project_include_performance_related_to_each_location)\n",
    "* [Is_Performance_tab_Complete](#Is_Performance_tab_Complete)\n",
    "* [Is_at_least_one_performance_activities_related_to_the_Activity_Category_of_planned_project](#Is_at_least_one_performance_activities_related_to_the_Activity_Category_of_planned_project)\n",
    "* [Is_Long_Lead_Project_Cost_and_RTL_completed_and_consistent](#Is_Long_Lead_Project_Cost_and_RTL_completed_and_consistent)\n",
    "* [Are_all_Project_Locations_(PM)_Valid](#Are_all_Project_Locations_(PM)_Valid)\n",
    "* [Is_Drainage_Worksheet_Complete](#Is_Drainage_Worksheet_Complete)\n",
    "* [Are_all_conditions_selected_for_bridge_replacements](#Are_all_conditions_selected_for_bridge_replacements)\n",
    "* [Does_Bridge_Worksheet_need_updates](#Does_Bridge_Worksheet_need_updates)\n",
    "* [Does_the_Plan_Year_in_the_Pavement_Worksheet_match_the_Project_RTL](#Does_the_Plan_Year_in_the_Pavement_Worksheet_match_the_Project_RTL)\n",
    "* [Is_Pavement_Worksheet_Complete](#Is_Pavement_Worksheet_Complete)\n",
    "* [Is_the_Pavement_Work_Limits_Direction_in_the_Pavement_Worksheet_complete (Obselete)](#Is_the_Pavement_Work_Limits_Direction_in_the_Pavement_Worksheet_complete)\n",
    "* [Is_TMS_Worksheet_Complete](#Is_TMS_Worksheet_Complete)\n",
    "* [Does_the_RTL_Plan_Year_in_the_TMS_Worksheet_match_the_Project_RTL](#Does_the_RTL_Plan_Year_in_the_TMS_Worksheet_match_the_Project_RTL)\n",
    "* [Do_SHOPP_project_data_in_the_AM_Tool_match_CTIPS_data](#Do_SHOPP_project_data_in_the_AM_Tool_match_CTIPS_data)\n",
    "* [Is_PID_completed_and_uploaded_for_current_SHOPP_Candidates](#Is_PID_completed_and_uploaded_for_current_SHOPP_Candidates)\n",
    "* [Is_CCE_uploaded](#Is_CCE_uploaded)\n",
    "* [LL_not_in_POR](#LL_not_in_POR)\n",
    "* [Is_Pavement_limits_repeating_in_the_same_project](#Is_Pavement_limits_repeating_in_the_same_project)\n",
    "* [Repeated_Bridge_within_the_same_project](#Repeated_Bridge_within_the_same_project)\n",
    "* [Is_TMS_Asset_repeating_in_the_same_project](#Is_TMS_Asset_repeating_in_the_same_project)\n",
    "* [Repeated_Culvert_within_the_same_project](#Repeated_Culvert_within_the_same_project)\n",
    "* [Check_Flag](#Check_Flag)\n",
    "* [Project_missing_AMT_ID](#Project_missing_AMT_ID)\n",
    "\n",
    "* Is the Pavement Worksheet using updated inventory and condition data (2019 APCS) for planned projects?\n",
    "* Is the Drainage Worksheet using updated inventory and condition data (Sept 2021 or later) for planned projects?\n",
    "* Is the TMS Worksheet using updated inventory and condition data (June 2021 or later) for planned projects?\n",
    "* Is this project in the Project Book but not in the PID Workplan?  (Applies to non-reservation planned projects in years 6 or 7, and to Long Lead planned projects with PA&ED allocation in year 4.)\n",
    "* Is this project in the PID Workplan but not in the Project Book?  (Applies to non-reservation planned projects in years 6 or 7, and to Long Lead planned projects with PA&ED allocation in year 4.)\n",
    "* For planned projects, does the Performance Tab include a response to \"Is any location within the project limits Ped/Bike accessible?\"\n",
    "* Does the CCA date in the AM Tool match PRSM?\n",
    "* Does the CCA section including performance need to be completed?  (Applies to projects with CCA date on or after July 1, 2020)\n",
    "\n",
    "## [New Checks](#NewChecks)\n",
    "* 'Check Safety Comment'\n",
    "#Check comments about ActID of E55 and E58\n",
    "#if ActID == E55 and E58 and the comments include: 'HQ added the activity and District needs to review it.', mark the project\n",
    "* 'TMS data update needed for the project?'\n",
    "\n",
    "## [Internal Checks](#InteralChecks)\n",
    "* Does amendment date needs to be removed?\n",
    "* Need to add Resource in the PID workplan?\n",
    "* PRG section needs amendment date?\n",
    "* PPC section needs amendment date?\n",
    "\n",
    "## [Export Internal Check Summary](#Export_internal_check_summary)\n",
    "* internal check summary (csv)\n",
    "\n",
    "\n",
    "## [Export Check Summary](#Export_Table1)\n",
    "* data check summary matrix (csv)\n",
    "* data check summary punchlist (csv, tableau datasource)\n",
    "* project missing AMT_ID (csv)\n",
    "\n",
    "\n",
    "## [Export repeated EA or EFIS](#Export_repated_EA_EFIS)\n",
    "* repeated EA (csv)\n",
    "* repeated EFIS (csv)\n",
    "\n",
    "## [Export repeated assets](#Export_repeated_assets)\n",
    "* repeated assets (csv, tableau datasource)\n",
    "\n",
    "\n",
    "## [Export Projectbook](#Export_ProjectBook)\n",
    "* Project book for District asset manager (csv, tableau datasource)\n",
    "* Project book for HQ internal use (csv)\n",
    "\n",
    "\n",
    "## [Export Projectbook Check Sumary Key Dates](#Export_KeyDates)\n",
    "\n",
    "\n",
    "\n",
    "## [Final Clean Up](#FinalCleanUp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-mozambique",
   "metadata": {},
   "source": [
    "# Import common modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fundamental-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "\n",
    "# import requests\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "second-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intellectual-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show dataframe without skip column\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acquired-istanbul",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the Extract API 2.0, please save the output as .hyper format\n"
     ]
    }
   ],
   "source": [
    "# from config_datasource import *\n",
    "# from projectbookcheck_utilityfunction import *\n",
    "from constants import *\n",
    "import projectbookcheck_utilityfunction as uf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-aspect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "comic-disaster",
   "metadata": {},
   "source": [
    "# General Approach\n",
    "\n",
    "use SHOPP raw data as basis for data checks. \n",
    "Each project only occupies one line\n",
    "\n",
    "can expand columns, only if it will not create duplicate rows in the SHOPP raw dataset. \n",
    "\n",
    "\n",
    "Question: what is the entire list of projects, how to include projects without AMT_ID (SHOPP ID)\n",
    "Ans: use raw data, check missing SHOPP ID seperately \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-museum",
   "metadata": {},
   "source": [
    "# Data update procedure\n",
    "\n",
    "data schema: name, data type, default value if missing\n",
    "\n",
    "Option 1: Keep Excel column header fixed (number and sequence)\n",
    "\n",
    "Option 2: Maintain a dictionary of Excel column name and dataframe column name\n",
    "\n",
    "\n",
    "\n",
    "# Check update procedure\n",
    "\n",
    "Create utility function in python, in seperate module\n",
    "add the additional check in the main ETL module\n",
    "update the final data visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-celebrity",
   "metadata": {},
   "source": [
    "# Data clean process\n",
    "\n",
    "* funding amount: remove dollar sign, \n",
    "* fill missing value, string, numerical, \n",
    "* remove leading single quote for string value\n",
    "* strip off leading and trailing space \n",
    "\n",
    "* regulate column names\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-theory",
   "metadata": {},
   "source": [
    "<a id='GlobalConstants'></a>\n",
    "## Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sorted-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_FY = uf.fiscalyear(datetime.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-hindu",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "distinct-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#override to get the live data\n",
    "# DATA_SOURCE_TYPE = 'live'\n",
    "\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "\n",
    "# PROJECTBOOKCHECK_INPUT_FOLDER = r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_DataChecksSupport\\dev\\input'\n",
    "# PROJECTBOOKCHECK_HTTPSERVER_FOLDER = r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_DataChecksSupport\\dev\\output'\n",
    "# PROJECTBOOKCHECK_OUTPUT_FOLDER = r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_DataChecksSupport\\dev\\output'\n",
    "\n",
    "# LOG_FILE = r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_DataChecksSupport\\dev\\log\\ProjectBookExport.log'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-ready",
   "metadata": {},
   "source": [
    "<a id='Read_Data'></a>\n",
    "\n",
    "# Read Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "suitable-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DO NOT USE LIVE DATA, USE THE 5AM DATA LAKE ARCHIVES INSTEAD:\n",
    "# # for live data reading code, go to get_live_data.py\n",
    "# #it takes about 5 minutes to read live data, instead of 0.5 minute to read archived csv files.\n",
    "\n",
    "# if DATA_SOURCE_TYPE == 'live':\n",
    "\n",
    "#     import urllib\n",
    "\n",
    "#     #get live data\n",
    "#     dict_resource = {\n",
    "#         'TenYrShopp_RawData_' : \"http://10.56.12.86/pirs/tenyrshopp/Raw_data.cfm?selectdistrict=all&selectcounty=all&Route=&program=all&pid_cycle=all&shopp_yr=all&tenyearshopp=all&EA=&pID=&projectID=&proj_prog=all&report=Rawdata\"\n",
    "#         ,\n",
    "#         'TenYrShopp_PerfM_Raw_Data_': \"http://10.56.12.86/pirs/tenyrshopp/Raw_data_PerfM.cfm?selectdistrict=all&selectcounty=all&Route=&program=all&pid_cycle=all&shopp_yr=all&tenyearshopp=all&EA=&pID=&projectID=&proj_prog=all&fsection=all\"\n",
    "#         ,\n",
    "#         'Rawdata_Pavement_Worksheet_' : \"http://10.56.12.86/pirs/tenyrshopp/Rawdata_Pavement_WS.cfm?Program=SHOPP&fsection=all\"\n",
    "#         ,\n",
    "#         'Rawdata_Drainage_Worksheet_' : \"http://10.56.12.86/pirs/tenyrshopp/Rawdata_Drainage_WS.cfm?Program=SHOPP&fsection=all\"\n",
    "#         ,\n",
    "#         'Rawdata_Bridge_Worksheet_' : \"http://10.56.12.86/pirs/tenyrshopp/Rawdata_Bridge_WS.cfm?Program=SHOPP&fsection=all\"\n",
    "#         ,\n",
    "#         'Rawdata_TMS_Worksheet_' : \"http://10.56.12.86/pirs/tenyrshopp/Rawdata_TMS_WS.cfm?Program=SHOPP&fsection=all\"\n",
    "#         ,\n",
    "#         'PAC_Perfomrance_RawData_all_' : \"http://10.56.12.86/pirs/tenyrshopp/PAC_Performance_RawData.cfm?District=All\"\n",
    "#         ,\n",
    "#         'Project_Postmile_Check_' : \"http://10.56.12.86/pirs/TenYrShopp/project_Locations.cfm?District=all&view=All&program=All\",\n",
    "\n",
    "#         'Programming_Summary_' : \"http://10.56.12.86/pirs/tenyrshopp/?District=All&program=All&fsection=All&pID=&Placeholder=All&SType=A&page=RawdataProg&report=RawdataProg&selectdistrict=All&getreport=yes&submit=Get+Report\",\n",
    "\n",
    "#         'Minor_Project_Details_Raw_Data_' : 'http://10.56.12.86/pirs/tenyrshopp/?select&Route=&fPType=All&fAllocated=All&fAwarded=All&program=All&pID=&ProjectID=&EA=&FYInUse=&page=RawdataM&report=RawdataM&selectdistrict=All&getreport=yes&submit=Get+Report',\n",
    "\n",
    "#         'HM_Project_Details_Raw_Data_' : 'http://10.56.12.86/pirs/tenyrshopp/?selectcounty=All&Route=&program=All&pID=&ProjectID=&EA=&FYInUse=&Placeholder=All&page=RawdataHM&report=RawdataHM&selectdistrict=All&getreport=yes&submit=Get+Report'\n",
    "#     }\n",
    "\n",
    "#     sleep = 1\n",
    "\n",
    "#     filename = 'TenYrShopp_RawData_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_SHOPP_raw_data = pd.read_html(response.read())[-1]\n",
    "\n",
    "#     time.sleep(sleep)\n",
    "#     filename = 'TenYrShopp_PerfM_Raw_Data_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_perf_raw_data = pd.read_html(response.read())[-1]\n",
    "\n",
    "\n",
    "\n",
    "#     time.sleep(sleep)\n",
    "#     filename = 'Rawdata_Pavement_Worksheet_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_pav_raw_data = pd.read_html(response.read())[-1]\n",
    "#     df_pav_raw_data.columns = df_pav_raw_data.columns.droplevel()\n",
    "#     df_pav_raw_data.columns = df_pav_raw_data.columns.droplevel()\n",
    "\n",
    "#     time.sleep(sleep)\n",
    "#     filename = 'Rawdata_Drainage_Worksheet_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_drain_raw_data = pd.read_html(response.read())[-1]\n",
    "\n",
    "\n",
    "#     time.sleep(sleep)    \n",
    "#     filename = 'Rawdata_Bridge_Worksheet_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_brg_raw_data = pd.read_html(response.read())[-1]\n",
    "#     df_brg_raw_data.columns = df_brg_raw_data.columns.droplevel()\n",
    "\n",
    "\n",
    "#     time.sleep(sleep)    \n",
    "#     filename = 'Rawdata_TMS_Worksheet_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_tms_raw_data = pd.read_html(response.read())[-1]\n",
    "\n",
    "\n",
    "#     time.sleep(sleep)    \n",
    "#     filename = 'Project_Postmile_Check_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_pm_check = pd.read_html(response.read())[-1]\n",
    "\n",
    "\n",
    "#     time.sleep(sleep)\n",
    "#     filename = 'Programming_Summary_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_program_summary = pd.read_html(response.read())[-1]\n",
    "\n",
    "\n",
    "#     time.sleep(sleep)    \n",
    "#     filename = 'HM_Project_Details_Raw_Data_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_HM_raw_data = pd.read_html(response.read())[-1]\n",
    "\n",
    "\n",
    "#     time.sleep(sleep)    \n",
    "#     filename = 'Minor_Project_Details_Raw_Data_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_Minor_raw_data = pd.read_html(response.read())[-1]\n",
    "        \n",
    "        \n",
    "#     DATA_HHMM = datetime.now().strftime(\"%H%M\")\n",
    "# else:\n",
    "#     print('skip getting live data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "artificial-handy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s151589\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\s151589\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (29,30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "File_TimeStamp = '' #always read the latest timestamped file\n",
    "\n",
    "if DATA_SOURCE_TYPE == 'csv':\n",
    "    filename = 'TenYrShopp_RawData_'\n",
    "    df_SHOPP_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp))\n",
    "\n",
    "    filename = 'TenYrShopp_PerfM_Raw_Data_'\n",
    "    df_perf_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp))\n",
    "\n",
    "    filename = 'Rawdata_Bridge_Worksheet_'\n",
    "    df_brg_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), skiprows = [0], header = 0)\n",
    "\n",
    "    filename = 'Rawdata_Pavement_Worksheet_'\n",
    "    df_pav_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), skiprows = [0], header = 1)\n",
    "\n",
    "    filename = 'Rawdata_Drainage_Worksheet_'\n",
    "    df_drain_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), header = 0)\n",
    "\n",
    "    filename = 'Rawdata_TMS_Worksheet_'\n",
    "    df_tms_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), header = 0)\n",
    "\n",
    "    filename = 'Project_Postmile_Check_'\n",
    "    df_pm_check = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), header = 0)\n",
    "\n",
    "    filename = 'Programming_Summary_'\n",
    "    df_program_summary = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), header = 0)\n",
    "\n",
    "\n",
    "    filename = 'Minor_Project_Details_Raw_Data_'\n",
    "    df_Minor_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp))\n",
    "\n",
    "\n",
    "    filename = 'HM_Project_Details_Raw_Data_'\n",
    "    df_HM_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp))\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print('skip getting csv data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "absolute-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'TenYrShopp_RawData_'\n",
    "path_to_file = r'{}\\{}.csv'.format(DATALAKE_FOLDER, filename)\n",
    "t = os.path.getmtime(path_to_file)\n",
    "\n",
    "# File_TimeStamp = datetime.fromtimestamp(t).strftime(\"%m-%d-%Y_%H-%M\")\n",
    "Data_TimeStamp = datetime.fromtimestamp(t).strftime(\"%m-%d-%Y %H:%M:%S\")\n",
    "\n",
    "TARGETDATE = datetime.fromtimestamp(t).strftime(\"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-marketing",
   "metadata": {},
   "source": [
    "## read candidate project ID list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "secure-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Programmed_Projects_check_postmile&performance.xlsx'\n",
    "\n",
    "df_programmed_projects = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "industrial-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_programmed_projects['AMT_ID'] = df_programmed_projects['AMT_ID'].dropna()\n",
    "\n",
    "# df_programmed_projects['AMT_ID']= df_programmed_projects['AMT_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-reservation",
   "metadata": {},
   "source": [
    "## Read the AMT_ID with TMS data change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "suspended-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'AM tool Ids_TMS needs to update if in POR.xlsx'\n",
    "\n",
    "df_TMS_Datachange = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "indirect-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "sht = 'KeyDates'\n",
    "\n",
    "filename = 'GlobalParameters.xlsx'\n",
    "\n",
    "df_GlobalParameters = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), sheet_name =sht) \n",
    "\n",
    "TMS_Data_Date = df_GlobalParameters.loc[0,'TMS_Data_Date'].to_pydatetime().strftime(\"%m-%d-%Y\")\n",
    "\n",
    "TARGET_FY = df_GlobalParameters.loc[0,'TARGET_FY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aboriginal-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "DD_Approval_Placeholder_Date = df_GlobalParameters.loc[0,'District Director Approval Placeholder Date'].to_pydatetime().strftime(\"%m/%d/%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "binary-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD_Approval_Placeholder_Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-handling",
   "metadata": {},
   "source": [
    "<a id='Minor_Raw_Data'></a>\n",
    "\n",
    "## Minor and HM Raw Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "solid-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_Minor_raw_data['District'] = df_Minor_raw_data['District'].apply(uf.remove_punction)\n",
    "df_Minor_raw_data['District'] = df_Minor_raw_data['District'].astype(int)\n",
    "\n",
    "df_Minor_raw_data['Unique EA'] = df_Minor_raw_data.apply(uf.calc_unique_EA, axis = 1)\n",
    "\n",
    "dict_rename = {\n",
    "    'Project ID':'EFIS'\n",
    "              }\n",
    "df_Minor_raw_data = df_Minor_raw_data.rename(dict_rename, axis = 1)\n",
    "\n",
    "#conver EFIS to numeric, \n",
    "#filter null and 0 \n",
    "\n",
    "\n",
    "df_Minor_raw_data['EFIS'] = pd.to_numeric(df_Minor_raw_data['EFIS'], errors='coerce')\n",
    "df_Minor_raw_data = df_Minor_raw_data[(df_Minor_raw_data['EFIS'].notna()) & df_Minor_raw_data['EFIS'] != 0]\n",
    "\n",
    "#rename ID\n",
    "dict_rename = {\n",
    "    'ID': 'AMT_ID',\n",
    "    }\n",
    "df_Minor_raw_data= df_Minor_raw_data.rename(dict_rename, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "operational-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_HM_raw_data['Unique EA'] = df_HM_raw_data.apply(uf.calc_unique_EA, axis = 1)\n",
    "\n",
    "df_HM_raw_data['EFIS'] = pd.to_numeric(df_HM_raw_data['EFIS'], errors='coerce')\n",
    "df_HM_raw_data = df_HM_raw_data[(df_HM_raw_data['EFIS'].notna()) & df_HM_raw_data['EFIS'] != 0]\n",
    "\n",
    "#rename ID\n",
    "dict_rename = {'ID': 'AMT_ID',}\n",
    "df_HM_raw_data= df_HM_raw_data.rename(dict_rename, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-exception",
   "metadata": {},
   "source": [
    "<a id='SHOPP_Raw_Data'></a>\n",
    "## SHOPP Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "palestinian-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data.name = 'df_SHOPP_raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "usual-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "\n",
    "dict_rename_rawdata = {\n",
    "                       'County':'County TYP',\n",
    "                       'Route': 'Route TYP',\n",
    "                       'BackPM':'BackPM TYP',\n",
    "                       'AheadPM':'AheadPM TYP',\n",
    "                       'ID': 'AMT_ID',\n",
    "                       'Ten-Year Plan': 'Ten-Year Plan RD',\n",
    "                       'County.1' : 'County PRG',\n",
    "                       'Route.1' : 'Route PRG',\n",
    "                       'BackPM.1':'BackPM PRG',\n",
    "                       'AheadPM.1' : 'AheadPM PRG',\n",
    "                       'County.2' : 'County PCR',\n",
    "                       'Route.2' : 'Route PCR',\n",
    "                       'BackPM.2':'BackPM PCR',\n",
    "                       'AheadPM.2' : 'AheadPM PCR',\n",
    "                       'Activity Category': 'Activity'\n",
    "                      }\n",
    "df_SHOPP_raw_data = df_SHOPP_raw_data.rename(dict_rename_rawdata, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "directed-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove leading puncture for target columns\n",
    "cols_strip = ['District','Route TYP','EA','EFIS','Route PRG','PPNO','Route PCR']\n",
    "for c in cols_strip :\n",
    "    df_SHOPP_raw_data[c] = df_SHOPP_raw_data[c].str.strip(\"'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aquatic-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cost_columns = [\n",
    "    'RW Cost ($K)',\n",
    "    'Const Cost ($K)',\n",
    "    'Support Cost ($)',\n",
    "    'TYP Total Project Cost ($K)',\n",
    "    'PAED ($K)',\n",
    "    'PSE ($K)',\n",
    "    'R/W ($K)',\n",
    "    'CONS ($K)',\n",
    "    'Prog Support Cost ($)',\n",
    "    'Prog RW Cost ($K)',\n",
    "    'Prog Const Cost ($K)',\n",
    "    'Prog Total Project Cost ($K)',\n",
    "    'PCR R/W Cap ($K)',\n",
    "    'PCR Const Cap ($K)',\n",
    "    'PCR Support Cost ($K)',\n",
    "    'PCR Total Cost ($K)',\n",
    "    'Project Cost In Use',\n",
    "    'Total LL Prog ($K)',\n",
    "    'LL PAED Cost ($K)',\n",
    "    'LL CONS Cap ($K)'\n",
    "               ]\n",
    "\n",
    "for c in cost_columns:\n",
    "    df_SHOPP_raw_data[c] = df_SHOPP_raw_data[c].apply(uf.curreny_to_float)\n",
    "    df_SHOPP_raw_data[c].fillna(0, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "checked-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data clean \n",
    "#data type regulation\n",
    "#string/text data regulation\n",
    "\n",
    "df_SHOPP_raw_data['District'] =df_SHOPP_raw_data['District'].astype(int)\n",
    "\n",
    "df_SHOPP_raw_data['EFIS'] = pd.to_numeric(df_SHOPP_raw_data['EFIS'], errors='coerce')\n",
    "\n",
    "df_SHOPP_raw_data['Route PCR'] = df_SHOPP_raw_data['Route PCR'].astype(str)\n",
    "\n",
    "#data trimming\n",
    "#row trimming\n",
    "df_SHOPP_raw_data= df_SHOPP_raw_data[df_SHOPP_raw_data['District'] != 56]\n",
    "\n",
    "#column trimming\n",
    "df_SHOPP_raw_data.drop(['District Priority', 'PIR Performance Report'],\n",
    "  axis='columns', inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "#Question to be answered:\n",
    "#for EA Raw Data, the missing data is not null , but ''. Should we handle the missing data uniformly for string data?\n",
    "\n",
    "#TODO:\n",
    "#fill missing data\n",
    "#data quality check (checksum,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bright-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['CCA Date Miilestone (M600)'] = df_SHOPP_raw_data['CCA Date Miilestone (M600)'].apply(uf.regulate_timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "responsible-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-belief",
   "metadata": {},
   "source": [
    "<a id='TenYrShopp_Perf_RawData'></a>\n",
    "## TenYrShopp_Perf_RawData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "psychological-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "dict_rename_perf_rawdata = {\n",
    "                           'ID': 'AMT_ID',\n",
    "#                             'ProjectedRTL FY': 'Projected RTL FY',\n",
    "    \n",
    "# 'ActID':'Performance_ActID',\n",
    "# 'Quantity':    'Performance_Quantity'\n",
    "              }\n",
    "\n",
    "df_perf_raw_data = df_perf_raw_data.rename(dict_rename_perf_rawdata, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "found-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_strip = ['EA','EFIS','PPNO']\n",
    "for c in cols_strip :\n",
    "    df_perf_raw_data[c] = df_perf_raw_data[c].str.strip(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "secure-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-economy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "sustainable-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data clean \n",
    "#data type regulation\n",
    "\n",
    "df_perf_raw_data['Quantity'] = df_perf_raw_data['Quantity'].fillna(0)\n",
    "df_perf_raw_data['Assets in Good Cond'] = df_perf_raw_data['Assets in Good Cond'].fillna(0)\n",
    "df_perf_raw_data['Assets in Fair Cond'] = df_perf_raw_data['Assets in Fair Cond'].fillna(0)\n",
    "df_perf_raw_data['Assets in Poor Cond'] = df_perf_raw_data['Assets in Poor Cond'].fillna(0)\n",
    "df_perf_raw_data['New Assets Added'] = df_perf_raw_data['New Assets Added'].fillna(0)\n",
    "\n",
    "df_perf_raw_data['EFIS'] = pd.to_numeric(df_perf_raw_data['EFIS'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dedicated-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data trimming\n",
    "#row\n",
    "df_perf_raw_data= df_perf_raw_data[df_perf_raw_data['District'] != 56]\n",
    "#column\n",
    "df_perf_raw_data.drop(['PID Cycle', 'TYP','ProjectedSHOPP Cycle','RequestedRTL FY','DistrictPriority'],\n",
    "  axis='columns', inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "sapphire-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data.name = 'df_perf_raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-tampa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "finnish-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw = df_perf_raw_data.merge(df_SHOPP_raw_data, \n",
    "#                               how = 'outer', \n",
    "#                               left_on = 'AMT_ID', right_on = 'AMT_ID',\n",
    "#                              suffixes = ['_raw_perf', '_raw_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-healthcare",
   "metadata": {},
   "source": [
    "<a id='ProgrammingList'></a>\n",
    "## Programming List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "completed-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "shts = ['2010 SHOPP',\n",
    "        '2012 SHOPP',\n",
    "        '2014 SHOPP',\n",
    "        '2016 SHOPP',\n",
    "        '2018 SHOPP',\n",
    "        '2020 SHOPP',\n",
    "        'Long Lead'\n",
    "        ]\n",
    "\n",
    "filename = 'Programming_list.xlsx'\n",
    "\n",
    "df_dict = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), sheet_name =shts) \n",
    "\n",
    "\n",
    "df_program = pd.DataFrame()\n",
    "\n",
    "for k, v in df_dict.items():\n",
    "#     print(type(v))\n",
    "#     print(k)\n",
    "    v['Table Names'] = k\n",
    "#     print(v.columns)\n",
    "    df_program = df_program.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "optional-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "dict_rename_program = {\n",
    "#                         'EA':'EA', \n",
    "                        'EFIS':'EFIS_Program', \n",
    "#                         'PPNO':'PPNO Programming', \n",
    "                        'Total Capital & Support':'Total Capital & Support Cost',\n",
    "#                         'Route': 'Route Programming',\n",
    "              }\n",
    "\n",
    "df_program = df_program.rename(dict_rename_program, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "thick-drunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s151589\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\pandas\\core\\frame.py:4459: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n"
     ]
    }
   ],
   "source": [
    "#data clean \n",
    "#data type regulation\n",
    "# df_program['Dist'] = df_program['Dist'].astype(int)\n",
    "# df_program['EFIS'] = pd.to_numeric(df_program['EFIS'], errors='coerce')\n",
    "\n",
    "fillna_columns = ['Con Sup','RW Sup','PA&ED','PS&E', 'PA&ED', 'RW', 'Con']\n",
    "\n",
    "df_program[fillna_columns].fillna(0, inplace=True)\n",
    "df_program['Route'].fillna('Various', inplace=True)\n",
    "\n",
    "df_program['Support Cost'] = df_program['Con Sup']+df_program['RW Sup']+df_program['PA&ED']+df_program['PS&E']\n",
    "df_program['Capital Cost'] = df_program['Con']+df_program['RW']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "royal-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program[['Con Sup','RW Sup','PA&ED','PS&E']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "supreme-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program['PA&ED'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "hydraulic-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove leading puncture for all string columns\n",
    "# df_program.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "vulnerable-twelve",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "#data transformation\n",
    "\n",
    "# df_program.dropna(subset = ['EFIS_Program'], inplace = True)\n",
    "df_program['FY'] = df_program['FY'].apply(uf.FY_cleanup)\n",
    "\n",
    "df_program['Begin Post Miles'] = df_program['Post Miles'].apply(lambda x: str(x).split('/')[0])\n",
    "df_program['End Post Miles'] = df_program['Post Miles'].apply(lambda x: str(x).split('/')[0] if '/' in str(x) else np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "settled-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data trimming\n",
    "df_program.drop(['PM1BF', 'PM1B', 'PM1AF','PM1A',], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "suburban-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program.name = 'df_program'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "promising-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "senior-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "coastal-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data sanity check\n",
    "#check duplicates\n",
    "#check null\n",
    "#check data type\n",
    "\n",
    "if df_program['EFIS_Program'].value_counts(dropna = False).max() > 1:\n",
    "    Print('Duplicate EFIS ID found, please check the source data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "civilian-particle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_program['EFIS_Program'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-citizenship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "nearby-maldives",
   "metadata": {},
   "source": [
    "<a id='SHOPP_Candidates'></a>\n",
    "## SHOPP candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "alternate-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'SHOPP candidates.xlsx'\n",
    "\n",
    "df_shopp_candidate = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "appreciated-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rename_candidate = {\n",
    "                        'SHOPP ID':'AMT_ID', \n",
    "    'Advertised Year': 'Advertised Year_Candidate',\n",
    "    'Project Cost ($K)' : 'Project Cost ($K)_Candidate'\n",
    "              }\n",
    "\n",
    "df_shopp_candidate = df_shopp_candidate.rename(dict_rename_candidate, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "common-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shopp_candidate['AMT_ID'] = df_shopp_candidate['AMT_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bridal-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['AMT_ID', 'Candidate Type','Advertised Year_Candidate','Project Cost ($K)_Candidate']\n",
    "df_shopp_candidate = df_shopp_candidate[export_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "affecting-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shopp_candidate.name = 'df_shopp_candidate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "specialized-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_shopp_candidate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-edition",
   "metadata": {},
   "source": [
    "<a id='Counties'></a>\n",
    "## Counties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "standing-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Counties.xlsx'\n",
    "\n",
    "df_counties = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "virtual-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counties['Co. Name Abbr.'] = df_counties['Co. Name Abbr.'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bigger-termination",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 6)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_counties.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "informative-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counties.name = 'df_counties'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "spectacular-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_prog_county = df_perf_raw_prog_candidate.merge(df_counties, how = 'left', left_on = 'County', right_on = 'Co. Name Abbr.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "therapeutic-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no need for the following, already added to the df_perf_raw_data\n",
    "\n",
    "# #rename columns\n",
    "# dict_rename_4= {\n",
    "#                'Performance Objective':'Performance Objective Original', \n",
    "#               }\n",
    "\n",
    "# df_perf_raw_prog_county = df_perf_raw_prog_county.rename(dict_rename_4, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-server",
   "metadata": {},
   "source": [
    "<a id='Bridge_Inventory'></a>\n",
    "\n",
    "## Bridge Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "baking-leather",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'GFP_BrInvList_AllDistricts.xlsx'\n",
    "\n",
    "df_bridge_inventory = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), skiprows = 4, header = [0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "respiratory-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filename = 'GFP_BrInvList_AllDistricts_March_2020_05042020.xlsx'\n",
    "\n",
    "# df_bridge_inventory = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), skiprows = 4, header = [0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "capital-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = ['Bridge #', 'Deck Area, SF', 'Date', 'Health', 'Deck (58)',\n",
    "       'Super (59)', 'Sub (60)', 'Culv (62)', 'Scour', 'Seismic', 'Overall',\n",
    "       'VC', 'Permit', 'Rail Overall', 'Good, LF', 'Fair, LF', 'Poor, LF',\n",
    "       'Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "refined-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bridge_inventory = df_bridge_inventory[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "unknown-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "\n",
    "dict_rename_bridge_inventory = {'Bridge #':'BridgeNo',\n",
    "                               'Date':'Inspection Date', \n",
    "                               'Health': 'Bridge Health',\n",
    "                                'Scour':'Bridge Scour', \n",
    "                                'Seismic':'Bridge Seismic', \n",
    "                               'Deck (58)' : 'NBI Condition Ratings_Deck (58)',\n",
    "                                'Super (59)': 'NBI Condition Ratings_Super (59)',\n",
    "                                'Sub (60)':'NBI Condition Ratings_Sub (60)',\n",
    "                                'Culv (62)':'NBI Condition Ratings_Culv (62)',\n",
    "                                'Overall': 'Bridge Goods Movement_Overall',\n",
    "                                'VC': 'Bridge Goods Movement_VC',\n",
    "                                'Permit':'Bridge Goods Movement_Permit',\n",
    "                                'Rail Overall': 'Bridge Rail Upgrade_Rail Overall', \n",
    "                                'Good, LF': 'Bridge Rail Upgrade_Good, LF', \n",
    "                                'Fair, LF': 'Bridge Rail Upgrade_Fair, LF', \n",
    "                                'Poor, LF': 'Bridge Rail Upgrade_Poor, LF', \n",
    "                      }\n",
    "\n",
    "df_bridge_inventory.rename(dict_rename_bridge_inventory, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "neural-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_to_good(value):\n",
    "    if value and value in ['Poor','Fair','Good']:\n",
    "        return value\n",
    "    else:\n",
    "        return 'Good'\n",
    "\n",
    "data_recondition_columns = ['Bridge Health','Bridge Scour','Bridge Seismic','Bridge Goods Movement_Overall',\n",
    "       'Bridge Goods Movement_VC', 'Bridge Goods Movement_Permit','Bridge Rail Upgrade_Rail Overall',]\n",
    "\n",
    "for c in data_recondition_columns:\n",
    "    df_bridge_inventory[c] = df_bridge_inventory[c].apply(default_to_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "descending-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bridge_inventory.name = 'df_bridge_inventory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-dependence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "coated-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bridge_inventory.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-prisoner",
   "metadata": {},
   "source": [
    "<a id='Bridge_Worksheet'></a>\n",
    "\n",
    "## Raw Data Bridge Worksheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "economic-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "#with manual edits\n",
    "\n",
    "dict_rename_bridge_worksheet = {\n",
    " 'ID': 'AMT_ID',\n",
    " 'Bridge №': 'BridgeNo',\n",
    " 'Work Type': 'WorkType',\n",
    " 'Brdige / TunnelWork Description': 'WorkDescription',\n",
    " 'Bridge /TunnelHealth Pre': 'Health Pre',\n",
    " 'Bridge /TunnelHealth Post': 'Health Post',\n",
    " 'BridgeScourPre': 'Scour_Pre',\n",
    " 'BridgeScourPost': 'Scour_Post',\n",
    " 'BridgeSeismicPre': 'Seismic_Pre',\n",
    " 'BridgeSeismicPost': 'Seismic_Post',\n",
    " 'BridgeGds MvmtPre': 'GdsMvmt_Pre',\n",
    " 'BridgeGds MvmtPost': 'GdsMvmt_Post',\n",
    " 'Exist(sf)': 'Deck_Exist(sf)',\n",
    " 'Additional(sf)': 'Deck_Additional(sf)',\n",
    " 'Y/N': 'Paint_Y/N',\n",
    " 'Condition': 'Paint_Condition',\n",
    " 'Paint Area(sf)': 'Paint Area(sf)',\n",
    " 'Y/N.1': 'ElectricalMechanical_Y/N',\n",
    " 'Condition.1': 'ElectricalMechanical_Condition',\n",
    " 'Area(sf)': 'ElectricalMechanical_Area(sf)',\n",
    " 'Y/N.2': 'ApproachSlab_Y/N',\n",
    " 'Replaced(sf)': 'ApproachSlab_Replaced(sf)',\n",
    " 'New(sf)': 'ApproachSlab_New(sf)',\n",
    " 'Y/N.3': 'Rail_Y/N',\n",
    " 'Good(lf)': 'Rail_Good(lf)',\n",
    " 'Fair(lf)': 'Rail_Fair(lf)',\n",
    " 'Poor(lf)': 'Rail_Poor(lf)',\n",
    " 'Additonal(lf)': 'Rail_Additonal(lf)',\n",
    " 'Post Good(lf)': 'Rail_Post Good(lf)',\n",
    " 'Post Fair(lf)': 'Rail_Post Fair(lf)',\n",
    " 'Post Poor(lf)': 'Rail_Post Poor(lf)',\n",
    " 'Post New(lf)': 'Rail_Post New(lf)',\n",
    " 'FishPassage(Y/N)': 'FishPassage(Y/N)',\n",
    "}\n",
    "\n",
    "df_brg_raw_data.rename(dict_rename_bridge_worksheet, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "unlike-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brg_raw_data.name = 'df_brg_raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "correct-tanzania",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_brg_raw_data['Rail_Good(lf)'].fillna(0, inplace = True)\n",
    "df_brg_raw_data['Rail_Fair(lf)'].fillna(0, inplace = True)\n",
    "df_brg_raw_data['Rail_Poor(lf)'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "latin-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brg_raw_data['Rail_Total(lf)'] = (df_brg_raw_data['Rail_Good(lf)'] \n",
    "                                             + df_brg_raw_data[ 'Rail_Fair(lf)'] \n",
    "                                             + df_brg_raw_data['Rail_Poor(lf)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-shipping",
   "metadata": {},
   "source": [
    "<a id='Pavement_Worksheet'></a>\n",
    "\n",
    "## Raw Data Pavement Worksheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-accounting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "sixth-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "#with manual editing\n",
    "\n",
    "dict_rename_pavement_worksheet = {\n",
    " 'ID': 'AMT_ID',\n",
    " 'Class': 'RoadwayClass',\n",
    " 'Green': 'TriditionalCondition_Green',\n",
    " 'Yellow': 'TriditionalCondition_Yellow',\n",
    " 'Blue': 'TriditionalCondition_Blue',\n",
    " 'Orange': 'TriditionalCondition_Orange',\n",
    " 'Red': 'TriditionalCondition_Red',\n",
    " 'Good': 'MAP21_Good',\n",
    " 'Fair': 'MAP21_Fair',\n",
    " 'Poor': 'MAP21_Poor',\n",
    " 'Total LaneMiles': 'Total LaneMiles',\n",
    " 'Green.1': 'TriditionalCondition_Post_Green',\n",
    " 'Yellow.1': 'TriditionalCondition_Post_Yellow',\n",
    " 'Blue.1': 'TriditionalCondition_Post_Blue',\n",
    " 'Orange.1': 'TriditionalCondition_Post_Orange',\n",
    " 'Red.1': 'TriditionalCondition_Post_Red',\n",
    " 'Good.1': 'MAP21_Post_Good',\n",
    " 'Fair.1': 'MAP21_Post_Fair',\n",
    " 'Poor.1': 'MAP21_Post_Poor',\n",
    "#  'Date': 'Date'\n",
    "                               }\n",
    "df_pav_raw_data.rename(dict_rename_pavement_worksheet, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-potter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ordered-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pav_raw_data.name = 'df_pav_raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "controlling-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pav_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "collect-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pav_raw_data['Plan Year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-canvas",
   "metadata": {},
   "source": [
    "<a id='Drainage_Worksheet'></a>\n",
    "## Raw Data Drainage Worksheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "collective-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drain_raw_data.name = 'df_drain_raw_data'\n",
    "\n",
    "dict_drain_rename = {\n",
    " 'ID': 'AMT_ID',\n",
    " 'Data Date':'Data Date_Drainage'\n",
    "                               }\n",
    "df_drain_raw_data.rename(dict_drain_rename, axis = 1, inplace = True)\n",
    "\n",
    "cols = ['EA','EFIS','SYSNO','INETNO','OUTETNO']\n",
    "for c in cols: \n",
    "    df_drain_raw_data[c] = df_drain_raw_data[c].apply(uf.remove_punction)\n",
    "\n",
    "\n",
    "df_drain_raw_data['Data Date_Drainage'] = df_drain_raw_data['Data Date_Drainage'].apply(uf.regulate_timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "infectious-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#question to be answered: what to do with meanless or null values.\n",
    "#return null if DrainageWorksheet_SYSNO,DrainageWorksheet_INETNO,DrainageWorksheet_OUTETNO are null or empty\n",
    "\n",
    "# df_drain_raw_data['Unique Culvert ID'] = (df_drain_raw_data['SYSNO'] + \"_\"\n",
    "#                                           + df_drain_raw_data['INETNO'] + \"_\"\n",
    "#                                          + df_drain_raw_data['OUTETNO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "located-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_drain_raw_data['SYSNO'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "particular-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_drain_unique_ID(df):\n",
    "    if pd.isnull(df['SYSNO']) or pd.isnull(df['INETNO']) or pd.isnull(df['OUTETNO']):\n",
    "        return None\n",
    "    else:\n",
    "        return (df['SYSNO'] + \"_\"+ df['INETNO'] + \"_\"+ df['OUTETNO'])\n",
    "df_drain_raw_data['Unique Culvert ID'] = df_drain_raw_data.apply(calc_drain_unique_ID, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "liberal-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_drain_raw_data.select_dtypes(include=[object]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "acknowledged-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (columnName, columnData) in df_drain_raw_data.select_dtypes(include=[object]).iteritems():\n",
    "#     df_drain_raw_data[columnName]=columnData.apply(remove_punction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "generous-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_drain_ws = df_drain_raw_data.merge(df_SHOPP_raw_data, how = 'left', \n",
    "#                   left_on = ['AMT_ID', 'Date'], \n",
    "#                   right_on = ['AMT_ID', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-patio",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "virgin-conclusion",
   "metadata": {},
   "source": [
    "<a id='TMS_Worksheet'></a> \n",
    "## Raw Data TMS Worksheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "happy-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_TMS_rename = {\n",
    " 'ID': 'AMT_ID',\n",
    " 'Data Date':'Data Date_TMS'\n",
    "                               }\n",
    "df_tms_raw_data.rename(dict_TMS_rename, axis = 1, inplace = True)\n",
    "\n",
    "# df_tms_ws = df_tms_raw_data.merge(df_SHOPP_raw_data, how = 'left', \n",
    "#                   left_on = ['AMT_ID', 'Date'], \n",
    "#                   right_on = ['AMT_ID', 'Date'])\n",
    "\n",
    "df_tms_raw_data.name = 'df_tms_raw_data'\n",
    "\n",
    "df_tms_raw_data.shape\n",
    "\n",
    "\n",
    "\n",
    "df_tms_raw_data['Data Date_TMS'] = df_tms_raw_data['Data Date_TMS'].apply(uf.regulate_timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-processor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "classified-boost",
   "metadata": {},
   "source": [
    "<a id='Postmile_Check'></a>\n",
    "## Postmile Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "adult-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_PM_ck_rename = {\n",
    " 'ID': 'AMT_ID',\n",
    " '№': 'No'                            }\n",
    "df_pm_check.rename(dict_PM_ck_rename, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "alike-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pm_check['District'] = df_pm_check['District'].str.strip(\"'\")\n",
    "df_pm_check['District'] =df_pm_check['District'].astype(int)\n",
    "df_pm_check = df_pm_check[df_pm_check['District']!= 56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "neutral-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pm_check = df_pm_check[df_pm_check['Program'] == 'SHOPP']\n",
    "df_pm_check['AMT_ID'] = df_pm_check['AMT_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "differential-directory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['No', 'District', 'AMT_ID', 'EA', 'EFIS', 'Location', 'Section',\n",
       "       'County', 'Route', 'BackPM', 'AheadPM', 'Alignment', 'Valid PM',\n",
       "       'Activity Category', 'Program', 'BackPMLatitude', 'BackPMLongitude',\n",
       "       'BackPMAssemblyDistrict', 'BackPMCongressDistrict',\n",
       "       'BackPMSenateDistrict', 'AheadPMLatitude', 'AheadPMLongitude',\n",
       "       'AheadPMAssemblyDistrict', 'AheadPMCongressDistrict',\n",
       "       'AheadPMSenateDistrict', 'AssemblyDistrict(s)', 'CongressDistrict(s)',\n",
       "       'SenateDistrict(s)', 'Status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pm_check.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "amazing-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pm_check.columns = ['No', 'District', 'AMT_ID', 'EA', 'EFIS', 'Location',\n",
    "#        'Section', 'County', 'Route', 'BackPM', 'AheadPM',\n",
    "#        'Alignment', 'Valid PM', 'Activity Category', 'Program',\n",
    "#        'BackPMLatitude', 'BackPMLongitude',\n",
    "#        'BackPMAssemblyDistrict', 'BackPMCongressDistrict',\n",
    "#        'BackPMSenateDistrict', 'AheadPMLatitude',\n",
    "#        'AheadPMLongitude', 'AheadPMAssemblyDistrict',\n",
    "#        'AheadPMCongressDistrict', 'AheadPMSenateDistrict',\n",
    "#        'AssemblyDistrict(s)', 'CongressDistrict(s)',\n",
    "#        'SenateDistrict(s)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "molecular-passenger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7979, 29)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pm_check.name = 'df_pm_check'\n",
    "df_pm_check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "governing-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataCheck_perf_loc = df_dataCheckFlow_all_ws.merge(df_pm_check, how = 'outer', \n",
    "#                   left_on = ['AMT_ID','Date'], \n",
    "#                   right_on = ['AMT_ID', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "shared-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataCheck_perf_loc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "annoying-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataCheck_perf_loc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-ensemble",
   "metadata": {},
   "source": [
    "<a id='PID_Workload'></a>\n",
    "\n",
    "## 2022 PID workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "interim-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PID_workload = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "turkish-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = 'PID SHOPP WL.xlsx'\n",
    "shts = ['PID workload', 'ColumnHeaderDictionary']\n",
    "\n",
    "dict_df = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), sheet_name = shts) \n",
    "\n",
    "df_PID_workload = dict_df['PID workload']\n",
    "df_rename = dict_df['ColumnHeaderDictionary'].iloc[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "robust-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "\n",
    "# dict_rename_PID_workplan = {'SHOPP Tool ID': 'AMT_ID',\n",
    "#                                }\n",
    "\n",
    "dict_col_rename = dict(zip(df_rename.OriginalName, df_rename.NewName))\n",
    "df_PID_workload.rename(dict_col_rename, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "friendly-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PID_workload['District'] = df_PID_workload['District'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "latin-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PID_workload.name = 'df_PID_workload'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "incident-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_PID_workload.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-feelings",
   "metadata": {},
   "source": [
    "<a id='Check_Exceptions'></a>\n",
    "## Check Exceptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "downtown-landscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s151589\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:312: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "filename = 'Checks_exception.xlsx'\n",
    "\n",
    "df_ck_exceptions = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "female-extraction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 4)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ck_exceptions.name = 'df_ck_exceptions'\n",
    "df_ck_exceptions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "authentic-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rename_ck_exception = {'Exception ID': 'AMT_ID',\n",
    "                               }\n",
    "df_ck_exceptions.rename(dict_rename_ck_exception, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "invalid-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataCheckwException = df_dataCheck_PID.merge(df_ck_exceptions, how = 'outer', \n",
    "#                   left_on = ['AMT_ID'], \n",
    "#                   right_on = ['AMT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "hairy-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_ck_exceptions.shape)\n",
    "# print(df_dataCheckwException.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "presidential-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question\n",
    "#we will not ensure only one entry in the exception\n",
    "#we need to consider all exception\n",
    "#  df_ck_exceptions[(df_ck_exceptions['AMT_ID'] == AMT_ID) & ( df_ck_exceptions['Type of Exception'] =='Repeated Culvert')].shape[0]>0:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-cookie",
   "metadata": {},
   "source": [
    "<a id='Project_Obselete'></a>\n",
    "## Project to be obseleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dependent-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Combined projects.xlsx'\n",
    "\n",
    "df_project_obselete = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "agricultural-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_project_obselete.name = 'df_project_obselete'\n",
    "# df_project_obselete.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "medium-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_rename_project_obselete = {'TOOL ID to obsolete': 'AMT_ID to obsolete',\n",
    "                               }\n",
    "df_project_obselete.rename(dict_rename_project_obselete, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "entertaining-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataCheck_combined =  df_dataCheckwException.merge(df_project_obselete, how = 'left', \n",
    "#                   left_on = ['AMT_ID'], \n",
    "#                   right_on = ['TOOL ID to obsolete'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "killing-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_project_obselete['TOOL ID to obsolete'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-comedy",
   "metadata": {},
   "source": [
    "<a id='Project_Detail_Report'></a>\n",
    "## Project Detail Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "blank-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'project_detail_report_nickname.xlsx'\n",
    "\n",
    "# df_project_detail =pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), sheet_name = 'project_detail_report') \n",
    "\n",
    "\n",
    "\n",
    "# shts = ['project_detail_report','Instruction']\n",
    "\n",
    "# dict_df = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), sheet_name = shts) \n",
    "\n",
    "# df_project_detail = dict_df['project_detail_report']\n",
    "\n",
    "# datatimestamp = dict_df['Instruction'].iloc[0,0]\n",
    "# df_project_detail['QMRS Report Date'] = datatimestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "previous-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Project Detail Report.csv'\n",
    "\n",
    "df_project_detail =pd.read_csv(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "conscious-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_cca_date(df):\n",
    "    \n",
    "    if pd.isna(df['Cca Finish Date']):\n",
    "        return np.nan\n",
    "    \n",
    "    dt = datetime.strptime(df['Cca Finish Date'], '%d-%b-%y') \n",
    "    \n",
    "    if dt < datetime.strptime('01-01-1978', '%M-%d-%Y'):\n",
    "        dt = datetime(dt.year+100, dt.month, dt.day, dt.hour, dt.minute, dt.second, dt.microsecond, dt.tzinfo)\n",
    "    return dt.strftime(\"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "approximate-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_project_detail['Cca Finish Date'] = df_project_detail.apply(cleanup_cca_date, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "theoretical-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_project_detail.name = 'df_project_detail'\n",
    "# df_project_detail.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "demanding-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rename_project_detail = {'Project ID': 'EFIS',\n",
    "                               }\n",
    "df_project_detail.rename(dict_rename_project_detail, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "broke-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_project_detail['EFIS'] = pd.to_numeric(df_project_detail['EFIS'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "cheap-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataCheckwNickname = df_dataCheck_combined.merge(df_project_detail, how = 'left', \n",
    "#                   left_on = ['EFIS'], \n",
    "#                   right_on = ['Project ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-authorization",
   "metadata": {},
   "source": [
    "<a id='Programming_Summary'></a>\n",
    "## Programming Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-lighting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "equivalent-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data trimming\n",
    "#row\n",
    "df_program_summary = df_program_summary[df_program_summary['Program'] == 'SHOPP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "handed-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporary for debug\n",
    "df_program_summary['ID'] = df_program_summary['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fatty-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rename_program_summary = {'ID': 'AMT_ID',\n",
    "                               }\n",
    "df_program_summary.rename(dict_rename_program_summary, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "advanced-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_summary.dropna(subset = ['District'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "russian-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_summary['Performance Value'] = df_program_summary['Performance Value'].astype(str)\n",
    "df_program_summary['Performance Value'].fillna('N/A', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "solar-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove leading puncture for all string columns\n",
    "col_strip=['EA', 'EFIS',  'Program Code',]\n",
    "for c in col_strip:\n",
    "    df_program_summary[c] = df_program_summary[c].str.strip(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fallen-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_summary['EFIS'] = pd.to_numeric(df_program_summary['EFIS'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "mineral-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program_summary['EFIS'].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "actual-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program_summary['AMT_ID'] = df_program_summary['AMT_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "executed-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_summary['Performance Value'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "documentary-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_summary['Performance Value'] = df_program_summary['Performance Value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "grave-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_summary.name = 'df_program_summary'\n",
    "# df_program_summary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-blend",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cultural-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert percentage to actual quantity\n",
    "\n",
    "def percent_to_quantity(df, col, subtotal_col):\n",
    "    if isinstance(df[c], str) and '%' in df[c]:\n",
    "        return (float(df[c].strip('%'))/100) * df[subtotal_col]\n",
    "    else:\n",
    "        return float(df[c])\n",
    "\n",
    "ck_cols = [ 'Pre-Good', 'Pre-Fair', 'Pre-Poor',]\n",
    "\n",
    "for c in ck_cols:\n",
    "    df_program_summary[c] = df_program_summary.apply(percent_to_quantity, args=(c,'Pre-Total'), axis = 1)\n",
    "\n",
    "ck_cols = [ 'Post Good', 'New', 'Post Good+New', 'Post-Fair',\n",
    "       'Post-Poor']\n",
    "\n",
    "for c in ck_cols:\n",
    "    df_program_summary[c] = df_program_summary.apply(percent_to_quantity, args=(c,'Post-Total'), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-defensive",
   "metadata": {},
   "source": [
    "<a id='AddDataColumns'></a>\n",
    "## Calculate and join additional fields\n",
    "\n",
    "- Section\n",
    "- County\n",
    "- Route\n",
    "- End Postmile\n",
    "- Begin Postmile\n",
    "\n",
    "\n",
    "* Advertised Year\n",
    "* Last Year FY POR\n",
    "* Last Year of Fiscal Year\n",
    "* Will this project be included in the Project Book?\n",
    "* AM Tool RTL (Section in Use)\n",
    "* FY\n",
    "* Activity (group)\n",
    "\n",
    "* Total Project Cost ($K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "linear-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['EA_'] = df_SHOPP_raw_data['EA'].apply(lambda x: \"'\" + x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-affect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "favorite-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'FY','Total Capital & Support Cost' to raw data\n",
    "df_SHOPP_raw_data.drop(columns =['EFIS_Program','FY','Total Capital & Support Cost','Capital Cost', 'Support Cost'], inplace=True, errors = 'ignore')\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, df_program[['EFIS_Program','FY','Total Capital & Support Cost','Capital Cost', 'Support Cost' ]], \n",
    "                             how = 'left', left_on = 'EFIS', right_on='EFIS_Program')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-salmon",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "stunning-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_and_convert_FY(FY):\n",
    "    #get the last two number of FY string and convert to 4 digit FY value, \n",
    "    #for na value, fill 0\n",
    "    if FY is np.nan:\n",
    "        return 0\n",
    "#     elif int(FY[-2:]) == 0:\n",
    "#         return 0\n",
    "    else:\n",
    "        return int(FY[-2:])+2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "aggressive-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['Target RTL FY Number'] = df_SHOPP_raw_data['Target RTL FY'].apply(calc_and_convert_FY)\n",
    "\n",
    "df_SHOPP_raw_data['Requested RTL FY Number'] = df_SHOPP_raw_data['Requested RTL FY'].apply(calc_and_convert_FY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "raised-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['Target RTL FY Number'].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "tough-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this logic needs to consider the programming list\n",
    "df_SHOPP_raw_data['Section'] = df_SHOPP_raw_data.apply(uf.cal_section_in_use, axis=1)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "fancy-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reconcile: \n",
    "# AMT_ID = 16813\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID][['AMT_ID','Section',\n",
    "# 'CCA Date Miilestone (M600)' ,'PCR SHOPP Amendment Date' ,\n",
    "# 'SHOPP Amendment Date' ,'EFIS_Program' ,'Long Lead' ,\n",
    "\n",
    "# 'Resourced In PID WP' ,'Requested RTL FY' ,'Dist Dir Appr','PID Uploaded' ,\n",
    "# 'LL RTL FY PRG' ,\n",
    "\n",
    "# ] ]#needs to be TYP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-diabetes",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "secret-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['County'] = df_SHOPP_raw_data.apply(uf.county_to_use, axis=1)\n",
    "# df_SHOPP_raw_data['County'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "reserved-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['Route'] = df_SHOPP_raw_data.apply(uf.route_to_use, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "auburn-railway",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['Begin Postmile'] = df_SHOPP_raw_data.apply(uf.beginPM_to_use, axis=1)\n",
    "df_SHOPP_raw_data['End Postmile'] = df_SHOPP_raw_data.apply(uf.endPM_to_use, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "outdoor-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['Unique EA'] = df_SHOPP_raw_data.apply(uf.calc_unique_EA, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "gross-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['Activity (group)'] = df_SHOPP_raw_data['Activity'].apply(uf.calc_activity_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "figured-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['Activity Book'] = df_SHOPP_raw_data['Activity'].apply(uf.calc_activity_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "enclosed-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['AM Tool RTL (Section in Use)'] = df_SHOPP_raw_data.apply(uf.calc_SIU_RTL, axis=1)\n",
    "\n",
    "df_SHOPP_raw_data['AM Tool RTL (Section in Use)'].fillna('00', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-actress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "local-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['Advertised Year'] = df_SHOPP_raw_data.apply(\n",
    "    lambda x: x['AM Tool RTL (Section in Use)'] if pd.isnull(x['FY']) else x['FY'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "grand-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data[['AM Tool RTL (Section in Use)','FY','Advertised Year','Last Year of Fiscal Year', 'PID Cycle']].head()\n",
    "# 'Last Year of FY POR', 'Plan Year','RTL Plan Year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "sixth-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last Year of Fiscal Year\n",
    "\n",
    "# cal ='''\n",
    "# FLOAT(Right([Advertised Year],2))\n",
    "# '''\n",
    "\n",
    "df_SHOPP_raw_data['Last Year of Fiscal Year'] = df_SHOPP_raw_data['Advertised Year'].str[-2:].astype(int)+2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "chemical-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['Last Year of Fiscal Year'] != df_SHOPP_raw_data['Last Year FY POR'] ][['AMT_ID','Last Year of Fiscal Year','Last Year FY POR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "adaptive-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == 11296][]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "excessive-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last Year FY POR\n",
    "\n",
    "# cal ='''\n",
    "# FLOAT(Right([AM Tool RTL (Section in Use)],2))\n",
    "# '''\n",
    "\n",
    "df_SHOPP_raw_data['Last Year FY POR'] = df_SHOPP_raw_data['AM Tool RTL (Section in Use)'].str[-2:].astype(int)+2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "attached-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['Last Year FY POR'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-nightlife",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "automated-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Planning or Post-Planning\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# IF(isnull([EFIS Programmed Projects]) and isnull([SHOPP Amendment Date])) Then \"Planning\"\n",
    "# ELSE \"Post-Planning\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "#Loren: is it possible logic does not compare well between different sources? Currently EFIS is converted as integer.\n",
    "\n",
    "ck_col = 'Planning or Post-Planning'\n",
    "def calc_planning_stage(df):\n",
    "    if pd.isnull(df['EFIS_Program']) and pd.isnull(df['SHOPP Amendment Date']):\n",
    "        return 'Planning' \n",
    "        \n",
    "    elif df['SHOPP Amendment Date'] == DD_Approval_Placeholder_Date:\n",
    "        return 'Planning'\n",
    "        \n",
    "    else:\n",
    "        return 'Post-Planning'\n",
    "    \n",
    "df_SHOPP_raw_data[ck_col]= df_SHOPP_raw_data.apply(calc_planning_stage, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "certain-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #UnitTest\n",
    "\n",
    "# AMT_IDs= [20275,\n",
    "# 20277,\n",
    "# 20274,\n",
    "# 20620,\n",
    "# ]\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'].isin(AMT_IDs)][['AMT_ID','EFIS','SHOPP Amendment Date','Dist Dir Appr','Section','Planning or Post-Planning']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-extra",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "jewish-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SB-1 Priority\n",
    "\n",
    "# IF([Activity (group)]=\"SB-1\") Then \"Yes\"\n",
    "# Else \" \"\n",
    "# END\n",
    "\n",
    "\n",
    "df_SHOPP_raw_data['SB-1 Priority'] = df_SHOPP_raw_data['Activity'].apply(uf.calc_SB1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-batch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "pressed-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long Lead?\n",
    "\n",
    "# cal = '''\n",
    "# If([Last Year of Fiscal Year]>24 and [Long Lead]=\"Y\") THEN \"Yes\" Else \"No\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "def verify_longlead(df):\n",
    "    if df['Last Year of Fiscal Year'] > TARGET_FY + 5 and df['Long Lead'] == \"Y\": \n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "df_SHOPP_raw_data['Active Long Lead']= df_SHOPP_raw_data.apply(verify_longlead, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "posted-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df_SHOPP_raw_data['Target RTL FY'].str[-2:].astype(str)\n",
    "\n",
    "# df_SHOPP_raw_data['Requested RTL FY'].value_counts(dropna = False)\n",
    "\n",
    "df_SHOPP_raw_data['PA&ED FY Number'] = df_SHOPP_raw_data.apply(uf.calc_PAED_FY, axis = 1).astype(int)\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['Target RTL FY'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "impressive-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "reflected-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data=df_SHOPP_raw_data_copy\n",
    "\n",
    "# df_SHOPP_raw_data_copy = df_SHOPP_raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "plastic-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_include_5year_POR(df):\n",
    "    if df['Ten-Year Plan RD'] == 9999:\n",
    "        return  'No'\n",
    "    elif(df['Last Year FY POR']>TARGET_FY and df['Last Year FY POR']<TARGET_FY + 6) :   \n",
    "        if df['Activity (group)'] == 'Reservation' and pd.isnull(df['SHOPP Amendment Date']): \n",
    "            return  'Yes' \n",
    "        else: \n",
    "            return 'No' \n",
    "    elif (df['Last Year FY POR']>TARGET_FY + 5 and df['Last Year FY POR']<TARGET_FY + 11) : \n",
    "        if(df['Long Lead'] == \"Y\") and (df['Section'] == \"PRG\")  :\n",
    "             return  'Yes'\n",
    "        elif pd.isnull(df['SHOPP Amendment Date']): \n",
    "             return  'Yes' \n",
    "        else: \n",
    "            return 'No' \n",
    "    else: \n",
    "        return 'No'\n",
    "\n",
    "df_SHOPP_raw_data['Include 5-year POR?'] = df_SHOPP_raw_data.apply(calc_include_5year_POR, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-carrier",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "final-finance",
   "metadata": {},
   "source": [
    "<a id='DataJoining'></a>\n",
    "## Data Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "accessible-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add performance entry counts to raw data \n",
    "#Done: count unique location based on column 'Location'\n",
    "temp = df_perf_raw_data.groupby(['AMT_ID','Section'])['Location'].nunique().reset_index(name = 'perf_entry_count')\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['perf_entry_count'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp, how = 'left', \n",
    "                  left_on = ['AMT_ID','Section'], \n",
    "                  right_on = ['AMT_ID','Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "liable-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Candidate Type to raw data \n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['Candidate Type', 'Advertised Year_Candidate','Project Cost ($K)_Candidate'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, df_shopp_candidate[['AMT_ID','Candidate Type', 'Advertised Year_Candidate','Project Cost ($K)_Candidate']], \n",
    "                             how = 'left', left_on = 'AMT_ID', right_on='AMT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "musical-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add MPO/RTPA to raw data \n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['MPO/RTPA','County Name',],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, df_counties[['Co. Name Abbr.','County Name','MPO/RTPA']].drop_duplicates(), \n",
    "                             how = 'left', left_on = 'County', right_on='Co. Name Abbr.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "contemporary-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find number of drainage worksheet entries for each project\n",
    "temp = df_drain_raw_data.groupby(['AMT_ID','Section'])['AMT_ID'].count().reset_index(name = 'No of Drainage Entries')\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['No of Drainage Entries'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp, how = 'left', \n",
    "                  left_on = ['AMT_ID','Section'], \n",
    "                  right_on = ['AMT_ID','Section'])\n",
    "\n",
    "df_SHOPP_raw_data['No of Drainage Entries'].fillna(0, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-cyprus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "suitable-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data['drainage_in_performance'] = df_perf_raw_data['Performance Objective'].str[:20] == 'Drainage Restoration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "hairy-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_perf_raw_data.groupby(['AMT_ID','Section'])['drainage_in_performance'].agg(max).reset_index()\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['drainage_in_performance'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID','Section','drainage_in_performance']], how = 'left', \n",
    "                  left_on = ['AMT_ID','Section'], \n",
    "                  right_on = ['AMT_ID','Section'])\n",
    "\n",
    "df_SHOPP_raw_data['drainage_in_performance'].fillna(False, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bearing-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data[(df_perf_raw_data['AMT_ID'] == 9000) & (df_perf_raw_data['Section'] == 'PPC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "stuck-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-programming",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "homeless-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add drainage cleaned dates\n",
    "temp = df_drain_raw_data[~df_drain_raw_data['Cleaned date'].isna()]\n",
    "# temp['Cleaned date'] = temp['Cleaned date'].apply(lambda x: x.strftime(\"%m-%d-%Y\"))\n",
    "\n",
    "temp_group = temp.groupby(['AMT_ID','Section',])['Cleaned date'].agg(set).reset_index(name = 'Cleaned Dates')\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['Cleaned Dates'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp_group[['AMT_ID','Section','Cleaned Dates']], \n",
    "                             how = 'left', \n",
    "                  left_on = ['AMT_ID','Section'], \n",
    "                  right_on = ['AMT_ID','Section'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-classroom",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "adaptive-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if performance objective has pavement\n",
    "\n",
    "temp = df_perf_raw_data[df_perf_raw_data['Performance Objective'].str[:8] == \"Pavement\"].groupby(['AMT_ID','Section']).first().reset_index()\n",
    "\n",
    "temp['Performance Objective has Pavement'] = 'Yes'\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['Performance Objective has Pavement'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID','Section','Performance Objective has Pavement']], \n",
    "                             how= 'left', left_on = ['AMT_ID', 'Section'], right_on = ['AMT_ID','Section'])\n",
    "\n",
    "df_SHOPP_raw_data['Performance Objective has Pavement'].fillna('No', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "directed-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data[df_perf_raw_data['Performance Objective'].str[:8] == \"Pavement\"].groupby(['AMT_ID','Section']).agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "functional-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if performance objective has TMS\n",
    "\n",
    "temp =df_perf_raw_data[df_perf_raw_data['Performance Objective'].isin([\"Transportation Management Systems\", 'Transportation Management System Structures'])].groupby(['AMT_ID','Section']).first().reset_index()\n",
    "\n",
    "temp['Performance Objective has TMS'] = 'Yes'\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns =['Performance Objective has TMS'],inplace=True, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID','Section','Performance Objective has TMS']], \n",
    "                             how= 'left', left_on = ['AMT_ID', 'Section'], right_on = ['AMT_ID','Section'])\n",
    "\n",
    "df_SHOPP_raw_data['Performance Objective has TMS'].fillna('No', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "resistant-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add pavement plan year to df_SHOPP_raw_data\n",
    "\n",
    "temp =df_pav_raw_data.groupby(['AMT_ID','Section',])['Plan Year'].agg(Pavement_PlanYear='first', No_Pavement_PlanYear='nunique').reset_index()\n",
    "df_SHOPP_raw_data.drop(columns=['Pavement_PlanYear', 'No_Pavement_PlanYear'], inplace=True, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp, how= 'left', \n",
    "                             left_on = ['AMT_ID', 'Section'], right_on = ['AMT_ID','Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "subject-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp =df_pav_raw_data.groupby(['AMT_ID','Section',])['Plan Year'].agg(Pavement_PlanYears=set, No_Pavement_PlanYear='nunique').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "august-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp[temp['No_Pavement_PlanYear']> 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-writer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "equivalent-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #add pavement plan year to df_SHOPP_raw_data\n",
    "\n",
    "\n",
    "# temp =df_pav_raw_data.groupby(['AMT_ID','Section',])['Plan Year'].agg(Pavement_PlanYears=set, No_Pavement_PlanYear='nunique').reset_index()\n",
    "# df_SHOPP_raw_data.drop(columns=['Pavement_PlanYears', 'No_Pavement_PlanYear'], inplace=True, errors='ignore')\n",
    "\n",
    "# df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp, how= 'left', \n",
    "#                              left_on = ['AMT_ID', 'Section'], right_on = ['AMT_ID','Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "chief-helping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Program', 'District', 'AMT_ID', 'County-Route-PM', 'Section',\n",
       "       'Plan Year', 'District.1', 'County', 'Route', 'RS', 'BPP', 'Beg PM',\n",
       "       'BPS', 'EPP', 'End PM', 'EPS', 'Direction', 'Lane', 'Treatment',\n",
       "       'ActID', 'RoadwayClass', 'TriditionalCondition_Green',\n",
       "       'TriditionalCondition_Yellow', 'TriditionalCondition_Blue',\n",
       "       'TriditionalCondition_Orange', 'TriditionalCondition_Red', 'MAP21_Good',\n",
       "       'MAP21_Fair', 'MAP21_Poor', 'Total LaneMiles', 'SHOPPEffectiveness %',\n",
       "       'RehabEffectiveness %', 'MAP-21Effective-ness %', 'PCRScenarioNo',\n",
       "       'District's Notes', 'Last Saved', 'Saved by', 'Data Year', 'Status'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pav_raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "tight-clear",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s151589\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\pandas\\core\\series.py:4460: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n"
     ]
    }
   ],
   "source": [
    "# add pavement direction check to df_SHOPP_raw_data\n",
    "temp = df_pav_raw_data[df_pav_raw_data['Direction'].isna()]\n",
    "#Question do you mean to catch the project with pavement direction missing? data missing in the original dataset. \n",
    "# Left     1451\n",
    "# Right    1428\n",
    "# NaN        49\n",
    "\n",
    "temp['Direction'].fillna('Direction Info Missing', inplace= True) \n",
    "temp = temp.groupby(['AMT_ID', 'Section'])['Direction'].first().reset_index()\n",
    "temp.columns = ['AMT_ID', 'Section','Direction_check']\n",
    "df_SHOPP_raw_data.drop(columns=['Direction_check'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID', 'Section','Direction_check']], \n",
    "                             how= 'left', left_on = ['AMT_ID','Section'], right_on = ['AMT_ID', 'Section'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "banned-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to get all scenarios of the pavement, how to deal with nan values\n",
    "# skip check for nan data of PCR Scenario\n",
    "df_pav_raw_data['PCRScenarioNo'] = df_pav_raw_data['PCRScenarioNo'].astype(str)\n",
    "temp = df_pav_raw_data[df_pav_raw_data['PCRScenarioNo']!= 'nan'].groupby(['AMT_ID', 'Section'])['PCRScenarioNo'].agg(first_PCRScenarioNo='first', count_PCRScenarioNo='nunique').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "former-locator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMT_ID</th>\n",
       "      <th>Section</th>\n",
       "      <th>first_PCRScenarioNo</th>\n",
       "      <th>count_PCRScenarioNo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>15879</td>\n",
       "      <td>PPC</td>\n",
       "      <td>2671 ( PCR Scenario 2671 Mod #D02R01Y2020 )</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>16476</td>\n",
       "      <td>PPC</td>\n",
       "      <td>3096</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>18329</td>\n",
       "      <td>PRG</td>\n",
       "      <td>2671</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>18437</td>\n",
       "      <td>TYP</td>\n",
       "      <td>3299</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>20054</td>\n",
       "      <td>TYP</td>\n",
       "      <td>3299</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>21441</td>\n",
       "      <td>TYP</td>\n",
       "      <td>3299</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     AMT_ID Section                          first_PCRScenarioNo  \\\n",
       "70    15879     PPC  2671 ( PCR Scenario 2671 Mod #D02R01Y2020 )   \n",
       "114   16476     PPC                                         3096   \n",
       "215   18329     PRG                                         2671   \n",
       "223   18437     TYP                                         3299   \n",
       "507   20054     TYP                                         3299   \n",
       "706   21441     TYP                                         3299   \n",
       "\n",
       "     count_PCRScenarioNo  \n",
       "70                     2  \n",
       "114                    2  \n",
       "215                    2  \n",
       "223                    2  \n",
       "507                    2  \n",
       "706                    2  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temp['PCRScenarioNos'] = temp['PCRScenarioNo'].apply(','.join)\n",
    "# temp['PCRScenarioNo_len'] = temp['PCRScenarioNo'].apply(len)\n",
    "temp[temp['count_PCRScenarioNo']> 1]\n",
    "# df_SHOPP_raw_data.drop(columns=['PCRScenarioNo'],inplace=True , errors='ignore')\n",
    "\n",
    "# df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID', 'Section','PCRScenarioNo']], \n",
    "#                              how= 'left', left_on = ['AMT_ID','Section'], right_on = ['AMT_ID', 'Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "advised-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = df_pav_raw_data.groupby(['AMT_ID', 'Section'])['PCRScenarioNo'].first().reset_index()\n",
    "\n",
    "# df_SHOPP_raw_data.drop(columns=['PCRScenarioNo'],inplace=True , errors='ignore')\n",
    "\n",
    "# df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID', 'Section','PCRScenarioNo']], \n",
    "#                              how= 'left', left_on = ['AMT_ID','Section'], right_on = ['AMT_ID', 'Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "alternative-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data.drop(columns=['first_PCRScenarioNo','count_PCRScenarioNo'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID', 'Section','first_PCRScenarioNo','count_PCRScenarioNo']], \n",
    "                             how= 'left', left_on = ['AMT_ID','Section'], right_on = ['AMT_ID', 'Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-samoa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "intimate-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tms_raw_data['Data Date_TMS'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "shaped-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data date for TMS is unique for each project and section. no need to check all \n",
    "temp =df_tms_raw_data.groupby(['AMT_ID','Section',])['Data Date_TMS'].first().reset_index()\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['Data Date_TMS'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID', 'Section','Data Date_TMS']], \n",
    "                             how= 'left', left_on = ['AMT_ID','Section'], right_on = ['AMT_ID', 'Section'])\n",
    "\n",
    "# 'Data Date_Drainage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "roman-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data date for drainage is unique for each project and section. no need to check al\n",
    "\n",
    "temp =df_drain_raw_data.groupby(['AMT_ID','Section',])['Data Date_Drainage'].first().reset_index()\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['Data Date_Drainage'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID', 'Section','Data Date_Drainage']], \n",
    "                             how= 'left', left_on = ['AMT_ID','Section'], right_on = ['AMT_ID', 'Section'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-coordinate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "textile-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "satisfactory-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data.drop(columns =['Cca Finish Date', 'Cca Percent Comp'],inplace=True, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, df_project_detail[['EFIS','Cca Finish Date', 'Cca Percent Comp']], \n",
    "                             how= 'left', left_on = ['EFIS'], right_on = ['EFIS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "informal-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "preliminary-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp = df_perf_raw_data[(df_perf_raw_data['ActID'] == 'H32') & (~df_perf_raw_data['Quantity'].isna())].groupby(['AMT_ID', 'Section']).first().reset_index()\n",
    "\n",
    "def ck_ActID_H32(df):\n",
    "    if (df['ActID'] == 'H32' and pd.notnull(df['Quantity'])):\n",
    "        return 'OK'\n",
    "    else:\n",
    "        return 'NG'\n",
    "\n",
    "temp['ck_ActID_H32'] = temp.apply(ck_ActID_H32, axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #debug: there are multiple activities for each projec tand section. Needs to create a list or a concat string before join with SHOPP raw data\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns =['ck_ActID_H32'],inplace=True, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID','Section','ck_ActID_H32']], \n",
    "                             how= 'left', left_on = ['AMT_ID', 'Section'], right_on = ['AMT_ID','Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cardiovascular-warrior",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "smooth-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add TMS plan year to df_SHOPP_raw_data\n",
    "\n",
    "temp =df_tms_raw_data.groupby(['AMT_ID','Section',])['RTL Plan Year'].agg(TMS_PlanYear='first', No_TMS_PlanYear='nunique').reset_index()\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['TMS_PlanYear', 'No_TMS_PlanYear'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp, how= 'left', \n",
    "                             left_on = ['AMT_ID', 'Section'], right_on = ['AMT_ID','Section'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "decreased-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add columns from df_PID_workload\n",
    "df_SHOPP_raw_data.drop(columns=['PID Status'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, df_PID_workload[['AMT_ID','PID Status']], \n",
    "                             how = 'left', left_on = 'AMT_ID', right_on='AMT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "filled-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Valid PM to df_SHOPP_raw_data\n",
    "\n",
    "#question to be answered: please confirm: confirmed by manpaul and loren\n",
    "# the Section information is missing for any location other than \"Primary Location\",which cause original logic to skip this row\n",
    "# '''The logic is updated such that it will check the PM for entire project, regardless of the section. If any invalid PM exists in a project, it will be flagged.'''\n",
    "\n",
    "temp =df_pm_check.groupby(['AMT_ID'])['Valid PM'].agg(list).reset_index()\n",
    "temp['PM_Check'] = temp['Valid PM'].apply(lambda x: \"Has Invalid PM\" if 'No' in x else 'OK')\n",
    "\n",
    "df_SHOPP_raw_data.drop(['PM_Check'], axis=1, errors='ignore')\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID','PM_Check']], \n",
    "                                            how = 'left',\n",
    "                                            left_on = ['AMT_ID'],\n",
    "                                            right_on = ['AMT_ID'])\n",
    "df_SHOPP_raw_data['PM_Check'].fillna('OK', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "meaningful-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['PM_Check'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-arthur",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-perspective",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "underlying-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add obsolete project\n",
    "\n",
    "temp =df_project_obselete.groupby(['AMT_ID to obsolete']).first().reset_index()\n",
    "df_SHOPP_raw_data.drop(['AMT_ID to obsolete'], axis=1, errors='ignore')\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID to obsolete']], \n",
    "                how = 'left', left_on = 'AMT_ID', right_on = 'AMT_ID to obsolete')\n",
    "\n",
    "df_SHOPP_raw_data['AMT_ID to obsolete'] = df_SHOPP_raw_data['AMT_ID to obsolete'].apply(lambda x: 'Not Obselete' if pd.isnull(x) else 'Obselete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "final-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a column of program_summary_performance_value_sum\n",
    "\n",
    "temp = df_program_summary.groupby(['AMT_ID','Section',])['Performance Value'].sum().reset_index()\n",
    "dict_rename = {'Performance Value':'Performance Value Sum'}\n",
    "temp = temp.rename(dict_rename, axis = 1)\n",
    "\n",
    "\n",
    "#delete column 'Performance Value Sum' if exists\n",
    "# add 'Performance Value Sum' to raw data via merge\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns =['Performance Value Sum'],inplace=True, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID','Section','Performance Value Sum']], \n",
    "                             how= 'left', left_on = ['AMT_ID', 'Section'], right_on = ['AMT_ID', 'Section'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "optional-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add exception type to each project \n",
    "\n",
    "#question to be answered, should we maintain max one exception for each project entry\n",
    "#This logic extend the type of exception strings\n",
    "temp = df_ck_exceptions.groupby(['AMT_ID'])['Type of Exception'].apply(','.join).reset_index()\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns =['Type of Exception'],inplace=True, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID','Type of Exception']], \n",
    "                             how= 'left', left_on = ['AMT_ID'], right_on = ['AMT_ID'])\n",
    "\n",
    "df_SHOPP_raw_data['Type of Exception'].fillna('No Related Exception', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "friendly-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 'Nickname' into df_SHOPP_raw_data\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, df_project_detail[['EFIS','Nickname']], how ='left', left_on = 'EFIS', right_on = 'EFIS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-cooper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "pressed-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['Shopp Tool Cost to use'] = df_SHOPP_raw_data.apply(uf.calc_SHOPP_tool_cost, axis = 1)\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['Shopp Tool Cost to use'].isna()]\n",
    "\n",
    "df_SHOPP_raw_data['Shopp Tool Cost to use'].fillna(0, inplace= True)\n",
    "\n",
    "#Question: how to handle this 7 null values \n",
    "#mara: fill null with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "lucky-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_SHOPP_raw_data['Total Project Cost ($K)'] = df_SHOPP_raw_data.apply(uf.calc_total_project_cost, axis = 1)\n",
    "\n",
    "# df_SHOPP_raw_data['Total Project Cost ($K)'].isna().any()\n",
    "df_SHOPP_raw_data['Total Project Cost ($K)'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "peaceful-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project Cost ($K)\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# {Fixed [SHOPP ID], [EFIS ],[Date], [Advertised Year], [District]:Max([Total Project Cost ($K)])}\n",
    "# '''\n",
    "\n",
    "#Question: I do not understand this logic, why group SHOPP ID,  EFIS, Advertised Year and District, why get the max\n",
    "#Mara: just get the Total Project Cost ($K) for each SHOPP ID\n",
    "\n",
    "df_SHOPP_raw_data['Project Cost ($K)'] = df_SHOPP_raw_data['Total Project Cost ($K)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "premier-decimal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMT_ID</th>\n",
       "      <th>Section</th>\n",
       "      <th>EFIS</th>\n",
       "      <th>EFIS_Program</th>\n",
       "      <th>Shopp Tool Cost to use</th>\n",
       "      <th>Long Lead</th>\n",
       "      <th>Total Project Cost ($K)</th>\n",
       "      <th>TYP Total Project Cost ($K)</th>\n",
       "      <th>LL PAED Cost ($K)</th>\n",
       "      <th>Project Cost ($K)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4170</th>\n",
       "      <td>22171</td>\n",
       "      <td>CCA</td>\n",
       "      <td>419000284.0</td>\n",
       "      <td>419000284.0</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>4670.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3350.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      AMT_ID Section         EFIS  EFIS_Program  Shopp Tool Cost to use  \\\n",
       "4170   22171     CCA  419000284.0   419000284.0                  3350.0   \n",
       "\n",
       "     Long Lead  Total Project Cost ($K)  TYP Total Project Cost ($K)  \\\n",
       "4170       NaN                   3350.0                       4670.0   \n",
       "\n",
       "      LL PAED Cost ($K)  Project Cost ($K)  \n",
       "4170                0.0             3350.0  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reconcile: the total project cost can not be nan \n",
    "\n",
    "\n",
    "AMT_ID= 22171\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID][['AMT_ID','Section','EFIS','EFIS_Program','Shopp Tool Cost to use',\n",
    "                                                          'Long Lead','Total Project Cost ($K)','TYP Total Project Cost ($K)','LL PAED Cost ($K)','Project Cost ($K)']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "fallen-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12+ np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "developmental-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program[df_program['EFIS_Program'] == 319000045]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "exciting-fitness",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMT_ID\n",
      "Project Book\n",
      "PID Uploaded\n",
      "PIP Uploaded\n",
      "CCE Uploaded\n",
      "Multiple Loc\n",
      "Loc Count\n",
      "Ten-Year Plan RD\n",
      "District\n",
      "County TYP\n",
      "Route TYP\n",
      "BackPM TYP\n",
      "AheadPM TYP\n",
      "Activity\n",
      "Activity Location\n",
      "Target RTL FY\n",
      "LL PAED Cost ($K)\n",
      "HQ PM Conc TYP\n",
      "HQ PAC Review TYP\n",
      "LL RTL FY TYP\n",
      "RW Cost ($K)\n",
      "Const Cost ($K)\n",
      "Support Cost ($)\n",
      "TYP Total Project Cost ($K)\n",
      "EA\n",
      "EFIS\n",
      "PID Cycle\n",
      "PID Type\n",
      "Projected SHOPP Cycle\n",
      "Resourced In PID WP\n",
      "PID Start Date\n",
      "PID Finish Date\n",
      "Project Manager\n",
      "Comment\n",
      "Update Accomp/Perf\n",
      "HQ PM Conc PRG\n",
      "HQ PAC Review PRG\n",
      "County PRG\n",
      "Route PRG\n",
      "BackPM PRG\n",
      "AheadPM PRG\n",
      "Requested SHOPP Cycle\n",
      "PPNO\n",
      "Requested RTL FY\n",
      "Dist Dir Appr\n",
      "SHOPP Amendment Date\n",
      "PAED ($K)\n",
      "PSE ($K)\n",
      "R/W ($K)\n",
      "CONS ($K)\n",
      "Prog Support Cost ($)\n",
      "Prog RW Cost ($K)\n",
      "Prog Const Cost ($K)\n",
      "Prog Total Project Cost ($K)\n",
      "Long Lead\n",
      "LL RTL FY PRG\n",
      "LL PSE ($K)\n",
      "LL Adl RW ($K)\n",
      "LL CONS ($K)\n",
      "LL RW Cap ($K)\n",
      "LL CONS Cap ($K)\n",
      "Total LL Prog ($K)\n",
      "PCR SHOPP Amendment Date\n",
      "PCR Update Accomp/Perf\n",
      "PCR Approval Date\n",
      "Split/ Combine\n",
      "Cross Ref EFIS ID#\n",
      "PCR SHOPP Cycle\n",
      "County PCR\n",
      "Route PCR\n",
      "BackPM PCR\n",
      "AheadPM PCR\n",
      "PCR ID\n",
      "PCR Type\n",
      "PCR Number\n",
      "PCR RTL\n",
      "PCR Activity/Project Location\n",
      "PCR R/W Cap ($K)\n",
      "PCR Const Cap ($K)\n",
      "PCR Support Cost ($K)\n",
      "PCR Total Cost ($K)\n",
      "PCR Comments\n",
      "CCA Date Miilestone (M600)\n",
      "CCA Updated Performance?\n",
      "CCA County\n",
      "CCA Route\n",
      "CCA BackPM\n",
      "CCA AheadPM\n",
      "CCA Comments\n",
      "Date Created\n",
      "Last Saved\n",
      "Saved By\n",
      "Section In Use\n",
      "RTL In Use\n",
      "Project Cost In Use\n",
      "PA&ED Cost\n",
      "Status\n",
      "EA_\n",
      "EFIS_Program\n",
      "FY\n",
      "Total Capital & Support Cost\n",
      "Capital Cost\n",
      "Support Cost\n",
      "Target RTL FY Number\n",
      "Requested RTL FY Number\n",
      "Section\n",
      "County\n",
      "Route\n",
      "Begin Postmile\n",
      "End Postmile\n",
      "Unique EA\n",
      "Activity (group)\n",
      "Activity Book\n",
      "AM Tool RTL (Section in Use)\n",
      "Advertised Year\n",
      "Last Year of Fiscal Year\n",
      "Last Year FY POR\n",
      "Planning or Post-Planning\n",
      "SB-1 Priority\n",
      "Active Long Lead\n",
      "PA&ED FY Number\n",
      "Include 5-year POR?\n",
      "perf_entry_count\n",
      "Candidate Type\n",
      "Advertised Year_Candidate\n",
      "Project Cost ($K)_Candidate\n",
      "Co. Name Abbr.\n",
      "County Name\n",
      "MPO/RTPA\n",
      "No of Drainage Entries\n",
      "drainage_in_performance\n",
      "Cleaned Dates\n",
      "Performance Objective has Pavement\n",
      "Performance Objective has TMS\n",
      "Pavement_PlanYear\n",
      "No_Pavement_PlanYear\n",
      "Direction_check\n",
      "first_PCRScenarioNo\n",
      "count_PCRScenarioNo\n",
      "Data Date_TMS\n",
      "Data Date_Drainage\n",
      "Cca Finish Date\n",
      "Cca Percent Comp\n",
      "ck_ActID_H32\n",
      "TMS_PlanYear\n",
      "No_TMS_PlanYear\n",
      "PID Status\n",
      "PM_Check\n",
      "AMT_ID to obsolete\n",
      "Performance Value Sum\n",
      "Type of Exception\n",
      "Nickname\n",
      "Shopp Tool Cost to use\n",
      "Total Project Cost ($K)\n",
      "Project Cost ($K)\n"
     ]
    }
   ],
   "source": [
    "for c in df_SHOPP_raw_data.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-chambers",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "'HOV Degradation.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-chick",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "exempt-rental",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(285, 36)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total of 285 projects claimed ['Activity Detail']=='DVHD Reduced (201.310)'\n",
    "df_perf_raw_data[(df_perf_raw_data['Activity Detail']=='DVHD Reduced (201.310)')].groupby(['AMT_ID']).first().reset_index().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "robust-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DVHD = df_perf_raw_data[(df_perf_raw_data['Activity Detail']=='DVHD Reduced (201.310)')].groupby(['AMT_ID']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "cleared-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total 8 projects with ['Activity Detail']=='HOV Degradation Mitigation'\n",
    "df_HOV = df_perf_raw_data[df_perf_raw_data['Activity Detail']=='HOV Degradation Mitigation'].groupby(['AMT_ID']).first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "charged-eugene",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AMT_ID', 'District', 'EA', 'EFIS', 'PPNO', 'Location', 'County',\n",
       "       'Route', 'BackPM', 'AheadPM', 'ProjectedRTL FY',\n",
       "       'Main Activity Category', 'Section', 'ActID', 'Perf Activity Category',\n",
       "       'Activity Detail', 'Performance Objective', 'Unit of Measurement',\n",
       "       'Quantity', 'Assets in Good Cond', 'Assets in Fair Cond',\n",
       "       'Assets in Poor Cond', 'New Assets Added', 'Comment', 'Guidance',\n",
       "       'Last Saved', 'Saved By', 'Post-Good', 'Post-Fair', 'Post-Poor',\n",
       "       'HQ ProgramReview - Agree with District?', 'HQ Comment', 'Review Date',\n",
       "       'PerformanceChange Date After Review', 'Status',\n",
       "       'drainage_in_performance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_HOV.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "together-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOV_cols = ['AMT_ID', 'District', 'Unit of Measurement','Quantity', 'Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "appreciated-pickup",
   "metadata": {},
   "outputs": [],
   "source": [
    "DVHD_cols = ['AMT_ID', 'Unit of Measurement','Quantity', 'Comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "substantial-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOPP_cols =['AMT_ID','Activity', 'Activity Location','Total Project Cost ($K)','AM Tool RTL (Section in Use)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "peaceful-bearing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMT_ID</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Activity Location</th>\n",
       "      <th>Total Project Cost ($K)</th>\n",
       "      <th>AM Tool RTL (Section in Use)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [AMT_ID, Activity, Activity Location, Total Project Cost ($K), AM Tool RTL (Section in Use)]\n",
       "Index: []"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == 17139][shopp_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "interracial-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.merge(df_HOV[HOV_cols], df_DVHD[DVHD_cols], \n",
    "                  how = 'left', left_on ='AMT_ID', right_on ='AMT_ID', \n",
    "                 suffixes = ['_HOV', '_DVHD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "senior-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.merge(df_out, df_SHOPP_raw_data[SHOPP_cols], \n",
    "                  how = 'left', left_on ='AMT_ID', right_on ='AMT_ID',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "overhead-membrane",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_out.sort_values('District')[['AMT_ID', 'District',  'Activity', 'Activity Location', 'Unit of Measurement_HOV', 'Quantity_HOV',\n",
    "       'Comment_HOV', 'Unit of Measurement_DVHD', 'Quantity_DVHD',\n",
    "       'Comment_DVHD',\n",
    "       'Total Project Cost ($K)', 'AM Tool RTL (Section in Use)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "canadian-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv('HOV_Summary.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOV_projects = df_perf_raw_data[df_perf_raw_data['Activity Detail']=='HOV Degradation Mitigation'].groupby(['AMT_ID']).first().reset_index()['AMT_ID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-blend",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOV_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "#out of total 8 projects with ['Activity Detail']=='HOV Degradation Mitigation', 5 projects claimed DVHD Reduction\n",
    "df_perf_raw_data[(df_perf_raw_data['AMT_ID'].isin(HOV_projects)) & (df_perf_raw_data['Activity Detail']=='DVHD Reduced (201.310)')].groupby(['AMT_ID']).first().reset_index().AMT_ID.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-recycling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
