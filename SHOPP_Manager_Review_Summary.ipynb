{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "foreign-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-rouge",
   "metadata": {},
   "source": [
    "# Update Notes: \n",
    "\n",
    "### 12-20-2021\n",
    "\n",
    "* change Date -> Data_Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-roots",
   "metadata": {},
   "source": [
    "## TODO\n",
    "needs to change the FY related number to dynamic number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-diagnosis",
   "metadata": {},
   "source": [
    "# Tip for quick search\n",
    "\n",
    "* Needs attention: the place where needs update or better logic\n",
    "* question to be answered: the place where things are still not clear\n",
    "* Manual Check: Unit test where you can drill in to find the data that leads to the check results for a specific project and specific check\n",
    "* TODO: things needs to be done\n",
    "* bookmark: stop point from last visit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-excess",
   "metadata": {},
   "source": [
    "# Admin Notes:\n",
    "\n",
    "\n",
    "1. The AMTool dataset is archived daily as csv files and used for the project book check. \n",
    "The csv files are located at: \n",
    "r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Tableau Dashboards\\DataLake'\n",
    "\n",
    "2. The excel input files are checked daily and archived with datestamp whenever it is modified.\n",
    "The continuously updated excel input files are located at: r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_WorkingFolder\\excel'\n",
    "The excel input file are archived at: r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Tableau Dashboards\\Data_MiscInput'\n",
    "To recover the archived excel file used in project book check for a target date, select the excel file with latest datestamp but is still earlier than the target date.\n",
    "\n",
    "3. The check summary export action is logged daily. It can be used for daily monitoring. \n",
    "The file export log is located at: \\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_WorkingFolder\\output_internal\\log\n",
    "\n",
    "4. The published data are at:\n",
    "\n",
    "    * csv files for district asset manager: http://svgcshopp.dot.ca.gov/DataLake/ProjectBookCheck/\n",
    "    * csv files for HQ AM: \\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_WorkingFolder\\output_internal\n",
    "    * tableau workbook with live data source: https://tableau.dot.ca.gov/#/site/AssetManagement/workbooks/1815/views\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-confidentiality",
   "metadata": {},
   "source": [
    "<a id='TableOfContents'></a>\n",
    "\n",
    "# Table Of Contents\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "### [Global Constants](#GlobalConstants)\n",
    "\n",
    "\n",
    "### [Load and cleanup source data](#Read_Data)\n",
    "\n",
    "* [Counties](#Counties)\n",
    "* [Programming_Summary](#Programming_Summary)\n",
    "* [ProgrammingList](#ProgrammingList)\n",
    "* [SHOPP_Raw_Data](#SHOPP_Raw_Data)\n",
    "* [TenYrShopp_Perf_RawData](#TenYrShopp_Perf_RawData)\n",
    "\n",
    "\n",
    "## Add fields to SHOPP raw data (calculate and join)\n",
    "* [Calculated Fields](#AddDataColumns)\n",
    "* [Join Tables](#DataJoining)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## [Export Projectbook Check Sumary Key Dates](#Export_KeyDates)\n",
    "\n",
    "\n",
    "\n",
    "## [Final Clean Up](#FinalCleanUp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-mozambique",
   "metadata": {},
   "source": [
    "# Import common modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fundamental-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "\n",
    "# import requests\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "small-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "second-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intellectual-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show dataframe without skip column\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acquired-istanbul",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the Extract API 2.0, please save the output as .hyper format\n"
     ]
    }
   ],
   "source": [
    "# from config_datasource import *\n",
    "from projectbookcheck_utilityfunction import *\n",
    "from constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-aspect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "piano-celebrity",
   "metadata": {},
   "source": [
    "# Data clean process\n",
    "\n",
    "* funding amount: remove dollar sign, \n",
    "* fill missing value, string, numerical, \n",
    "* remove leading single quote for string value\n",
    "* strip off leading and trailing space \n",
    "\n",
    "* regulate column names\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-theory",
   "metadata": {},
   "source": [
    "<a id='GlobalConstants'></a>\n",
    "## Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-helena",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sorted-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETDATE = datetime.today().strftime(\"%m-%d-%Y\")\n",
    "CURRENT_FY = fiscalyear (datetime.today())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-ready",
   "metadata": {},
   "source": [
    "<a id='Read_Data'></a>\n",
    "\n",
    "# Read Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "artificial-handy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s151589\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (38) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\s151589\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (29,30,31,32,34) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "if DATA_SOURCE_TYPE == 'csv':\n",
    "\n",
    "    filename = 'TenYrShopp_PerfM_Raw_Data_'\n",
    "    df_perf_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, TARGETDATE))\n",
    "\n",
    "    filename = 'Rawdata_Bridge_Worksheet_'\n",
    "    df_brg_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, TARGETDATE), skiprows = [0], header = 0)\n",
    "\n",
    "    filename = 'Rawdata_Drainage_Worksheet_'\n",
    "    df_drain_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, TARGETDATE), header = 0)\n",
    "\n",
    "    filename = 'Rawdata_TMS_Worksheet_'\n",
    "    df_tms_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, TARGETDATE), header = 0)\n",
    "\n",
    "    filename = 'Rawdata_FishPassage_Worksheet_'\n",
    "    df_fp_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, TARGETDATE), header = 0)\n",
    "    \n",
    "    filename = 'projectbook_draft'\n",
    "    filepath_draft_projectbook = r'{}\\{}.csv'.format(r'http://svgcshopp.dot.ca.gov/DataLake/ProjectBookCheck/', filename)\n",
    "    df_draft_pb = pd.read_csv(filepath_draft_projectbook, header = 0)\n",
    "    \n",
    "    \n",
    "#     filename = 'TenYrShopp_RawData_'\n",
    "#     path_to_file = r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, TARGETDATE)\n",
    "#     t = os.path.getctime(path_to_file)\n",
    "#     DATA_HHMM = int(datetime.fromtimestamp(t).strftime('%H%M'))\n",
    "else:\n",
    "    print('skip getting csv data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-jersey",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "vital-chancellor",
   "metadata": {},
   "source": [
    "## Raw Data Bridge Worksheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "incident-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "#with manual edits\n",
    "\n",
    "dict_rename_bridge_worksheet = {\n",
    " 'ID': 'AMT_ID',\n",
    " 'Bridge â„–': 'BridgeNo',\n",
    " 'Work Type': 'WorkType',\n",
    " 'Brdige / TunnelWork Description': 'WorkDescription',\n",
    " 'Bridge /TunnelHealth Pre': 'Health Pre',\n",
    " 'Bridge /TunnelHealth Post': 'Health Post',\n",
    " 'BridgeScourPre': 'Scour_Pre',\n",
    " 'BridgeScourPost': 'Scour_Post',\n",
    " 'BridgeSeismicPre': 'Seismic_Pre',\n",
    " 'BridgeSeismicPost': 'Seismic_Post',\n",
    " 'BridgeGds MvmtPre': 'GdsMvmt_Pre',\n",
    " 'BridgeGds MvmtPost': 'GdsMvmt_Post',\n",
    " 'Exist(sf)': 'Deck_Exist(sf)',\n",
    " 'Additional(sf)': 'Deck_Additional(sf)',\n",
    " 'Y/N': 'Paint_Y/N',\n",
    " 'Condition': 'Paint_Condition',\n",
    " 'Paint Area(sf)': 'Paint Area(sf)',\n",
    " 'Y/N.1': 'ElectricalMechanical_Y/N',\n",
    " 'Condition.1': 'ElectricalMechanical_Condition',\n",
    " 'Area(sf)': 'ElectricalMechanical_Area(sf)',\n",
    " 'Y/N.2': 'ApproachSlab_Y/N',\n",
    " 'Replaced(sf)': 'ApproachSlab_Replaced(sf)',\n",
    " 'New(sf)': 'ApproachSlab_New(sf)',\n",
    " 'Y/N.3': 'Rail_Y/N',\n",
    " 'Good(lf)': 'Rail_Good(lf)',\n",
    " 'Fair(lf)': 'Rail_Fair(lf)',\n",
    " 'Poor(lf)': 'Rail_Poor(lf)',\n",
    " 'Additonal(lf)': 'Rail_Additonal(lf)',\n",
    " 'Post Good(lf)': 'Rail_Post Good(lf)',\n",
    " 'Post Fair(lf)': 'Rail_Post Fair(lf)',\n",
    " 'Post Poor(lf)': 'Rail_Post Poor(lf)',\n",
    " 'Post New(lf)': 'Rail_Post New(lf)',\n",
    " 'FishPassage(Y/N)': 'FishPassage(Y/N)',\n",
    "}\n",
    "\n",
    "df_brg_raw_data.rename(dict_rename_bridge_worksheet, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "prime-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brg_raw_data.name = 'df_brg_raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dynamic-junction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_brg_raw_data['Rail_Good(lf)'].fillna(0, inplace = True)\n",
    "df_brg_raw_data['Rail_Fair(lf)'].fillna(0, inplace = True)\n",
    "df_brg_raw_data['Rail_Poor(lf)'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "egyptian-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brg_raw_data['Rail_Total(lf)'] = (df_brg_raw_data['Rail_Good(lf)'] \n",
    "                                             + df_brg_raw_data[ 'Rail_Fair(lf)'] \n",
    "                                             + df_brg_raw_data['Rail_Poor(lf)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-webmaster",
   "metadata": {},
   "source": [
    "## Raw Data Drainage Worksheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fifty-median",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drain_raw_data.name = 'df_drain_raw_data'\n",
    "\n",
    "dict_drain_rename = {\n",
    " 'ID': 'AMT_ID',\n",
    " 'Data Date':'Data Date_Drainage'\n",
    "                               }\n",
    "df_drain_raw_data.rename(dict_drain_rename, axis = 1, inplace = True)\n",
    "\n",
    "df_drain_raw_data['EA'] = df_drain_raw_data['EA'].apply(remove_punction)\n",
    "df_drain_raw_data['EFIS'] = df_drain_raw_data['EFIS'].apply(remove_punction)\n",
    "\n",
    "df_drain_raw_data['SYSNO'] = df_drain_raw_data['SYSNO'].apply(remove_punction)\n",
    "df_drain_raw_data['INETNO'] = df_drain_raw_data['INETNO'].apply(remove_punction)\n",
    "df_drain_raw_data['OUTETNO'] = df_drain_raw_data['OUTETNO'].apply(remove_punction)\n",
    "\n",
    "df_drain_raw_data['Data Date_Drainage'] = df_drain_raw_data['Data Date_Drainage'].apply(regulate_timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acute-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_drain_unique_ID(df):\n",
    "    if pd.isnull(df['SYSNO']) or pd.isnull(df['INETNO']) or pd.isnull(df['OUTETNO']):\n",
    "        return None\n",
    "    else:\n",
    "        return (df['SYSNO'] + \"_\"+ df['INETNO'] + \"_\"+ df['OUTETNO'])\n",
    "df_drain_raw_data['Unique Culvert ID'] = df_drain_raw_data.apply(calc_drain_unique_ID, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-threat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "excess-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_drain_raw_data_1[df_drain_raw_data_1['AMT_ID']==22867]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-worst",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "above-mortality",
   "metadata": {},
   "source": [
    "## Raw Data TMS Worksheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "neither-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_TMS_rename = {\n",
    " 'ID': 'AMT_ID',\n",
    " 'Data Date':'Data Date_TMS'\n",
    "                               }\n",
    "df_tms_raw_data.rename(dict_TMS_rename, axis = 1, inplace = True)\n",
    "\n",
    "df_tms_raw_data.name = 'df_tms_raw_data'\n",
    "\n",
    "df_tms_raw_data['Data Date_TMS'] = df_tms_raw_data['Data Date_TMS'].apply(regulate_timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-embassy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "muslim-tanzania",
   "metadata": {},
   "source": [
    "## Raw Data FP Worksheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "prompt-trace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "dict_rename = {\n",
    "    'ID': 'AMT_ID',\n",
    "    'Fish Passage Type(Priority List /Not Priority List)' : 'Fish Passage Type (Priority List / Not Priority List)',\n",
    "    'Should countas addressingFish Passage(Yes/No)?': 'Should count as addressing Fish Passage (Yes/No)?'\n",
    "              }\n",
    "\n",
    "df_fp_raw_data = df_fp_raw_data.rename(dict_rename, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-belief",
   "metadata": {},
   "source": [
    "<a id='TenYrShopp_Perf_RawData'></a>\n",
    "## TenYrShopp_Perf_RawData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "psychological-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "dict_rename_perf_rawdata = {\n",
    "    'ID': 'AMT_ID',\n",
    "              }\n",
    "df_perf_raw_data = df_perf_raw_data.rename(dict_rename_perf_rawdata, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "found-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_strip = ['EA','EFIS','PPNO']\n",
    "for c in cols_strip :\n",
    "    df_perf_raw_data[c] = df_perf_raw_data[c].str.strip(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "sustainable-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data clean \n",
    "#data type regulation\n",
    "\n",
    "df_perf_raw_data['Quantity'] = df_perf_raw_data['Quantity'].fillna(0)\n",
    "df_perf_raw_data['Assets in Good Cond'] = df_perf_raw_data['Assets in Good Cond'].fillna(0)\n",
    "df_perf_raw_data['Assets in Fair Cond'] = df_perf_raw_data['Assets in Fair Cond'].fillna(0)\n",
    "df_perf_raw_data['Assets in Poor Cond'] = df_perf_raw_data['Assets in Poor Cond'].fillna(0)\n",
    "df_perf_raw_data['New Assets Added'] = df_perf_raw_data['New Assets Added'].fillna(0)\n",
    "\n",
    "df_perf_raw_data['EFIS'] = pd.to_numeric(df_perf_raw_data['EFIS'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dedicated-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data trimming\n",
    "#row\n",
    "df_perf_raw_data= df_perf_raw_data[df_perf_raw_data['District'] != 56]\n",
    "#column\n",
    "df_perf_raw_data.drop(['PID Cycle', 'TYP','ProjectedSHOPP Cycle','RequestedRTL FY','DistrictPriority'],\n",
    "  axis='columns', inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sapphire-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data.name = 'df_perf_raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-congo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fluid-partner",
   "metadata": {},
   "source": [
    "# Add columns to performance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "streaming-running",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include only the project in projectbook and active section\n",
    "\n",
    "df_perf_raw_data_1 = pd.merge(df_perf_raw_data,\n",
    "                              df_draft_pb[['AMT_ID', 'Section','Planning or Post-Planning', 'Advertised Year']], \n",
    "                how ='inner', \n",
    "                left_on = ['AMT_ID', 'Section'],\n",
    "                right_on = ['AMT_ID', 'Section'], \n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dietary-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data_1['Post-Fair'].fillna(0, inplace = True)\n",
    "\n",
    "df_perf_raw_data_1['F2G Achieved'] = df_perf_raw_data_1['Post-Fair'] - df_perf_raw_data_1['Assets in Fair Cond']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "identical-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data_1['Post-Poor'].fillna(0, inplace = True)\n",
    "\n",
    "df_perf_raw_data_1['P2G Achieved'] = df_perf_raw_data_1['Post-Poor'] - df_perf_raw_data_1['Assets in Poor Cond']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "systematic-volunteer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rename_performance ={\n",
    "    'New Assets Added':'New Achieved',\n",
    "}\n",
    "\n",
    "df_perf_raw_data_1 = df_perf_raw_data_1.rename(dict_rename_performance, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "american-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data_1['Performance Objective'].fillna('',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-pasta",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "disturbed-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data_1['Concatenate ID+Objective'] = df_perf_raw_data_1['AMT_ID'].astype(str) + df_perf_raw_data_1['Performance Objective']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "suburban-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ck_reviewed(df):\n",
    "    if pd.isna(df['Review Date']):\n",
    "        return 'No'\n",
    "    else:\n",
    "        return 'Yes'\n",
    "        \n",
    "df_perf_raw_data_1['Reviewed?'] = df_perf_raw_data_1.apply(ck_reviewed, axis = 1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "biological-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-vacuum",
   "metadata": {},
   "source": [
    "## common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "inner-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_review_status(df, col_name, NA_msg = 'No relavent data for review'):\n",
    "    \n",
    "    # if there is active entry of \"NO\" or \"New\", no active entry of \"YES\" --> Needs Review\n",
    "    # if there is active entry of \"NO\" or \"New\", and active entry of \"YES\" --> Partially Reviewed\n",
    "    # if there is no active entry of \"NO\" or \"New\", and active entry of \"YES\" --> Review Complete\n",
    "    # if there is no active entry of \"NO\" or \"New\", no active entry of \"YES\" --> No relavent data for review\n",
    "        \n",
    "    if ('No' in df[col_name]) : \n",
    "        if 'Yes' in df[col_name]:\n",
    "            return 'Partially Reviewed'\n",
    "        else:\n",
    "            return 'Needs Review'\n",
    "    elif 'Yes' in df[col_name]:\n",
    "        return 'Review Complete'\n",
    "    elif 'New' in df[col_name]:\n",
    "        return 'All New'\n",
    "    else:\n",
    "        return NA_msg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "enabling-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only projects within project book and active section\n",
    "\n",
    "df_brg_raw_data_1 = pd.merge(df_brg_raw_data, df_draft_pb[['AMT_ID', 'Section']], \n",
    "                             how = 'inner', left_on =['AMT_ID', 'Section'],right_on =['AMT_ID', 'Section'],)\n",
    "\n",
    "#keep only projects within project book and active section\n",
    "\n",
    "df_tms_raw_data_1 = pd.merge(df_tms_raw_data, df_draft_pb[['AMT_ID', 'Section']], \n",
    "                             how = 'inner', left_on =['AMT_ID', 'Section'],right_on =['AMT_ID', 'Section'],)\n",
    "\n",
    "\n",
    "#keep only projects within project book and active section\n",
    "\n",
    "df_drain_raw_data_1 = pd.merge(df_drain_raw_data, df_draft_pb[['AMT_ID', 'Section']], \n",
    "                             how = 'inner', left_on =['AMT_ID', 'Section'],right_on =['AMT_ID', 'Section'],)\n",
    "\n",
    "\n",
    "df_fp_raw_data_1= pd.merge(df_fp_raw_data, df_draft_pb[['AMT_ID', 'Section']], \n",
    "                             how = 'inner', left_on =['AMT_ID', 'Section'],right_on =['AMT_ID', 'Section'],)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-constant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dried-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Bridge WS Health Review Status column to performance raw data\n",
    "def ck_brg_health_data(df):\n",
    "    if pd.isna(df['Health Pre']):\n",
    "        return \"No Bridge Health\"\n",
    "    elif pd.isna(df['Health Post']):\n",
    "        return 'No'\n",
    "    else:\n",
    "        return 'Yes'\n",
    "        \n",
    "df_brg_raw_data_1['Post-Condition for Bridge Health entered?']= df_brg_raw_data_1.apply(ck_brg_health_data, axis = 1)     \n",
    "\n",
    "temp1 = df_brg_raw_data_1.groupby(['AMT_ID', 'Section'])['Post-Condition for Bridge Health entered?'].agg(['unique']).reset_index()\n",
    "\n",
    "temp1['Bridge WS Health Review Status'] = temp1.apply(calc_review_status, args = ['unique'], axis = 1)\n",
    "        \n",
    "df_perf_raw_data_1 = pd.merge(df_perf_raw_data_1, temp1[['AMT_ID', 'Section','Bridge WS Health Review Status']], \n",
    "                              how = 'left', left_on = ['AMT_ID', 'Section'],  right_on = ['AMT_ID', 'Section'])\n",
    "\n",
    "df_perf_raw_data_1['Bridge WS Health Review Status'].fillna('No Bridge Worksheet', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "private-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data_1.groupby(['AMT_ID', 'Section'])['Post-Condition for Bridge Health entered?'].agg(['unique']).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "generic-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "synthetic-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp1 = df_brg_raw_data_1.groupby(['AMT_ID', 'Section'])['Post-Condition for Bridge Health entered?'].agg(['unique']).reset_index()\n",
    "\n",
    "# temp1['Bridge WS Health Review Status'] = temp1.apply(calc_review_status, args = ['unique'], axis = 1)\n",
    "\n",
    "\n",
    "# temp1[temp1['AMT_ID'] ==  11281]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "understood-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add TMS WS Health Review Status column to performance raw data\n",
    "\n",
    "def ck_tms_data(df):\n",
    "    if pd.isna(df['Asset Post-Condition']):\n",
    "        return 'No'\n",
    "    elif df['Asset Post-Condition'] == 'New':\n",
    "        return 'New'\n",
    "    else:\n",
    "        return 'Yes'\n",
    "        \n",
    "df_tms_raw_data_1['Post-Condition entered?']= df_tms_raw_data_1.apply(ck_tms_data, axis = 1)     \n",
    "\n",
    "temp1 = df_tms_raw_data_1.groupby(['AMT_ID', 'Section','TMS Structural or Technology'])['Post-Condition entered?'].agg(['unique']).reset_index()\n",
    "\n",
    "temp1['TMS WS Review Status'] = temp1.apply(calc_review_status, args = ['unique', 'No TMS Worksheet'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-marine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "allied-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1['Performance Objective'] = temp1.apply(lambda df: 'Transportation Management System Structures'  if 'Structures' in df['TMS Structural or Technology'] else np.nan, axis = 1)\n",
    "temp1['TMS Structure Review Status'] = temp1.apply(lambda df: df['TMS WS Review Status']  if 'Structures' in df['TMS Structural or Technology'] else np.nan, axis = 1)\n",
    "df_perf_raw_data_1 = pd.merge(df_perf_raw_data_1, temp1[['AMT_ID', 'Section','Performance Objective','TMS Structure Review Status']], \n",
    "                              how = 'left', left_on = ['AMT_ID', 'Section','Performance Objective'],  right_on = ['AMT_ID', 'Section','Performance Objective'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "freelance-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1['Performance Objective'] = temp1.apply(lambda df: 'Transportation Management Systems' if 'Technology' in df['TMS Structural or Technology'] else np.nan, axis = 1)\n",
    "temp1['TMS Technology Review Status'] = temp1.apply(lambda df: df['TMS WS Review Status']  if 'Technology' in df['TMS Structural or Technology'] else np.nan, axis = 1)\n",
    "df_perf_raw_data_1 = pd.merge(df_perf_raw_data_1, temp1[['AMT_ID', 'Section','Performance Objective','TMS Technology Review Status']], \n",
    "                              how = 'left', left_on = ['AMT_ID', 'Section','Performance Objective'],  right_on = ['AMT_ID', 'Section','Performance Objective'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "foster-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_tms_ws_reviews(df):\n",
    "    '''\n",
    "    combine review status of the two columns of the tms structure and tms technology\n",
    "    '''\n",
    "    if pd.isnull(df['TMS Technology Review Status']):\n",
    "        return df['TMS Structure Review Status']\n",
    "    else:\n",
    "        return df['TMS Technology Review Status']\n",
    "df_perf_raw_data_1['TMS WS Review Status'] = df_perf_raw_data_1.apply(combine_tms_ws_reviews, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "improving-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# df_perf_raw_data_1 = pd.merge(df_perf_raw_data_1, temp1[['AMT_ID', 'Section','TMS Structural or Technology','TMS WS Review Status']], \n",
    "#                               how = 'left', left_on = ['AMT_ID', 'Section'],  right_on = ['AMT_ID', 'Section'])\n",
    "\n",
    "df_perf_raw_data_1['TMS WS Review Status'].fillna('No TMS Worksheet', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "skilled-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tms_raw_data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "conceptual-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data_1['Performance Objective'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "amino-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMT_ID = 19289\n",
    "# df_perf_raw_data_1[(df_perf_raw_data_1['AMT_ID'] == AMT_ID)\n",
    "#                    & (df_perf_raw_data_1['Performance Objective'].isin(['Transportation Management System', 'Transportation Management System Structures']))\n",
    "# #                   & (df_perf_raw_data_1['TMS Structural or Technology'] == 'Technology')\n",
    "                   \n",
    "# #                    & (df_perf_raw_data_1['TMS Structural or Technology'] == 'Technology & Structures')\n",
    "#                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "lesbian-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp1 = df_tms_raw_data_1.groupby(['AMT_ID', 'Section','TMS Structural or Technology'])['Post-Condition entered?'].agg(['unique']).reset_index()\n",
    "\n",
    "# temp1['TMS WS Review Status'] = temp1.apply(calc_review_status, args = ['unique'], axis = 1)\n",
    "\n",
    "# temp1[temp1['AMT_ID'] == AMT_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "twelve-contractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tms_raw_data_1[df_tms_raw_data_1['AMT_ID'] == AMT_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "advance-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_review_status_for_same_performance_objective(df, ws_review_status_col, WS_missing_msg):\n",
    "    \n",
    "    if WS_missing_msg in df[ws_review_status_col] :\n",
    "        return WS_missing_msg\n",
    "    \n",
    "    elif len(df['Reviewed?']) ==1 and ('No' in df['Reviewed?']):\n",
    "        if 'Review Complete' not in df[ws_review_status_col]:\n",
    "            return 'Needs Review'\n",
    "        else: \n",
    "            return 'Partially Reviewed'\n",
    "    else:    # df['Reviewed?'] = 'Yes'\n",
    "        if 'Needs Review' not in df[ws_review_status_col]:\n",
    "            return 'Review Complete'\n",
    "        else:\n",
    "            return 'Partially Reviewed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "banned-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data_2 = df_perf_raw_data_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "stable-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data_1 = df_perf_raw_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "periodic-marathon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-73290855e2b7>:5: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  temp1 = df_perf_raw_data_1[\n"
     ]
    }
   ],
   "source": [
    "performance_objective = 'Transportation Management Systems'\n",
    "ws_review_status_col = 'TMS WS Review Status'\n",
    "combined_ws_review_status_col = 'Combined TMS Technology Review Status'\n",
    "\n",
    "temp1 = df_perf_raw_data_1[\n",
    "    (df_perf_raw_data_1['Performance Objective'] == performance_objective) \n",
    "#     & (df_perf_raw_data_1['TMS Structural or Technology'] == 'Technology')                 \n",
    "                          ].groupby(['AMT_ID', 'Section','Performance Objective'])['Reviewed?', ws_review_status_col ].agg(set).reset_index()\n",
    "\n",
    "temp1[combined_ws_review_status_col] = temp1.apply(\n",
    "    combine_review_status_for_same_performance_objective, \n",
    "    args = [ws_review_status_col,'No TMS Worksheet'], axis = 1)\n",
    "\n",
    "\n",
    "df_perf_raw_data_1 = pd.merge(df_perf_raw_data_1, temp1[['AMT_ID', 'Section', 'Performance Objective',combined_ws_review_status_col]], \n",
    "                              how = 'left', left_on = ['AMT_ID', 'Section','Performance Objective'],  right_on = ['AMT_ID', 'Section','Performance Objective'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "saving-grace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-85585e497dfa>:5: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  temp1 = df_perf_raw_data_1[\n"
     ]
    }
   ],
   "source": [
    "performance_objective = 'Transportation Management System Structures'\n",
    "ws_review_status_col = 'TMS WS Review Status'\n",
    "combined_ws_review_status_col = 'Combined TMS Structures Review Status'\n",
    "\n",
    "temp1 = df_perf_raw_data_1[\n",
    "    (df_perf_raw_data_1['Performance Objective'] == performance_objective)\n",
    "#     &(df_perf_raw_data_1['TMS Structural or Technology'] == 'Technology & Structures')        \n",
    "                          ].groupby(['AMT_ID', 'Section','Performance Objective'])['Reviewed?', ws_review_status_col ].agg(set).reset_index()\n",
    "\n",
    "temp1[combined_ws_review_status_col] = temp1.apply(\n",
    "    combine_review_status_for_same_performance_objective, \n",
    "    args = [ws_review_status_col,'No TMS Worksheet'], axis = 1)\n",
    "\n",
    "\n",
    "df_perf_raw_data_1 = pd.merge(df_perf_raw_data_1, temp1[['AMT_ID', 'Section', 'Performance Objective',combined_ws_review_status_col]], \n",
    "                              how = 'left', left_on = ['AMT_ID', 'Section','Performance Objective'],  right_on = ['AMT_ID', 'Section','Performance Objective'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "under-transaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-44914227c22e>:5: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  temp1 = df_perf_raw_data_1[df_perf_raw_data_1['Performance Objective'] == performance_objective].groupby(['AMT_ID', 'Section', 'Performance Objective'])['Reviewed?', ws_review_status_col ].agg(set).reset_index()\n"
     ]
    }
   ],
   "source": [
    "performance_objective = 'Bridge and Tunnel Health'\n",
    "ws_review_status_col = 'Bridge WS Health Review Status'\n",
    "combined_ws_review_status_col = 'Combined Bridge Health Review Status'\n",
    "\n",
    "temp1 = df_perf_raw_data_1[df_perf_raw_data_1['Performance Objective'] == performance_objective].groupby(['AMT_ID', 'Section', 'Performance Objective'])['Reviewed?', ws_review_status_col ].agg(set).reset_index()\n",
    "\n",
    "temp1[combined_ws_review_status_col] = temp1.apply(\n",
    "    combine_review_status_for_same_performance_objective, \n",
    "    args = [ws_review_status_col, 'No Bridge Worksheet'], axis = 1)\n",
    "\n",
    "\n",
    "df_perf_raw_data_1 = pd.merge(df_perf_raw_data_1, temp1[['AMT_ID', 'Section', combined_ws_review_status_col]], \n",
    "                              how = 'left', left_on = ['AMT_ID', 'Section'],  right_on = ['AMT_ID', 'Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "complimentary-drunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-b6a7f2088214>:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp1[temp1['Combined ActID']!= 'Can not be combined']['Combined ActID Review Status'] = 'Not Applicable'\n"
     ]
    }
   ],
   "source": [
    "#group performance objectives and calculate review status for groupable ACT Ids\n",
    "#summarize review status for each group of [AMT_ID, Section, Combined ActID]\n",
    "\n",
    "def group_target_act_id(df):\n",
    "    '''\n",
    "    CS, SLR, ADA, \n",
    "    '''\n",
    "    if df['ActID'] in ['H05','H06','H08','H13','H21','H33']:\n",
    "        return 'Complete Street'\n",
    "    elif df['ActID'] in ['I19','I20']:\n",
    "        return 'Sea Level Rise'\n",
    "    elif df['ActID'] in ['F21','F22','F23','F24','F25','F26','F27','F28', 'F31', 'F34']:\n",
    "        return 'ADA'    \n",
    "    else:\n",
    "        return 'Can not be combined'\n",
    "    \n",
    "df_perf_raw_data_1['Combined ActID'] = df_perf_raw_data_1.apply(group_target_act_id, axis = 1)     \n",
    "\n",
    "temp1 = df_perf_raw_data_1.groupby(['AMT_ID', 'Section','Combined ActID'])['Reviewed?'].agg(['unique']).reset_index()\n",
    "\n",
    "temp1['Combined ActID Review Status'] = temp1.apply(calc_review_status, args = ['unique'], axis = 1)   \n",
    "\n",
    "temp1[temp1['Combined ActID']!= 'Can not be combined']['Combined ActID Review Status'] = 'Not Applicable'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fifth-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    'Combined ActID': 'Combined Performance Objective',\n",
    "}\n",
    "\n",
    "temp1 = temp1.rename(rename_dict, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-siemens",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "secret-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_combined_act_id(df):\n",
    "    if df['Performance Objective'] in [\"Complete Streets Fix Existing\",\"Complete Streets Build New\"]:\n",
    "        return 'Complete Street'\n",
    "        \n",
    "    elif df['Performance Objective'] in [\"ADA Pedestrian Infrastructure\"]:\n",
    "        return 'ADA'\n",
    "\n",
    "    elif df['Performance Objective'] in [\"Sea Level Rise\"]:\n",
    "        return 'Sea Level Rise'\n",
    "    else:\n",
    "        return 'Can not be combined'\n",
    "\n",
    "    \n",
    "df_perf_raw_data_1['Combined Performance Objective'] = df_perf_raw_data_1.apply(mark_combined_act_id, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "oriental-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data_1 = pd.merge(df_perf_raw_data_1, temp1[['AMT_ID', 'Section', 'Combined Performance Objective','Combined ActID Review Status']], \n",
    "                              how = 'left', left_on = ['AMT_ID', 'Section', 'Combined Performance Objective',],  \n",
    "                              right_on = ['AMT_ID', 'Section', 'Combined Performance Objective',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dated-product",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data_1 ['Combined ActID Review Status'].fillna('No Valid Act ID Data Available', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-milton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "abandoned-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize review status for each group of [AMT_ID, Section, Program Objective]\n",
    "\n",
    "temp = df_perf_raw_data_1.groupby(['AMT_ID', 'Section','Performance Objective'])['Reviewed?'].agg(['unique']).reset_index()\n",
    "\n",
    "temp['Performance Objective Review Status'] = temp.apply(calc_review_status, args = ['unique'], axis = 1)   \n",
    "\n",
    "df_perf_raw_data_1 = pd.merge(df_perf_raw_data_1, temp[['AMT_ID', 'Section', 'Performance Objective','Performance Objective Review Status']], \n",
    "                              how = 'left', left_on = ['AMT_ID', 'Section', 'Performance Objective'],  \n",
    "                              right_on = ['AMT_ID', 'Section', 'Performance Objective'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "floral-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_review_status(df):\n",
    "    if df['Combined Performance Objective'] == 'Can not be combined':\n",
    "        return df['Performance Objective Review Status']\n",
    "    else:\n",
    "        return df['Combined ActID Review Status']\n",
    "\n",
    "df_perf_raw_data_1['Review Status'] = df_perf_raw_data_1.apply(combine_review_status, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "rubber-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data_1['Review Status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "female-locator",
   "metadata": {},
   "source": [
    "## Fish Passage review status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-texture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "informative-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ck_priority_fp_review(df, FP1, FP2):\n",
    "    '''\n",
    "    check if the FP1 column to see if the activity for current data row is within FP priority list\n",
    "    return 'Not in Priority list' if not in the priority list\n",
    "    If it is in the priority list, check FP2 column, to see the data should be counted as priority list \n",
    "    '''\n",
    "    if df[FP1] != 'Priority List':\n",
    "        return \"Not in Priority list\"\n",
    "    elif pd.isna(df[FP2]) : \n",
    "        return 'No'\n",
    "    else:\n",
    "        return 'Yes'\n",
    "\n",
    "def ck_NONpriority_fp_review(df, FP1, FP2):\n",
    "    if df[FP1] == 'Priority List':\n",
    "        return \"In Priority list\"\n",
    "    elif pd.isna(df[FP2]): \n",
    "        return 'No'\n",
    "    else:\n",
    "        return 'Yes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-breakfast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "asian-cemetery",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    'Fish PassagePriority List(Yes/No)': 'Fish Passage Priority List (Yes/No)',\n",
    "    'Is the proposedtreatmentexpected toremediate thefish passagepriority barrier?(Yes/No/NA)': 'Should Count toward Fish Passage Priority List (Yes/No)',\n",
    "    'PriorityIdentifier': 'Priority Identifier',\n",
    "    'AddressingFish Passagenot in the Priority List(Yes/No)': 'Addressing Fish Passage not in the Priority List (Yes/No)?',\n",
    "}\n",
    "\n",
    "df_brg_raw_data_1 = df_brg_raw_data_1.rename(rename_dict, axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "rename_dict = {\n",
    "    'Is the proposed treatment expected to remediate the fish passage priority barrier? (Yes/No/NA)': 'Should Count toward Fish Passage Priority List (Yes/No)',\n",
    "}\n",
    "\n",
    "df_drain_raw_data_1 = df_drain_raw_data_1.rename(rename_dict, axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "rename_dict = {\n",
    "    'Fish Passage Type (Priority List / Not Priority List)': 'Fish Passage Priority List (Yes/No)',\n",
    "    'Should count as addressing Fish Passage (Yes/No)?': 'Should Count toward Fish Passage Priority List (Yes/No)',\n",
    "    'Priority Identifieror FP Identification': 'Priority Identifier',    \n",
    "}\n",
    "\n",
    "df_fp_raw_data_1 = df_fp_raw_data_1.rename(rename_dict, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "respiratory-fighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_brg_raw_data_1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-hearts",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "heard-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "FP1 = 'Fish Passage Priority List (Yes/No)'\n",
    "FP2 = 'Should Count toward Fish Passage Priority List (Yes/No)'\n",
    "\n",
    "df_brg_raw_data_1['Fish Passage in the Priority List'] = df_brg_raw_data_1['Fish Passage Priority List (Yes/No)'].apply(lambda x: 'Yes' if x == 'Yes' else 'No')\n",
    "df_brg_raw_data_1['FP in Priority List reviewed by FP program?'] = df_brg_raw_data_1.apply(ck_priority_fp_review, args = [FP1, FP2], axis = 1)  \n",
    "df_brg_raw_data_1['Fish Passage NOT in the Priority List'] = df_brg_raw_data_1['Addressing Fish Passage not in the Priority List (Yes/No)?'].apply(lambda x: 'Yes' if x == 'Yes' else 'No')\n",
    "df_brg_raw_data_1['FP NOT in Priority List reviewed by FP program?'] = df_brg_raw_data_1.apply(ck_NONpriority_fp_review, args = [FP1, FP2], axis = 1)     \n",
    "\n",
    "\n",
    "df_drain_raw_data_1['Fish Passage in the Priority List'] = df_drain_raw_data_1['Fish Passage Priority List (Yes/No)'].apply(lambda x: 'Yes' if x == 'Yes' else 'No')\n",
    "df_drain_raw_data_1['FP in Priority List reviewed by FP program?'] = df_drain_raw_data_1.apply(ck_priority_fp_review, args = [FP1, FP2], axis = 1)  \n",
    "df_drain_raw_data_1['Fish Passage NOT in the Priority List'] = df_drain_raw_data_1['Addressing Fish Passage not in the Priority List (Yes/No)?'].apply(lambda x: 'Yes' if x == 'Yes' else 'No')\n",
    "df_drain_raw_data_1['FP NOT in Priority List reviewed by FP program?'] = df_drain_raw_data_1.apply(ck_NONpriority_fp_review, args = [FP1, FP2], axis = 1)  \n",
    "\n",
    "\n",
    "df_fp_raw_data_1['Fish Passage in the Priority List'] = df_fp_raw_data_1['Fish Passage Priority List (Yes/No)'].apply(lambda x: 'Yes' if x == 'Priority List' else 'No')\n",
    "df_fp_raw_data_1['FP in Priority List reviewed by FP program?'] = df_fp_raw_data_1.apply(ck_priority_fp_review, args = [FP1, FP2], axis = 1)\n",
    "df_fp_raw_data_1['Fish Passage NOT in the Priority List'] = df_fp_raw_data_1['Fish Passage Priority List (Yes/No)'].apply(lambda x: 'No' if x == 'Priority List' else 'Yes')\n",
    "df_fp_raw_data_1['FP NOT in Priority List reviewed by FP program?'] = df_fp_raw_data_1.apply(ck_NONpriority_fp_review, args = [FP1, FP2], axis = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "actual-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_drain_raw_data_1[df_drain_raw_data_1['AMT_ID']==22867]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "intellectual-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['AMT_ID', 'Section','District','Priority Identifier','Should Count toward Fish Passage Priority List (Yes/No)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "tested-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_brg_raw_data_1[df_brg_raw_data_1['Fish Passage Priority List (Yes/No)'] == 'Yes'][target_cols]\n",
    "temp['Worksheet'] = 'Bridge'\n",
    "\n",
    "temp1 = df_drain_raw_data_1[df_drain_raw_data_1['Fish Passage Priority List (Yes/No)'] == 'Yes'][target_cols]\n",
    "temp1['Worksheet'] = 'Drainage'\n",
    "temp = temp.append(temp1)\n",
    "\n",
    "temp2 = df_fp_raw_data_1[df_fp_raw_data_1['Fish Passage Priority List (Yes/No)'] == 'Priority List'][target_cols]\n",
    "temp2['Worksheet'] = 'Fish Passage'\n",
    "temp = temp.append(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "thrown-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['Priority Identifier'].fillna(0, inplace = True)\n",
    "temp['Priority Identifier'] = temp['Priority Identifier'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "junior-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "current-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['Should Count toward Fish Passage Priority List (Yes/No)'].fillna('Needs Review', inplace = True)\n",
    "\n",
    "temp_group = temp.groupby(['Worksheet','AMT_ID', 'Section', 'District', 'Priority Identifier'])['Should Count toward Fish Passage Priority List (Yes/No)'].agg('value_counts').reset_index(name = 'Counts')\n",
    "\n",
    "df_fp_list = temp_group.pivot(index=['Worksheet','AMT_ID', 'Section', 'District', 'Priority Identifier',], columns='Should Count toward Fish Passage Priority List (Yes/No)', values='Counts').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "caring-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'No' not in df_fp_list.columns:\n",
    "    df_fp_list['No'] = 0\n",
    "\n",
    "if 'Yes' not in df_fp_list.columns:\n",
    "    df_fp_list['Yes'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fossil-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fp_list.fillna(0, inplace = True)\n",
    "\n",
    "df_fp_list['Grand Total'] = df_fp_list['Needs Review'] + df_fp_list['No'] + df_fp_list['Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "isolated-strength",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fp_list = pd.merge(df_fp_list, df_draft_pb[['AMT_ID','Section','EA_','Advertised Year',]],\n",
    "                     how = 'left', left_on = ['AMT_ID','Section',], right_on = ['AMT_ID','Section',]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "distinguished-slovenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fp_list['RTL'] =  df_fp_list['Advertised Year'].apply(lambda x: int(x[-2:]) +2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "clean-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    'EA_': 'EA',\n",
    "    'Needs Review':'Should Count Towards FP Priority List: Needs Review',\n",
    "    'No':'Should Count Towards FP Priority List: No',\n",
    "    'Yes':'Should Count Towards FP Priority List: Yes',\n",
    "    'Grand Total': 'Should Count Towards FP Priority List: Grand Total',\n",
    "}\n",
    "\n",
    "df_fp_list = df_fp_list.rename(rename_dict, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dental-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fp_list['EA'] = df_fp_list['EA'].apply(remove_punction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "elder-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fp_list= df_fp_list[['Worksheet', 'AMT_ID', 'Section', 'District', 'EA','RTL', \n",
    "                        'Priority Identifier',\n",
    "       'Should Count Towards FP Priority List: Needs Review',\n",
    "       'Should Count Towards FP Priority List: No',\n",
    "       'Should Count Towards FP Priority List: Yes',\n",
    "       'Should Count Towards FP Priority List: Grand Total', ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "regulation-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "impossible-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_fp_list.groupby(['Worksheet', 'AMT_ID', 'Section','District', 'EA','RTL',])[['Should Count Towards FP Priority List: Needs Review',\n",
    "       'Should Count Towards FP Priority List: No',\n",
    "       'Should Count Towards FP Priority List: Yes',\n",
    "       'Should Count Towards FP Priority List: Grand Total', ]].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ignored-immunology",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_FP_review_status(df):\n",
    "#     if df['Priority Identifier'] == 0: \n",
    "#         return 'No priority FP identified'\n",
    "    \n",
    "    if df['Should Count Towards FP Priority List: Needs Review'] == 0:\n",
    "        return 'Review Complete'\n",
    "    elif df['Should Count Towards FP Priority List: No'] + df['Should Count Towards FP Priority List: Yes'] == 0:\n",
    "        return 'Needs Review'\n",
    "    else:\n",
    "        return 'Partially Reviewed'\n",
    "temp['Priority FP Review Status'] = temp.apply(calc_FP_review_status, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ecological-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fp_worksheet(df):\n",
    "    if df['Performance Objective'] == 'Fish Passage':\n",
    "        if df['Perf Activity Category'] == 'Sustainability/Climate Change':\n",
    "            return 'Fish Passage'\n",
    "        else:\n",
    "            return df['Perf Activity Category']\n",
    "    else:\n",
    "        return 'NA'\n",
    "    \n",
    "df_perf_raw_data_1['Worksheet'] = df_perf_raw_data_1.apply(calc_fp_worksheet, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "thermal-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data_1 = pd.merge(df_perf_raw_data_1, \n",
    "                              temp[['AMT_ID','Section','Worksheet','Priority FP Review Status']], \n",
    "                              how = 'left', \n",
    "                              left_on = ['AMT_ID','Section','Worksheet'], \n",
    "                              right_on = ['AMT_ID','Section','Worksheet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "becoming-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fp_list.groupby(['AMT_ID','Section','Worksheet'])['AMT_ID'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "thousand-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data_1['Priority FP Review Status'].fillna('No priority FP identified', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "injured-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_review_status_columns(df):\n",
    "    '''\n",
    "    combine review status from different review status columns into one combined review status column\n",
    "    '''\n",
    "    AMT_ID = df['AMT_ID']\n",
    "    Section = df['Section']\n",
    "    \n",
    "    if df['Performance Objective'] == 'Fish Passage':\n",
    "        return df['Priority FP Review Status']\n",
    "    \n",
    "    elif df['Performance Objective'] == \"Bridge and Tunnel Health\":\n",
    "        return df['Combined Bridge Health Review Status']\n",
    "    \n",
    "    elif df['Performance Objective'] == \"Transportation Management Systems\":\n",
    "        return df['Combined TMS Technology Review Status']\n",
    "    \n",
    "    elif df['Performance Objective'] == \"Transportation Management System Structures\":\n",
    "        return df['Combined TMS Structures Review Status']        \n",
    "    \n",
    "    else:\n",
    "        return df['Review Status'] \n",
    "    \n",
    "df_perf_raw_data_1['Combined Review Status'] = df_perf_raw_data_1.apply(combine_review_status_columns, axis = 1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-norfolk",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "incorrect-birthday",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AMT_ID = 18672 \n",
    "# Section = 'PRG'\n",
    "\n",
    "# df_perf_raw_data_1[(df_perf_raw_data_1['AMT_ID'] == AMT_ID) & (df_perf_raw_data_1['Section'] == Section)\n",
    "#                    & (df_perf_raw_data_1['Performance Objective'] == 'Complete Streets Fix Existing')\n",
    "#                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "complimentary-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMT_ID = 18672 \n",
    "# Section = 'PRG'\n",
    "\n",
    "# df_perf_raw_data_1[(df_perf_raw_data_1['AMT_ID'] == AMT_ID) & (df_perf_raw_data_1['Section'] == Section)\n",
    "#                    & (df_perf_raw_data_1['Combined ActID'] == \"Complete Street\")\n",
    "#                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "coral-blink",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ck_HQ_review_complete(df):\n",
    "    # if modified after review, return Needs Review\n",
    "    if pd.notna(df['PerformanceChange Date After Review']):\n",
    "        return 'Needs Re-review'\n",
    "    else: #\n",
    "        return df['Combined Review Status']\n",
    "    \n",
    "     \n",
    "df_perf_raw_data_1['Is HQ Review Complete?'] = df_perf_raw_data_1.apply(ck_HQ_review_complete, axis = 1)   \n",
    "\n",
    "df_perf_raw_data_1['Is HQ Review Complete?'].fillna('No Need for Review', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-sending",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "necessary-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data_1['Date'] = TARGETDATE\n",
    "\n",
    "df_perf_raw_data_1['PerformanceChange Date After Review'].fillna('NA', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-adolescent",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-friday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "indonesian-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DEBUG: Bridge Health\n",
    "# # AMT_ID = 13550\n",
    "# AMT_ID = 23253\n",
    "# AMT_ID = 21974\n",
    "\n",
    "# df_perf_raw_data_1[\n",
    "#     (df_perf_raw_data_1['AMT_ID'] == AMT_ID)\n",
    "#     & (df_perf_raw_data_1['Performance Objective'].isin([\"Bridge and Tunnel Health\"]))\n",
    "# #     & (df_perf_raw_data_1['Performance Objective'] == 'No Performance Objective in the SHSMP')\n",
    "# ][['AMT_ID','Section','ActID','Review Date','Reviewed?','Worksheet','Combined ActID','Bridge WS Health Review Status','Is HQ Review Complete?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "anticipated-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_brg_raw_data_1[df_brg_raw_data_1['AMT_ID'] == AMT_ID][['AMT_ID', 'Section','Health Pre','Health Post','Post-Condition for Bridge Health entered?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "matched-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DEBUG: TMS Technology\n",
    "\n",
    "# AMT_ID = 15955   # No TMS worksheet\n",
    "# AMT_ID = 21663   # Partially Reviewed\n",
    "# # AMT_ID = 19289  # Needs Review\n",
    "# AMT_ID = 19939   # Partially Reviewed\n",
    "# df_perf_raw_data_1[\n",
    "#     (df_perf_raw_data_1['AMT_ID'] == AMT_ID)\n",
    "#     & (df_perf_raw_data_1['Performance Objective']==\"Transportation Management Systems\")\n",
    "# #     & (df_perf_raw_data_1['Performance Objective'] == 'No Performance Objective in the SHSMP')\n",
    "# ][['AMT_ID','Section','ActID','Review Date','Reviewed?','Worksheet','Performance Objective','TMS WS Review Status','Combined TMS Technology Review Status','Is HQ Review Complete?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "enhanced-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DEBUG: TMS Structures\n",
    "# AMT_ID = 19543   # Partially Reviewed\n",
    "\n",
    "# df_perf_raw_data_1[\n",
    "#     (df_perf_raw_data_1['AMT_ID'] == AMT_ID)\n",
    "#     & (df_perf_raw_data_1['Performance Objective'].isin([\"Transportation Management System Structures\"]))\n",
    "# #     & (df_perf_raw_data_1['Performance Objective'] == 'No Performance Objective in the SHSMP')\n",
    "# ][['AMT_ID','Section','ActID','Review Date','Reviewed?','Worksheet','Performance Objective','TMS WS Review Status','Combined TMS Technology Review Status','Combined TMS Structures Review Status','Is HQ Review Complete?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "lightweight-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tms_raw_data_1[(df_tms_raw_data_1['AMT_ID'] == AMT_ID)]\n",
    "# [['AMT_ID','Section','TMS Structural or Technology','Post-Condition entered?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "inappropriate-fisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DEBUG: Complete Street\n",
    "\n",
    "# AMT_ID = 20245\n",
    "# df_perf_raw_data_1[\n",
    "#     (df_perf_raw_data_1['AMT_ID'] == AMT_ID)\n",
    "# #     & (df_perf_raw_data_1['Performance Objective'].isin([\"Complete Streets Fix Existing\",\"Complete Streets Build New\"]))\n",
    "# #     & (df_perf_raw_data_1['Performance Objective'] == 'No Performance Objective in the SHSMP')\n",
    "# ][['AMT_ID','Section','ActID','Review Date','Reviewed?','Worksheet',\n",
    "#    'Combined ActID','Combined ActID Review Status','Performance Objective','Performance Objective Review Status','Review Status','Is HQ Review Complete?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bronze-testimony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMT_ID</th>\n",
       "      <th>Section</th>\n",
       "      <th>ActID</th>\n",
       "      <th>Review Date</th>\n",
       "      <th>Reviewed?</th>\n",
       "      <th>Worksheet</th>\n",
       "      <th>Combined ActID</th>\n",
       "      <th>Priority FP Review Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7981</th>\n",
       "      <td>22867</td>\n",
       "      <td>TYP</td>\n",
       "      <td>C17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>Drainage</td>\n",
       "      <td>Can not be combined</td>\n",
       "      <td>Partially Reviewed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7986</th>\n",
       "      <td>22867</td>\n",
       "      <td>TYP</td>\n",
       "      <td>I17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>Fish Passage</td>\n",
       "      <td>Can not be combined</td>\n",
       "      <td>Review Complete</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      AMT_ID Section ActID Review Date Reviewed?     Worksheet  \\\n",
       "7981   22867     TYP   C17         NaN        No      Drainage   \n",
       "7986   22867     TYP   I17         NaN        No  Fish Passage   \n",
       "\n",
       "           Combined ActID Priority FP Review Status  \n",
       "7981  Can not be combined        Partially Reviewed  \n",
       "7986  Can not be combined           Review Complete  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DEBUG Fish Passage\n",
    "\n",
    "AMT_ID = 22867\n",
    "# AMT_ID = 20275\n",
    "df_perf_raw_data_1[\n",
    "    (df_perf_raw_data_1['AMT_ID'] == AMT_ID)\n",
    "    & (df_perf_raw_data_1['Performance Objective'] == 'Fish Passage')\n",
    "#     & (df_perf_raw_data_1['Performance Objective'] == 'No Performance Objective in the SHSMP')\n",
    "][['AMT_ID','Section','ActID','Review Date','Reviewed?','Worksheet','Combined ActID','Priority FP Review Status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "residential-moses",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fp_list[df_fp_list['AMT_ID'] == AMT_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "hawaiian-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data_1[\n",
    "#     (df_perf_raw_data_1['AMT_ID'] == 20240)\n",
    "#     & (df_perf_raw_data_1['Performance Objective'] == 'Complete Streets Fix Existing')\n",
    "# #     & (df_perf_raw_data_1['Performance Objective'] == 'No Performance Objective in the SHSMP')\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-freeze",
   "metadata": {},
   "source": [
    "<a id='Export_FaceSheets'></a>\n",
    "\n",
    "# Export Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-tennessee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "romance-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drain_raw_data_1['Should Count toward Fish Passage not in the Priority List (Yes/No)'].fillna('N/A', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "defensive-saskatchewan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting existing hyper file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "processing table: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'Extract' does not exist in extract bridge_worksheet.hyper, creating.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing table: 888it [00:00, 6209.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signing into AssetManagement at https://tableau.dot.ca.gov\n",
      "Publishing bridge_worksheet.hyper to Sandbox_ProjectBookCheck_Automation...\n",
      "deleting existing hyper file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing table: 566it [00:00, 5659.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'Extract' does not exist in extract drainage_worksheet.hyper, creating.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing table: 11243it [00:02, 5510.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signing into AssetManagement at https://tableau.dot.ca.gov\n",
      "Publishing drainage_worksheet.hyper to Sandbox_ProjectBookCheck_Automation...\n",
      "deleting existing hyper file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing table: 5it [00:00, 5000.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'Extract' does not exist in extract fishpassage_worksheet.hyper, creating.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signing into AssetManagement at https://tableau.dot.ca.gov\n",
      "Publishing fishpassage_worksheet.hyper to Sandbox_ProjectBookCheck_Automation...\n",
      "deleting existing hyper file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing table: 26it [00:00, 8681.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'Extract' does not exist in extract fishpassage_list.hyper, creating.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signing into AssetManagement at https://tableau.dot.ca.gov\n",
      "Publishing fishpassage_list.hyper to Sandbox_ProjectBookCheck_Automation...\n"
     ]
    }
   ],
   "source": [
    "df_brg_raw_data_1['Data_Date'] = TARGETDATE\n",
    "df_drain_raw_data_1['Data_Date'] = TARGETDATE\n",
    "df_fp_raw_data_1['Data_Date'] = TARGETDATE\n",
    "df_fp_list['Data_Date'] = TARGETDATE\n",
    "\n",
    "export_data(df_brg_raw_data_1, 'bridge_worksheet', PROJECTBOOKCHECK_HTTPSEVER_FOLDER, LOG_FILE)\n",
    "\n",
    "export_data(df_drain_raw_data_1, 'drainage_worksheet', PROJECTBOOKCHECK_HTTPSEVER_FOLDER, LOG_FILE)\n",
    "\n",
    "export_data(df_fp_raw_data_1, 'fishpassage_worksheet', PROJECTBOOKCHECK_HTTPSEVER_FOLDER, LOG_FILE)\n",
    "\n",
    "export_data(df_fp_list, 'fishpassage_list', PROJECTBOOKCHECK_HTTPSEVER_FOLDER, LOG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "alternate-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_quantity(df):\n",
    "    '''\n",
    "    get the numerical rows of the quanity, assign zero for non-numerical rows\n",
    "    '''\n",
    "    try: \n",
    "        return float(df['Quantity'])\n",
    "    except:\n",
    "        if df['Quantity'] == 'Yes':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "df_perf_raw_data_1['Quantity_Number'] = df_perf_raw_data_1.apply(convert_quantity, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "balanced-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for v in df_perf_raw_data_1['Quantity'].values:\n",
    "#     try: \n",
    "#         _ = float(v)\n",
    "#     except:\n",
    "#         print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fallen-worker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting existing hyper file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing table: 534it [00:00, 5339.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'Extract' does not exist in extract performance_review_summary.hyper, creating.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing table: 20407it [00:03, 5320.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signing into AssetManagement at https://tableau.dot.ca.gov\n",
      "Publishing performance_review_summary.hyper to Sandbox_ProjectBookCheck_Automation...\n"
     ]
    }
   ],
   "source": [
    "# export performance raw data with review summary\n",
    "\n",
    "out_cols = [\n",
    "    'District', 'AMT_ID', 'EA', 'EFIS', 'PPNO', 'Location', 'County',\n",
    "       'Route', 'BackPM', 'AheadPM', 'ProjectedRTL FY',\n",
    "       'Main Activity Category', 'Section', 'ActID', 'Perf Activity Category',\n",
    "       'Activity Detail', 'Performance Objective', 'Unit of Measurement',\n",
    "       'Quantity_Number', 'Assets in Good Cond', 'Assets in Fair Cond',\n",
    "       'Assets in Poor Cond', 'New Achieved', 'Comment', 'Guidance',\n",
    "       'Last Saved', 'Saved By', 'Post-Good', 'Post-Fair', 'Post-Poor',\n",
    "       'HQ ProgramReview - Agree with District?', 'HQ Comment', 'Review Date',\n",
    "       'PerformanceChange Date After Review', 'Status','Concatenate ID+Objective',\n",
    "       'Planning or Post-Planning', 'Advertised Year', 'F2G Achieved',\n",
    "       'P2G Achieved', \n",
    "       'Is HQ Review Complete?', 'Data_Date', 'Combined ActID','Combined Performance Objective']\n",
    "\n",
    "export_data(df_perf_raw_data_1[out_cols], 'performance_review_summary', PROJECTBOOKCHECK_HTTPSEVER_FOLDER, LOG_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "comic-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data_1['Activity Detail'].unique()\n",
    "\n",
    "# df_perf_raw_data_1[\n",
    "#     (df_perf_raw_data_1['Activity Detail'] == 'Abandon/Remove Culvert (201.151)')\n",
    "#     & (df_perf_raw_data_1['ProjectedRTL FY'] == '2021/22')\n",
    "#     & (df_perf_raw_data_1['Unit of Measurement'] == 'Each')\n",
    "#                   ]['Quantity'].astype(float).sum()\n",
    "\n",
    "# df_perf_raw_data_1['ProjectedRTL FY'].unique()\n",
    "\n",
    "# df_perf_raw_data_1[\n",
    "#     (df_perf_raw_data_1['Activity Detail'] == 'Abandon/Remove Culvert (201.151)')\n",
    "#                   ]\n",
    "\n",
    "# df_perf_raw_data_1[df_perf_raw_data_1['AMT_ID'] == 11358][['AMT_ID', \n",
    "#        'Main Activity Category', 'Section', 'ActID', 'Perf Activity Category',\n",
    "#        'Activity Detail', 'Performance Objective','Unit of Measurement',\n",
    "#        'Quantity','Assets in Good Cond', 'Assets in Fair Cond',\n",
    "#        'Assets in Poor Cond', 'New Achieved']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-isaac",
   "metadata": {},
   "source": [
    "\n",
    "<a id='FinalCleanUp'></a>\n",
    "## Final Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-machine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "understanding-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#clean up tableau publishing log file\n",
    "\n",
    "import os\n",
    "import glob\n",
    "# get a recursive list of file paths that matches pattern\n",
    "fileList = glob.glob('./*.log')\n",
    "# Iterate over the list of filepaths & remove each file.\n",
    "for filePath in fileList:\n",
    "    try:\n",
    "        os.remove(filePath)\n",
    "    except OSError:\n",
    "        print(\"Error while deleting file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "durable-sponsorship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed : 60.64303469657898 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time =  time.time()\n",
    "elapsed = end_time - start_time\n",
    "print('time elapsed : {} seconds'.format(elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-consultancy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-baghdad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
