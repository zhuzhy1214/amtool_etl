{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "final-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-diagnosis",
   "metadata": {},
   "source": [
    "# Tip for quick search\n",
    "\n",
    "* Needs attention: the place where needs update or better logic\n",
    "* question to be answered: the place where things are still not clear\n",
    "* Unit Test: Unit test where you can drill in to find the data that leads to the check results for a specific project and specific check\n",
    "* TODO: things needs to be done\n",
    "* bookmark: stop point from last visit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-hawaii",
   "metadata": {},
   "source": [
    "# Update Note: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-excess",
   "metadata": {},
   "source": [
    "# Admin Notes:\n",
    "\n",
    "\n",
    "1. The AMTool dataset is archived daily as csv files and used for the project book check. \n",
    "The csv files are located at: \n",
    "r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Tableau Dashboards\\DataLake'\n",
    "\n",
    "2. The excel input files are checked daily and archived with datestamp whenever it is modified.\n",
    "The continuously updated excel input files are located at: r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_WorkingFolder\\excel'\n",
    "The excel input file are archived at: r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Tableau Dashboards\\Data_MiscInput'\n",
    "To recover the archived excel file used in project book check for a target date, select the excel file with latest datestamp but is still earlier than the target date.\n",
    "\n",
    "3. The check summary export action is logged daily. It can be used for daily monitoring. \n",
    "The file export log is located at: \\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_WorkingFolder\\output_internal\\log\n",
    "\n",
    "4. The published data are at:\n",
    "\n",
    "    * csv files for district asset manager: http://svgcshopp.dot.ca.gov/DataLake/ProjectBookCheck/\n",
    "    * csv files for HQ AM: \\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_WorkingFolder\\output_internal\n",
    "    * tableau workbook with live data source: https://tableau.dot.ca.gov/#/site/AssetManagement/workbooks/1815/views\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-confidentiality",
   "metadata": {},
   "source": [
    "<a id='TableOfContents'></a>\n",
    "\n",
    "# Table Of Contents\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "### [Global Constants](#GlobalConstants)\n",
    "\n",
    "\n",
    "### [Load and cleanup source data](#Read_Data)\n",
    "\n",
    "* [Bridge_Inventory](#Bridge_Inventory)\n",
    "* [Bridge_Worksheet](#Bridge_Worksheet)\n",
    "* [Check_Exceptions](#Check_Exceptions)\n",
    "* [Counties](#Counties)\n",
    "* [Drainage_Worksheet](#Drainage_Worksheet)\n",
    "* [Minor_Raw_Data](#Minor_Raw_Data)\n",
    "* [Pavement_Worksheet](#Pavement_Worksheet)\n",
    "* [PID_Workload](#PID_Workload)\n",
    "* [Postmile_Check](#Postmile_Check)\n",
    "* [Programming_Summary](#Programming_Summary)\n",
    "* [ProgrammingList](#ProgrammingList)\n",
    "* [Project_Detail_Report](#Project_Detail_Report)\n",
    "* [Project_Obselete](#Project_Obselete)\n",
    "* [SHOPP_Candidates](#SHOPP_Candidates)\n",
    "* [SHOPP_Raw_Data](#SHOPP_Raw_Data)\n",
    "* [TenYrShopp_Perf_RawData](#TenYrShopp_Perf_RawData)\n",
    "* [TMS_Worksheet](#TMS_Worksheet)\n",
    "\n",
    "\n",
    "## Add fields to SHOPP raw data (calculate and join)\n",
    "* [Calculated Fields](#AddDataColumns)\n",
    "* [Join Tables](#DataJoining)\n",
    "\n",
    "\n",
    "\n",
    "## Data Check and Export\n",
    "\n",
    "\n",
    "## [Data Check List](#Issue_Table1)\n",
    "The main table of check issues, \n",
    "one issue per row, \n",
    "\n",
    "\n",
    "\n",
    "## [New Checks](#NewChecks)\n",
    "* 'Check Safety Comment'\n",
    "#Check comments about ActID of E55 and E58\n",
    "#if ActID == E55 and E58 and the comments include: 'HQ added the activity and District needs to review it.', mark the project\n",
    "* 'TMS data update needed for the project?'\n",
    "\n",
    "## [Export repeated assets](#Export_repeated_assets)\n",
    "* repeated assets (csv, tableau datasource)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## [Final Clean Up](#FinalCleanUp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-mozambique",
   "metadata": {},
   "source": [
    "# Import common modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fundamental-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "\n",
    "# import requests\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "second-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intellectual-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show dataframe without skip column\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acquired-istanbul",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the Extract API 2.0, please save the output as .hyper format\n"
     ]
    }
   ],
   "source": [
    "# from config_datasource import *\n",
    "# from projectbookcheck_utilityfunction import *\n",
    "from constants import *\n",
    "import projectbookcheck_utilityfunction as uf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-aspect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "comic-disaster",
   "metadata": {},
   "source": [
    "# General Approach\n",
    "\n",
    "use SHOPP raw data as basis for data checks. \n",
    "Each project only occupies one line\n",
    "\n",
    "can expand columns, only if it will not create duplicate rows in the SHOPP raw dataset. \n",
    "\n",
    "\n",
    "Question: what is the entire list of projects, how to include projects without AMT_ID (SHOPP ID)\n",
    "Ans: use raw data, check missing SHOPP ID seperately \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-museum",
   "metadata": {},
   "source": [
    "# Data update procedure\n",
    "\n",
    "data schema: name, data type, default value if missing\n",
    "\n",
    "Option 1: Keep Excel column header fixed (number and sequence)\n",
    "\n",
    "Option 2: Maintain a dictionary of Excel column name and dataframe column name\n",
    "\n",
    "\n",
    "\n",
    "# Check update procedure\n",
    "\n",
    "Create utility function in python, in seperate module\n",
    "add the additional check in the main ETL module\n",
    "update the final data visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-celebrity",
   "metadata": {},
   "source": [
    "# Data clean process\n",
    "\n",
    "* funding amount: remove dollar sign, \n",
    "* fill missing value, string, numerical, \n",
    "* remove leading single quote for string value\n",
    "* strip off leading and trailing space \n",
    "\n",
    "* regulate column names\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-theory",
   "metadata": {},
   "source": [
    "<a id='GlobalConstants'></a>\n",
    "## Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sorted-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_FY = uf.fiscalyear(datetime.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distinct-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#override to get the live data\n",
    "# DATA_SOURCE_TYPE = 'live'\n",
    "\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "\n",
    "# PROJECTBOOKCHECK_INPUT_FOLDER = r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_DataChecksSupport\\dev\\input'\n",
    "# PROJECTBOOKCHECK_HTTPSERVER_FOLDER = r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_DataChecksSupport\\dev\\output'\n",
    "# PROJECTBOOKCHECK_OUTPUT_FOLDER = r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_DataChecksSupport\\dev\\output'\n",
    "\n",
    "# LOG_FILE = r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_DataChecksSupport\\dev\\log\\ProjectBookExport.log'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-ready",
   "metadata": {},
   "source": [
    "<a id='Read_Data'></a>\n",
    "\n",
    "# Read Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "artificial-handy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s151589\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (8) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "C:\\Users\\s151589\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (29,30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "File_TimeStamp = '' #always read the latest timestamped file\n",
    "\n",
    "if DATA_SOURCE_TYPE == 'csv':\n",
    "    filename = 'TenYrShopp_RawData_'\n",
    "    df_SHOPP_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp))\n",
    "\n",
    "    filename = 'TenYrShopp_PerfM_Raw_Data_'\n",
    "    df_perf_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp))\n",
    "\n",
    "    filename = 'Rawdata_Bridge_Worksheet_'\n",
    "    df_brg_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), skiprows = [0], header = 0)\n",
    "\n",
    "    filename = 'Rawdata_Pavement_Worksheet_'\n",
    "    df_pav_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), skiprows = [0], header = 1)\n",
    "\n",
    "    filename = 'Rawdata_Drainage_Worksheet_'\n",
    "    df_drain_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), header = 0)\n",
    "\n",
    "    filename = 'Rawdata_TMS_Worksheet_'\n",
    "    df_tms_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), header = 0)\n",
    "\n",
    "    filename = 'Programming_Summary_'\n",
    "    df_program_summary = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), header = 0)\n",
    "\n",
    "    filename = 'Minor_Project_Details_Raw_Data_'\n",
    "    df_Minor_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp))\n",
    "\n",
    "    filename = 'HM_Project_Details_Raw_Data_'\n",
    "    df_HM_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp))\n",
    "\n",
    "    filename = 'Minor_Rawdata_TMS_Worksheet_'\n",
    "    df_Minor_tms_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), header = 0) \n",
    "\n",
    "    filename = 'HM_Rawdata_TMS_Worksheet_'\n",
    "    df_HM_tms_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), header = 0) \n",
    "    \n",
    "    path_to_file = r'{}\\{}.csv'.format(DATALAKE_FOLDER, filename)\n",
    "    t = os.path.getmtime(path_to_file)\n",
    "    Data_TimeStamp = datetime.fromtimestamp(t).strftime(\"%m-%d-%Y %H:%M:%S\")\n",
    "    TARGETDATE = datetime.fromtimestamp(t).strftime(\"%m-%d-%Y\")\n",
    "    \n",
    "else:\n",
    "    print('skip getting csv data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-stack",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "auburn-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'TMS_Life_Cycle.xlsx'\n",
    "\n",
    "df_TMS_LifeCycle = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "superb-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Project Detail Report.csv'\n",
    "\n",
    "df_Project_Details = pd.read_csv(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename)) \n",
    "\n",
    "dict_rename = {\n",
    "    'Project ID': 'EFIS',\n",
    "    }\n",
    "df_Project_Details= df_Project_Details.rename(dict_rename, axis = 1)\n",
    "\n",
    "df_Project_Details['EFIS'] = pd.to_numeric(df_Project_Details['EFIS'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "signal-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Minor Approved list.xlsx'\n",
    "\n",
    "df_Minor_ApprovedList = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename)) \n",
    "\n",
    "dict_rename = {\n",
    "    'Project ID': 'EFIS',\n",
    "    }\n",
    "df_Minor_ApprovedList = df_Minor_ApprovedList.rename(dict_rename, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-magazine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "charming-handling",
   "metadata": {},
   "source": [
    "<a id='Minor_Raw_Data'></a>\n",
    "\n",
    "## Minor and HM Raw Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "solid-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_Minor_raw_data['District'] = df_Minor_raw_data['District'].apply(uf.remove_punction)\n",
    "df_Minor_raw_data['District'] = df_Minor_raw_data['District'].astype(int)\n",
    "\n",
    "df_Minor_raw_data['Unique EA'] = df_Minor_raw_data.apply(uf.calc_unique_EA, axis = 1)\n",
    "\n",
    "dict_rename = {'ID': 'AMT_ID',\n",
    "    'Project ID':'EFIS',\n",
    "    'FY.2': 'RTL'    \n",
    "              }\n",
    "df_Minor_raw_data = df_Minor_raw_data.rename(dict_rename, axis = 1)\n",
    "\n",
    "#conver EFIS to numeric, \n",
    "#filter null and 0 \n",
    "\n",
    "\n",
    "df_Minor_raw_data['EFIS'] = pd.to_numeric(df_Minor_raw_data['EFIS'], errors='coerce')\n",
    "df_Minor_raw_data = df_Minor_raw_data[(df_Minor_raw_data['EFIS'].notna()) & df_Minor_raw_data['EFIS'] != 0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "operational-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_raw_data['Unique EA'] = df_HM_raw_data.apply(uf.calc_unique_EA, axis = 1)\n",
    "\n",
    "df_HM_raw_data['EFIS'] = pd.to_numeric(df_HM_raw_data['EFIS'], errors='coerce')\n",
    "df_HM_raw_data = df_HM_raw_data[(df_HM_raw_data['EFIS'].notna()) & df_HM_raw_data['EFIS'] != 0]\n",
    "\n",
    "#rename ID\n",
    "dict_rename = {'ID': 'AMT_ID',\n",
    "              'Award FY.2': 'RTL'\n",
    "              }\n",
    "df_HM_raw_data= df_HM_raw_data.rename(dict_rename, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-exception",
   "metadata": {},
   "source": [
    "<a id='SHOPP_Raw_Data'></a>\n",
    "## SHOPP Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "palestinian-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data.name = 'df_SHOPP_raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "usual-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "\n",
    "dict_rename_rawdata = {\n",
    "                       'County':'County TYP',\n",
    "                       'Route': 'Route TYP',\n",
    "                       'BackPM':'BackPM TYP',\n",
    "                       'AheadPM':'AheadPM TYP',\n",
    "                       'ID': 'AMT_ID',\n",
    "                       'Ten-Year Plan': 'Ten-Year Plan RD',\n",
    "                       'County.1' : 'County PRG',\n",
    "                       'Route.1' : 'Route PRG',\n",
    "                       'BackPM.1':'BackPM PRG',\n",
    "                       'AheadPM.1' : 'AheadPM PRG',\n",
    "                       'County.2' : 'County PCR',\n",
    "                       'Route.2' : 'Route PCR',\n",
    "                       'BackPM.2':'BackPM PCR',\n",
    "                       'AheadPM.2' : 'AheadPM PCR',\n",
    "                       'Activity Category': 'Activity',\n",
    "                       'RTL In Use': 'RTL'\n",
    "                      }\n",
    "df_SHOPP_raw_data = df_SHOPP_raw_data.rename(dict_rename_rawdata, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "directed-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove leading puncture for target columns\n",
    "cols_strip = ['District','Route TYP','EA','EFIS','Route PRG','PPNO','Route PCR']\n",
    "for c in cols_strip :\n",
    "    df_SHOPP_raw_data[c] = df_SHOPP_raw_data[c].str.strip(\"'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aquatic-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cost_columns = [\n",
    "    'RW Cost ($K)',\n",
    "    'Const Cost ($K)',\n",
    "    'Support Cost ($)',\n",
    "    'TYP Total Project Cost ($K)',\n",
    "    'PAED ($K)',\n",
    "    'PSE ($K)',\n",
    "    'R/W ($K)',\n",
    "    'CONS ($K)',\n",
    "    'Prog Support Cost ($)',\n",
    "    'Prog RW Cost ($K)',\n",
    "    'Prog Const Cost ($K)',\n",
    "    'Prog Total Project Cost ($K)',\n",
    "    'PCR R/W Cap ($K)',\n",
    "    'PCR Const Cap ($K)',\n",
    "    'PCR Support Cost ($K)',\n",
    "    'PCR Total Cost ($K)',\n",
    "    'Project Cost In Use',\n",
    "    'Total LL Prog ($K)',\n",
    "    'LL PAED Cost ($K)',\n",
    "    'LL CONS Cap ($K)'\n",
    "               ]\n",
    "\n",
    "for c in cost_columns:\n",
    "    df_SHOPP_raw_data[c] = df_SHOPP_raw_data[c].apply(uf.curreny_to_float)\n",
    "    df_SHOPP_raw_data[c].fillna(0, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "checked-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data clean \n",
    "#data type regulation\n",
    "#string/text data regulation\n",
    "\n",
    "df_SHOPP_raw_data['District'] =df_SHOPP_raw_data['District'].astype(int)\n",
    "\n",
    "df_SHOPP_raw_data['EFIS'] = pd.to_numeric(df_SHOPP_raw_data['EFIS'], errors='coerce')\n",
    "\n",
    "df_SHOPP_raw_data['Route PCR'] = df_SHOPP_raw_data['Route PCR'].astype(str)\n",
    "\n",
    "#data trimming\n",
    "#row trimming\n",
    "df_SHOPP_raw_data= df_SHOPP_raw_data[df_SHOPP_raw_data['District'] != 56]\n",
    "\n",
    "#column trimming\n",
    "df_SHOPP_raw_data.drop(['District Priority', 'PIR Performance Report'],\n",
    "  axis='columns', inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "#Question to be answered:\n",
    "#for EA Raw Data, the missing data is not null , but ''. Should we handle the missing data uniformly for string data?\n",
    "\n",
    "#TODO:\n",
    "#fill missing data\n",
    "#data quality check (checksum,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bright-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['CCA Date Miilestone (M600)'] = df_SHOPP_raw_data['CCA Date Miilestone (M600)'].apply(uf.regulate_timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "responsible-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['Section'] = df_SHOPP_raw_data['Section In Use']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-belief",
   "metadata": {},
   "source": [
    "<a id='TenYrShopp_Perf_RawData'></a>\n",
    "## TenYrShopp_Perf_RawData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "psychological-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "dict_rename_perf_rawdata = {\n",
    "                           'ID': 'AMT_ID',\n",
    "#                             'ProjectedRTL FY': 'Projected RTL FY',\n",
    "    \n",
    "# 'ActID':'Performance_ActID',\n",
    "# 'Quantity':    'Performance_Quantity'\n",
    "              }\n",
    "\n",
    "df_perf_raw_data = df_perf_raw_data.rename(dict_rename_perf_rawdata, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "found-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_strip = ['EA','EFIS','PPNO']\n",
    "for c in cols_strip :\n",
    "    df_perf_raw_data[c] = df_perf_raw_data[c].str.strip(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "secure-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-economy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "sustainable-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data clean \n",
    "#data type regulation\n",
    "\n",
    "df_perf_raw_data['Quantity'] = df_perf_raw_data['Quantity'].fillna(0)\n",
    "df_perf_raw_data['Assets in Good Cond'] = df_perf_raw_data['Assets in Good Cond'].fillna(0)\n",
    "df_perf_raw_data['Assets in Fair Cond'] = df_perf_raw_data['Assets in Fair Cond'].fillna(0)\n",
    "df_perf_raw_data['Assets in Poor Cond'] = df_perf_raw_data['Assets in Poor Cond'].fillna(0)\n",
    "df_perf_raw_data['New Assets Added'] = df_perf_raw_data['New Assets Added'].fillna(0)\n",
    "\n",
    "df_perf_raw_data['EFIS'] = pd.to_numeric(df_perf_raw_data['EFIS'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dedicated-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data trimming\n",
    "#row\n",
    "df_perf_raw_data= df_perf_raw_data[df_perf_raw_data['District'] != 56]\n",
    "#column\n",
    "df_perf_raw_data.drop(['PID Cycle', 'TYP','ProjectedSHOPP Cycle','RequestedRTL FY','DistrictPriority'],\n",
    "  axis='columns', inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sapphire-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data.name = 'df_perf_raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-tampa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "naughty-healthcare",
   "metadata": {},
   "source": [
    "<a id='ProgrammingList'></a>\n",
    "## Programming List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "completed-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "shts = ['2010 SHOPP',\n",
    "        '2012 SHOPP',\n",
    "        '2014 SHOPP',\n",
    "        '2016 SHOPP',\n",
    "        '2018 SHOPP',\n",
    "        '2020 SHOPP',\n",
    "        'Long Lead'\n",
    "        ]\n",
    "\n",
    "filename = 'Programming_list.xlsx'\n",
    "\n",
    "df_dict = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), sheet_name =shts) \n",
    "\n",
    "\n",
    "df_program = pd.DataFrame()\n",
    "\n",
    "for k, v in df_dict.items():\n",
    "#     print(type(v))\n",
    "#     print(k)\n",
    "    v['Table Names'] = k\n",
    "#     print(v.columns)\n",
    "    df_program = df_program.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "optional-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "dict_rename_program = {\n",
    "#                         'EA':'EA', \n",
    "                        'EFIS':'EFIS_Program', \n",
    "#                         'PPNO':'PPNO Programming', \n",
    "                        'Total Capital & Support':'Total Capital & Support Cost',\n",
    "#                         'Route': 'Route Programming',\n",
    "              }\n",
    "\n",
    "df_program = df_program.rename(dict_rename_program, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "thick-drunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s151589\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\pandas\\core\\frame.py:4459: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n"
     ]
    }
   ],
   "source": [
    "#data clean \n",
    "#data type regulation\n",
    "# df_program['Dist'] = df_program['Dist'].astype(int)\n",
    "# df_program['EFIS'] = pd.to_numeric(df_program['EFIS'], errors='coerce')\n",
    "\n",
    "fillna_columns = ['Con Sup','RW Sup','PA&ED','PS&E', 'PA&ED', 'RW', 'Con']\n",
    "\n",
    "df_program[fillna_columns].fillna(0, inplace=True)\n",
    "df_program['Route'].fillna('Various', inplace=True)\n",
    "\n",
    "df_program['Support Cost'] = df_program['Con Sup']+df_program['RW Sup']+df_program['PA&ED']+df_program['PS&E']\n",
    "df_program['Capital Cost'] = df_program['Con']+df_program['RW']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "royal-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program[['Con Sup','RW Sup','PA&ED','PS&E']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "supreme-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program['PA&ED'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hydraulic-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove leading puncture for all string columns\n",
    "# df_program.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "vulnerable-twelve",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "#data transformation\n",
    "\n",
    "# df_program.dropna(subset = ['EFIS_Program'], inplace = True)\n",
    "df_program['FY'] = df_program['FY'].apply(uf.FY_cleanup)\n",
    "\n",
    "df_program['Begin Post Miles'] = df_program['Post Miles'].apply(lambda x: str(x).split('/')[0])\n",
    "df_program['End Post Miles'] = df_program['Post Miles'].apply(lambda x: str(x).split('/')[0] if '/' in str(x) else np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "settled-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data trimming\n",
    "df_program.drop(['PM1BF', 'PM1B', 'PM1AF','PM1A',], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "suburban-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program.name = 'df_program'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "promising-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "senior-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "coastal-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data sanity check\n",
    "#check duplicates\n",
    "#check null\n",
    "#check data type\n",
    "\n",
    "if df_program['EFIS_Program'].value_counts(dropna = False).max() > 1:\n",
    "    Print('Duplicate EFIS ID found, please check the source data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "civilian-particle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_program['EFIS_Program'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-citizenship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-server",
   "metadata": {},
   "source": [
    "<a id='Bridge_Inventory'></a>\n",
    "\n",
    "## Bridge Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "baking-leather",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'GFP_BrInvList_AllDistricts.xlsx'\n",
    "\n",
    "df_bridge_inventory = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), skiprows = 4, header = [0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "respiratory-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filename = 'GFP_BrInvList_AllDistricts_March_2020_05042020.xlsx'\n",
    "\n",
    "# df_bridge_inventory = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), skiprows = 4, header = [0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "capital-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = ['Bridge #', 'Deck Area, SF', 'Date', 'Health', 'Deck (58)',\n",
    "       'Super (59)', 'Sub (60)', 'Culv (62)', 'Scour', 'Seismic', 'Overall',\n",
    "       'VC', 'Permit', 'Rail Overall', 'Good, LF', 'Fair, LF', 'Poor, LF',\n",
    "       'Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "refined-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bridge_inventory = df_bridge_inventory[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "unknown-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "\n",
    "dict_rename_bridge_inventory = {'Bridge #':'BridgeNo',\n",
    "                               'Date':'Inspection Date', \n",
    "                               'Health': 'Bridge Health',\n",
    "                                'Scour':'Bridge Scour', \n",
    "                                'Seismic':'Bridge Seismic', \n",
    "                               'Deck (58)' : 'NBI Condition Ratings_Deck (58)',\n",
    "                                'Super (59)': 'NBI Condition Ratings_Super (59)',\n",
    "                                'Sub (60)':'NBI Condition Ratings_Sub (60)',\n",
    "                                'Culv (62)':'NBI Condition Ratings_Culv (62)',\n",
    "                                'Overall': 'Bridge Goods Movement_Overall',\n",
    "                                'VC': 'Bridge Goods Movement_VC',\n",
    "                                'Permit':'Bridge Goods Movement_Permit',\n",
    "                                'Rail Overall': 'Bridge Rail Upgrade_Rail Overall', \n",
    "                                'Good, LF': 'Bridge Rail Upgrade_Good, LF', \n",
    "                                'Fair, LF': 'Bridge Rail Upgrade_Fair, LF', \n",
    "                                'Poor, LF': 'Bridge Rail Upgrade_Poor, LF', \n",
    "                      }\n",
    "\n",
    "df_bridge_inventory.rename(dict_rename_bridge_inventory, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "neural-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_to_good(value):\n",
    "    if value and value in ['Poor','Fair','Good']:\n",
    "        return value\n",
    "    else:\n",
    "        return 'Good'\n",
    "\n",
    "data_recondition_columns = ['Bridge Health','Bridge Scour','Bridge Seismic','Bridge Goods Movement_Overall',\n",
    "       'Bridge Goods Movement_VC', 'Bridge Goods Movement_Permit','Bridge Rail Upgrade_Rail Overall',]\n",
    "\n",
    "for c in data_recondition_columns:\n",
    "    df_bridge_inventory[c] = df_bridge_inventory[c].apply(default_to_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "descending-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bridge_inventory.name = 'df_bridge_inventory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-dependence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "coated-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bridge_inventory.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-prisoner",
   "metadata": {},
   "source": [
    "<a id='Bridge_Worksheet'></a>\n",
    "\n",
    "## Raw Data Bridge Worksheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "economic-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "#with manual edits\n",
    "\n",
    "dict_rename_bridge_worksheet = {\n",
    " 'ID': 'AMT_ID',\n",
    " 'Bridge №': 'BridgeNo',\n",
    " 'Work Type': 'WorkType',\n",
    " 'Brdige / TunnelWork Description': 'WorkDescription',\n",
    " 'Bridge /TunnelHealth Pre': 'Health Pre',\n",
    " 'Bridge /TunnelHealth Post': 'Health Post',\n",
    " 'BridgeScourPre': 'Scour_Pre',\n",
    " 'BridgeScourPost': 'Scour_Post',\n",
    " 'BridgeSeismicPre': 'Seismic_Pre',\n",
    " 'BridgeSeismicPost': 'Seismic_Post',\n",
    " 'BridgeGds MvmtPre': 'GdsMvmt_Pre',\n",
    " 'BridgeGds MvmtPost': 'GdsMvmt_Post',\n",
    " 'Exist(sf)': 'Deck_Exist(sf)',\n",
    " 'Additional(sf)': 'Deck_Additional(sf)',\n",
    " 'Y/N': 'Paint_Y/N',\n",
    " 'Condition': 'Paint_Condition',\n",
    " 'Paint Area(sf)': 'Paint Area(sf)',\n",
    " 'Y/N.1': 'ElectricalMechanical_Y/N',\n",
    " 'Condition.1': 'ElectricalMechanical_Condition',\n",
    " 'Area(sf)': 'ElectricalMechanical_Area(sf)',\n",
    " 'Y/N.2': 'ApproachSlab_Y/N',\n",
    " 'Replaced(sf)': 'ApproachSlab_Replaced(sf)',\n",
    " 'New(sf)': 'ApproachSlab_New(sf)',\n",
    " 'Y/N.3': 'Rail_Y/N',\n",
    " 'Good(lf)': 'Rail_Good(lf)',\n",
    " 'Fair(lf)': 'Rail_Fair(lf)',\n",
    " 'Poor(lf)': 'Rail_Poor(lf)',\n",
    " 'Additonal(lf)': 'Rail_Additonal(lf)',\n",
    " 'Post Good(lf)': 'Rail_Post Good(lf)',\n",
    " 'Post Fair(lf)': 'Rail_Post Fair(lf)',\n",
    " 'Post Poor(lf)': 'Rail_Post Poor(lf)',\n",
    " 'Post New(lf)': 'Rail_Post New(lf)',\n",
    " 'FishPassage(Y/N)': 'FishPassage(Y/N)',\n",
    "}\n",
    "\n",
    "df_brg_raw_data.rename(dict_rename_bridge_worksheet, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "unlike-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brg_raw_data.name = 'df_brg_raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "correct-tanzania",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_brg_raw_data['Rail_Good(lf)'].fillna(0, inplace = True)\n",
    "df_brg_raw_data['Rail_Fair(lf)'].fillna(0, inplace = True)\n",
    "df_brg_raw_data['Rail_Poor(lf)'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "latin-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brg_raw_data['Rail_Total(lf)'] = (df_brg_raw_data['Rail_Good(lf)'] \n",
    "                                             + df_brg_raw_data[ 'Rail_Fair(lf)'] \n",
    "                                             + df_brg_raw_data['Rail_Poor(lf)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-shipping",
   "metadata": {},
   "source": [
    "<a id='Pavement_Worksheet'></a>\n",
    "\n",
    "## Raw Data Pavement Worksheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "sixth-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "#with manual editing\n",
    "\n",
    "dict_rename_pavement_worksheet = {\n",
    " 'ID': 'AMT_ID',\n",
    " 'Class': 'RoadwayClass',\n",
    " 'Green': 'TriditionalCondition_Green',\n",
    " 'Yellow': 'TriditionalCondition_Yellow',\n",
    " 'Blue': 'TriditionalCondition_Blue',\n",
    " 'Orange': 'TriditionalCondition_Orange',\n",
    " 'Red': 'TriditionalCondition_Red',\n",
    " 'Good': 'MAP21_Good',\n",
    " 'Fair': 'MAP21_Fair',\n",
    " 'Poor': 'MAP21_Poor',\n",
    " 'Total LaneMiles': 'Total LaneMiles',\n",
    " 'Green.1': 'TriditionalCondition_Post_Green',\n",
    " 'Yellow.1': 'TriditionalCondition_Post_Yellow',\n",
    " 'Blue.1': 'TriditionalCondition_Post_Blue',\n",
    " 'Orange.1': 'TriditionalCondition_Post_Orange',\n",
    " 'Red.1': 'TriditionalCondition_Post_Red',\n",
    " 'Good.1': 'MAP21_Post_Good',\n",
    " 'Fair.1': 'MAP21_Post_Fair',\n",
    " 'Poor.1': 'MAP21_Post_Poor',\n",
    "#  'Date': 'Date'\n",
    "                               }\n",
    "df_pav_raw_data.rename(dict_rename_pavement_worksheet, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-potter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ordered-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pav_raw_data.name = 'df_pav_raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "controlling-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pav_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "collect-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pav_raw_data['Plan Year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-canvas",
   "metadata": {},
   "source": [
    "<a id='Drainage_Worksheet'></a>\n",
    "## Raw Data Drainage Worksheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "collective-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drain_raw_data.name = 'df_drain_raw_data'\n",
    "\n",
    "dict_drain_rename = {\n",
    " 'ID': 'AMT_ID',\n",
    " 'Data Date':'Data Date_Drainage'\n",
    "                               }\n",
    "df_drain_raw_data.rename(dict_drain_rename, axis = 1, inplace = True)\n",
    "\n",
    "cols = ['EA','EFIS','SYSNO','INETNO','OUTETNO']\n",
    "for c in cols: \n",
    "    df_drain_raw_data[c] = df_drain_raw_data[c].apply(uf.remove_punction)\n",
    "\n",
    "\n",
    "df_drain_raw_data['Data Date_Drainage'] = df_drain_raw_data['Data Date_Drainage'].apply(uf.regulate_timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "infectious-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#question to be answered: what to do with meanless or null values.\n",
    "#return null if DrainageWorksheet_SYSNO,DrainageWorksheet_INETNO,DrainageWorksheet_OUTETNO are null or empty\n",
    "\n",
    "# df_drain_raw_data['Unique Culvert ID'] = (df_drain_raw_data['SYSNO'] + \"_\"\n",
    "#                                           + df_drain_raw_data['INETNO'] + \"_\"\n",
    "#                                          + df_drain_raw_data['OUTETNO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "located-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_drain_raw_data['SYSNO'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "particular-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_drain_unique_ID(df):\n",
    "    if pd.isnull(df['SYSNO']) or pd.isnull(df['INETNO']) or pd.isnull(df['OUTETNO']):\n",
    "        return None\n",
    "    else:\n",
    "        return (df['SYSNO'] + \"_\"+ df['INETNO'] + \"_\"+ df['OUTETNO'])\n",
    "df_drain_raw_data['Unique Culvert ID'] = df_drain_raw_data.apply(calc_drain_unique_ID, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-patio",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "virgin-conclusion",
   "metadata": {},
   "source": [
    "<a id='TMS_Worksheet'></a> \n",
    "## Raw Data TMS Worksheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "happy-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_TMS_rename = {\n",
    " 'ID': 'AMT_ID',\n",
    " 'Data Date':'Data Date_TMS'\n",
    "                               }\n",
    "df_tms_raw_data.rename(dict_TMS_rename, axis = 1, inplace = True)\n",
    "\n",
    "df_tms_raw_data.name = 'df_tms_raw_data'\n",
    "\n",
    "df_tms_raw_data.shape\n",
    "\n",
    "df_tms_raw_data['Data Date_TMS'] = df_tms_raw_data['Data Date_TMS'].apply(uf.regulate_timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "heated-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Minor_tms_raw_data.rename(dict_TMS_rename, axis = 1, inplace = True)\n",
    "\n",
    "df_Minor_tms_raw_data['Data Date_TMS'] = df_Minor_tms_raw_data['Data Date_TMS'].apply(uf.regulate_timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "confidential-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_tms_raw_data.rename(dict_TMS_rename, axis = 1, inplace = True)\n",
    "\n",
    "df_HM_tms_raw_data['Data Date_TMS'] = df_HM_tms_raw_data['Data Date_TMS'].apply(uf.regulate_timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "hungry-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tms_raw_data = pd.merge(df_tms_raw_data, df_SHOPP_raw_data[['AMT_ID','Section In Use']], how = 'left', \n",
    "                  left_on = ['AMT_ID'], \n",
    "                  right_on =  ['AMT_ID'])\n",
    "\n",
    "\n",
    "df_tms_raw_data.rename(dict_TMS_rename, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-motivation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "discrete-campbell",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Minor_tms_raw_data = pd.merge(df_Minor_tms_raw_data, df_Minor_raw_data[['AMT_ID','Section In Use']], how = 'left', \n",
    "                  left_on = ['AMT_ID'], \n",
    "                  right_on =  ['AMT_ID'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "liable-leeds",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_tms_raw_data = pd.merge(df_HM_tms_raw_data, df_HM_raw_data[['AMT_ID','Section In Use']], how = 'left', \n",
    "                  left_on = ['AMT_ID'], \n",
    "                  right_on =  ['AMT_ID'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-ghost",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "beautiful-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ck_HM_ActiveProject(df):\n",
    "    try:\n",
    "        if df['RTL'] == '0000': \n",
    "            return 'No'\n",
    "        elif int(df['RTL'][-2:]) > 22: \n",
    "            return 'No'\n",
    "        else:\n",
    "            return 'Yes'\n",
    "    except:\n",
    "        return 'No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "flush-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_HM_raw_data['Active Project?'] = df_HM_raw_data.apply(Ck_HM_ActiveProject, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "strong-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Minor_ApprovedList['Active Project?'] = 'Yes'\n",
    "\n",
    "df_Minor_raw_data = pd.merge(df_Minor_raw_data, df_Minor_ApprovedList[['EFIS','Active Project?']], how = 'left', \n",
    "                  left_on = ['EFIS'], \n",
    "                  right_on =  ['EFIS'])\n",
    "\n",
    "df_Minor_raw_data['Active Project?'].fillna('No', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "charitable-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mark SHOPP active project\n",
    "def mark_active_SHOPP_project(**kwargs):\n",
    "\n",
    "    isPlaceholder = kwargs['Ten-Year Plan'] == 9999\n",
    "    \n",
    "    reservation_keywords = ['Major Damage - Emergency Opening',\n",
    "    'Major Damage - Permanent Restoration',\n",
    "    'Reactive Safety',\n",
    "    'Relinquishment',\n",
    "    'Safety - SI',]\n",
    "\n",
    "    isReservationProject = kwargs['Activity Category'] in reservation_keywords\n",
    "    \n",
    "    isProgrammed = pd.notnull(kwargs['SHOPP Amendment Date'])\n",
    "    \n",
    "    if pd.isnull(kwargs['RTL In Use']):\n",
    "        RTL = 0 #to be answered by mara\n",
    "    else:\n",
    "        RTL = int(kwargs['RTL In Use'][-2:])\n",
    "    \n",
    "    if isPlaceholder: \n",
    "        return 'No'\n",
    "    elif RTL < 32: \n",
    "        return 'Yes'\n",
    "    elif RTL < 27 and (not isProgrammed) and (not isReservationProject): \n",
    "        return 'No'\n",
    "    else:\n",
    "        return 'Yes'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "international-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['Active Project?'] = df_SHOPP_raw_data.apply( lambda df: mark_active_SHOPP_project(\n",
    "**{'Ten-Year Plan': df['Ten-Year Plan RD'],\n",
    "'Activity Category': df['Activity'],\n",
    "'SHOPP Amendment Date': df['SHOPP Amendment Date'],\n",
    "'RTL In Use': df['RTL'],}\n",
    "), axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "stupid-issue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMT_ID</th>\n",
       "      <th>Ten-Year Plan RD</th>\n",
       "      <th>Active Project?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5249</th>\n",
       "      <td>23439</td>\n",
       "      <td>9999</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      AMT_ID  Ten-Year Plan RD Active Project?\n",
       "5249   23439              9999              No"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == 23439   ][['AMT_ID','Ten-Year Plan RD','Active Project?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "involved-blair",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data.drop(columns=['Cca Finish Date', 'Cca Percent Comp'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, df_Project_Details, how = 'left', \n",
    "                  left_on = ['EFIS'], \n",
    "                  right_on = ['EFIS'])\n",
    "\n",
    "df_SHOPP_raw_data['Cca Percent Comp'].fillna(0, inplace=True)\n",
    "df_SHOPP_raw_data['Cca Finish Date'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-sterling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "upset-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMS_cols = ['TMSID', 'AMT_ID', 'Section', 'Program', 'District', 'TMS Structural or Technology', 'Asset Pre-Condition at RTL', 'Section_In_Use']\n",
    "\n",
    "df_tms_all = df_tms_raw_data.append(df_Minor_tms_raw_data).append(df_HM_tms_raw_data)\n",
    "\n",
    "df_tms_all['Same as section in use?'] = df_tms_all['Section'] ==df_tms_all['Section In Use']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cultural-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TMS worksheet in PPC section?\n",
    "\n",
    "df_tms_in_PPC = df_tms_all[(df_tms_all['Section'] == 'PPC') & (df_tms_all['TMS Structural or Technology'].isin(['Technology & Structures', 'Technology']))][['AMT_ID']].groupby('AMT_ID').first().reset_index()\n",
    "\n",
    "df_tms_in_PPC['TMS worksheet in PPC section?'] ='Yes'\n",
    "\n",
    "\n",
    "df_SHOPP_raw_data.drop(['TMS worksheet in PPC section?'],\n",
    "  axis='columns', inplace=True, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, df_tms_in_PPC, how = 'left', \n",
    "                  left_on = ['AMT_ID'], \n",
    "                  right_on = ['AMT_ID'])\n",
    "\n",
    "df_SHOPP_raw_data['TMS worksheet in PPC section?'].fillna('No', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "designed-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data = pd.merge(df_perf_raw_data, df_SHOPP_raw_data[['AMT_ID','Section']], how = 'inner', \n",
    "                  left_on = ['AMT_ID','Section'], \n",
    "                  right_on =  ['AMT_ID','Section'])\n",
    "\n",
    "SHOPP_TMS_perf_Summary = df_perf_raw_data[df_perf_raw_data['Performance Objective'] == 'Transportation Management Systems'].groupby(['AMT_ID'])[['Assets in Good Cond', \n",
    "       'Assets in Poor Cond', 'New Assets Added',]].sum().reset_index()\n",
    "\n",
    "dict_rename = {\n",
    "     'Assets in Good Cond': 'AM Tool TMS Good',\n",
    "    'Assets in Poor Cond': 'AM Tool TMS Poor',\n",
    "    'New Assets Added': 'AM Tool TMS New',\n",
    "                               }\n",
    "SHOPP_TMS_perf_Summary.rename(dict_rename, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cleared-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOPP_TMS_perf_Summary['Claims TMS technology Performance'] = 'Yes'\n",
    "\n",
    "#column trimming\n",
    "df_SHOPP_raw_data.drop(['Claims TMS technology Performance'],\n",
    "  axis='columns', inplace=True, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, SHOPP_TMS_perf_Summary, how = 'left', \n",
    "                  left_on = ['AMT_ID'], \n",
    "                  right_on = ['AMT_ID'])\n",
    "\n",
    "df_SHOPP_raw_data['Claims TMS technology Performance'].fillna('No', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-violin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "experimental-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['AM Tool RTL (Section in Use)'] = df_SHOPP_raw_data.apply(uf.calc_SIU_RTL, axis=1)\n",
    "\n",
    "df_SHOPP_raw_data['AM Tool RTL (Section in Use)'].fillna('00', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "civilian-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last Year FY POR\n",
    "\n",
    "# cal ='''\n",
    "# FLOAT(Right([AM Tool RTL (Section in Use)],2))\n",
    "# '''\n",
    "\n",
    "df_SHOPP_raw_data['Last Year FY POR'] = df_SHOPP_raw_data['AM Tool RTL (Section in Use)'].str[-2:].astype(int)+2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bright-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['Activity (group)'] = df_SHOPP_raw_data['Activity'].apply(uf.calc_activity_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "complete-small",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TARGET_FY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-dddd77309493>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m'No'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mdf_SHOPP_raw_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Include 5-year POR?'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_SHOPP_raw_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalc_include_5year_POR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   7763\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7764\u001b[0m         )\n\u001b[1;32m-> 7765\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7767\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m                 \u001b[1;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m                 \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                     \u001b[1;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-dddd77309493>\u001b[0m in \u001b[0;36mcalc_include_5year_POR\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Ten-Year Plan RD'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m9999\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mreturn\u001b[0m  \u001b[1;34m'No'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Last Year FY POR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mTARGET_FY\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Last Year FY POR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mTARGET_FY\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Activity (group)'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Reservation'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SHOPP Amendment Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;32mreturn\u001b[0m  \u001b[1;34m'Yes'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TARGET_FY' is not defined"
     ]
    }
   ],
   "source": [
    "def calc_include_5year_POR(df):\n",
    "    if df['Ten-Year Plan RD'] == 9999:\n",
    "        return  'No'\n",
    "    elif(df['Last Year FY POR']>TARGET_FY and df['Last Year FY POR']<TARGET_FY + 6) :   \n",
    "        if df['Activity (group)'] == 'Reservation' and pd.isnull(df['SHOPP Amendment Date']): \n",
    "            return  'Yes' \n",
    "        else: \n",
    "            return 'No' \n",
    "    elif (df['Last Year FY POR']>TARGET_FY + 5 and df['Last Year FY POR']<TARGET_FY + 11) : \n",
    "        if(df['Long Lead'] == \"Y\") and (df['Section'] == \"PRG\")  :\n",
    "             return  'Yes'\n",
    "        elif pd.isnull(df['SHOPP Amendment Date']): \n",
    "             return  'Yes' \n",
    "        else: \n",
    "            return 'No' \n",
    "    else: \n",
    "        return 'No'\n",
    "\n",
    "df_SHOPP_raw_data['Include 5-year POR?'] = df_SHOPP_raw_data.apply(calc_include_5year_POR, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-string",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-plasma",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-experiment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-acquisition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-invitation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-mandate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_active_projects = ['AMT_ID', 'EFIS', 'RTL', 'Active Project?']\n",
    "\n",
    "df_active_projects = df_SHOPP_raw_data[col_active_projects].append(df_Minor_raw_data[col_active_projects]).append( df_HM_raw_data[col_active_projects])\n",
    "\n",
    "df_tms_all = pd.merge(df_tms_all, df_active_projects, how = 'left', \n",
    "                  left_on = ['AMT_ID'], \n",
    "                  right_on = ['AMT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-contemporary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-palestine",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tms_all.drop(columns=['Cca Finish Date', 'Cca Percent Comp'],inplace=True , errors='ignore')\n",
    "\n",
    "df_tms_all = pd.merge(df_tms_all, df_Project_Details[['EFIS','Cca Finish Date', 'Cca Percent Comp']], how = 'left', \n",
    "                  left_on = ['EFIS'], \n",
    "                  right_on = ['EFIS'])\n",
    "\n",
    "df_tms_all['Cca Finish Date'].fillna(\"No CCA Date\", inplace=True)\n",
    "df_tms_all['Cca Percent Comp'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-ontario",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tms_all.drop(columns=['Include 5-year POR?'],inplace=True , errors='ignore')\n",
    "df_tms_all = pd.merge(df_tms_all, df_SHOPP_raw_data[['AMT_ID', 'Section In Use', 'Include 5-year POR?']], how = 'left', \n",
    "                  left_on = ['AMT_ID', 'Section In Use'], \n",
    "                  right_on = ['AMT_ID', 'Section In Use'])\n",
    "\n",
    "df_tms_all['Include 5-year POR?'].fillna(\"No\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-salem",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_tms_all[(df_tms_all['Active Project?'] == 'Yes') \n",
    "           & (df_tms_all['Same as section in use?'] == True) \n",
    "           & (df_tms_all['TMS Structural or Technology'] != 'Structures')\n",
    "          & (df_tms_all['Cca Percent Comp'] == 0)].groupby('TMSID')['AMT_ID'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['Repeated TMS?'] = temp['AMT_ID'].apply(lambda x: 'Yes' if x > 1 else 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tms_all = pd.merge(df_tms_all, temp[['TMSID','Repeated TMS?' ]], how = 'left', \n",
    "                  left_on = ['TMSID'], \n",
    "                  right_on = ['TMSID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-tunisia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tms_all = pd.merge(df_tms_all, df_TMS_LifeCycle, how = 'left', \n",
    "                  left_on = ['Asset Type'], \n",
    "                  right_on = ['Asset Type'])\n",
    "\n",
    "df_tms_all['Repeated TMS?'].fillna(\"Not included\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-maria",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ck_repeated_TMS(df):\n",
    "    if df['TMS Structural or Technology'] == 'Structures':\n",
    "        return 'Structures'\n",
    "    elif df['Asset Pre-Condition at RTL'] == 'New':\n",
    "        return 'New Asset'\n",
    "    else:\n",
    "        return df['Repeated TMS?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-indonesia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tms_all['Repeated TMS?'] = df_tms_all.apply(ck_repeated_TMS, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-startup",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tms_all['RTL2'] = df_tms_all['RTL'].str[-2:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_delta_RTL(s):\n",
    "    RTLs = np.array(s.values)\n",
    "    RTLs.sort()\n",
    "    return min(np.diff(RTLs))\n",
    "    \n",
    "#     return RTLs[-1] - RTLs[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_s = pd.Series([28,22,24,30])\n",
    "\n",
    "calc_delta_RTL(test_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-decrease",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_tms_all[df_tms_all['Repeated TMS?']=='Yes'].groupby(['TMSID']).agg({\n",
    "    'RTL2': calc_delta_RTL,\n",
    "    'Asset Life Cycle Limit': min\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-index",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rename = {\n",
    "    'RTL2': 'Delta RTL',\n",
    "    }\n",
    "temp= temp.rename(dict_rename, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['Is Repeated TMS ID within Life Cycle?'] = temp.apply(lambda df: 'Yes' if df['Asset Life Cycle Limit'] > df['Delta RTL'] else 'No', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp2 = pd.merge(temp[temp['Is Repeated TMS ID within Life Cycle?'] == 'Yes'], \n",
    "#          df_tms_all[['District', 'TMSID','Asset Type', 'Program', 'AMT_ID', 'Section','EA', 'EFIS','Include 5-year POR?','RTL']],\n",
    "#          how = 'left', left_on=['TMSID'], right_on=['TMSID'])\n",
    "\n",
    "# temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp2[temp2['TMSID']=='1003-12FRMBQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tms_all[df_tms_all['TMSID']=='1003-12FRMBQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-matrix",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_duplicated_TMS = pd.merge(temp, \n",
    "         df_tms_all[['District', 'TMSID','Asset Type', 'Program', 'AMT_ID', 'Section','EA', 'EFIS','Include 5-year POR?','RTL','Active Project?']],\n",
    "         how = 'left', left_on=['TMSID'], right_on=['TMSID']).sort_values('TMSID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_duplicated_TMS[df_duplicated_TMS['TMSID']=='1003-12FRMBQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_duplicated_TMS[(df_duplicated_TMS['District'] == 12)\n",
    "#                   &(df_duplicated_TMS['Is Repeated TMS ID within Life Cycle?'] == 'Yes')\n",
    "#                  &(df_duplicated_TMS['Include 5-year POR?'] == 'Yes')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_duplicated_TMS[(df_duplicated_TMS['TMSID'] == '12CCTV164Via')\n",
    "#                   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-republican",
   "metadata": {},
   "source": [
    "<a id='Export_Output'></a>\n",
    "\n",
    "### Export Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-stuff",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cols = ['District', 'TMSID', 'Asset Type',\n",
    "       'Program', 'AMT_ID', 'Section', 'EA', 'EFIS', 'Include 5-year POR?', 'RTL', 'Delta RTL', 'Asset Life Cycle Limit',\n",
    "       'Is Repeated TMS ID within Life Cycle?',]\n",
    "\n",
    "df_out = df_duplicated_TMS\n",
    "hyper_name = 'Duplicate_TMS_Summary'\n",
    "uf.export_hyper(df_out, hyper_name, LOG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_tms_all\n",
    "hyper_name = 'TMS_With_Duplicate_Check'\n",
    "uf.export_hyper(df_out, hyper_name, LOG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_SHOPP_raw_data\n",
    "\n",
    "hyper_name = 'SHOPP_RawData_w_TMS_Info'\n",
    "uf.export_hyper(df_out, hyper_name, LOG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-isaac",
   "metadata": {},
   "source": [
    "\n",
    "<a id='FinalCleanUp'></a>\n",
    "## Final Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up tableau publishing log file\n",
    "\n",
    "import os\n",
    "import glob\n",
    "# get a recursive list of file paths that matches pattern\n",
    "fileList = glob.glob('./*.log')\n",
    "# Iterate over the list of filepaths & remove each file.\n",
    "for filePath in fileList:\n",
    "    try:\n",
    "        os.remove(filePath)\n",
    "    except OSError:\n",
    "        print(\"Error while deleting file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-sponsorship",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time =  time.time()\n",
    "elapsed = end_time - start_time\n",
    "print('time elapsed : {} seconds'.format(elapsed))\n",
    "\n",
    "file_export_log = open(LOG_FILE, \"a\")  # append mode\n",
    "file_export_log.write('#####time elapsed : {} seconds \\n'.format(elapsed))\n",
    "file_export_log.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
