{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "final-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-diagnosis",
   "metadata": {},
   "source": [
    "# Tip for quick search\n",
    "\n",
    "* Needs attention: the place where needs update or better logic\n",
    "* question to be answered: the place where things are still not clear\n",
    "* Unit Test: Unit test where you can drill in to find the data that leads to the check results for a specific project and specific check\n",
    "* TODO: things needs to be done\n",
    "* bookmark: stop point from last visit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-hawaii",
   "metadata": {},
   "source": [
    "# Update Note: \n",
    "\n",
    "11-2-2021: group activities in internal project book output file  <br>\n",
    "12-15-2021: \n",
    "* bring back the live data reading option\n",
    "* remove duplicated project ID in the internal project book\n",
    "12-17-2021:\n",
    "* download all SHOPP/Minor/HM worksheets. Filter the dataframe before data checks\n",
    "* remove defaul na option when downloading data\n",
    "\n",
    "12-20-2021:\n",
    "* import projectbookcheck_utilityfunction as uf \n",
    "* skip safety checks for TYP=9999\n",
    "* add Data_HourMinute, change Data_TimeStamp to Data_Date\n",
    "* use TenYrShopp_RawData_ file create time as Data_HourMinute\n",
    "\n",
    "12-23-2021\n",
    "* delete Data_HourMinute, Data_Date , change to Data_TimeStamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-excess",
   "metadata": {},
   "source": [
    "# Admin Notes:\n",
    "\n",
    "\n",
    "1. The AMTool dataset is archived daily as csv files and used for the project book check. \n",
    "The csv files are located at: \n",
    "r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Tableau Dashboards\\DataLake'\n",
    "\n",
    "2. The excel input files are checked daily and archived with datestamp whenever it is modified.\n",
    "The continuously updated excel input files are located at: r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_WorkingFolder\\excel'\n",
    "The excel input file are archived at: r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Tableau Dashboards\\Data_MiscInput'\n",
    "To recover the archived excel file used in project book check for a target date, select the excel file with latest datestamp but is still earlier than the target date.\n",
    "\n",
    "3. The check summary export action is logged daily. It can be used for daily monitoring. \n",
    "The file export log is located at: \\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_WorkingFolder\\output_internal\\log\n",
    "\n",
    "4. The published data are at:\n",
    "\n",
    "    * csv files for district asset manager: http://svgcshopp.dot.ca.gov/DataLake/ProjectBookCheck/\n",
    "    * csv files for HQ AM: \\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_WorkingFolder\\output_internal\n",
    "    * tableau workbook with live data source: https://tableau.dot.ca.gov/#/site/AssetManagement/workbooks/1815/views\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-confidentiality",
   "metadata": {},
   "source": [
    "<a id='TableOfContents'></a>\n",
    "\n",
    "# Table Of Contents\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "### [Global Constants](#GlobalConstants)\n",
    "\n",
    "\n",
    "### [Load and cleanup source data](#Read_Data)\n",
    "\n",
    "* [Bridge_Inventory](#Bridge_Inventory)\n",
    "* [Bridge_Worksheet](#Bridge_Worksheet)\n",
    "* [Check_Exceptions](#Check_Exceptions)\n",
    "* [Counties](#Counties)\n",
    "* [Drainage_Worksheet](#Drainage_Worksheet)\n",
    "* [Minor_Raw_Data](#Minor_Raw_Data)\n",
    "* [Pavement_Worksheet](#Pavement_Worksheet)\n",
    "* [PID_Workload](#PID_Workload)\n",
    "* [Postmile_Check](#Postmile_Check)\n",
    "* [Programming_Summary](#Programming_Summary)\n",
    "* [ProgrammingList](#ProgrammingList)\n",
    "* [Project_Detail_Report](#Project_Detail_Report)\n",
    "* [Project_Obselete](#Project_Obselete)\n",
    "* [SHOPP_Candidates](#SHOPP_Candidates)\n",
    "* [SHOPP_Raw_Data](#SHOPP_Raw_Data)\n",
    "* [TenYrShopp_Perf_RawData](#TenYrShopp_Perf_RawData)\n",
    "* [TMS_Worksheet](#TMS_Worksheet)\n",
    "\n",
    "\n",
    "## Add fields to SHOPP raw data (calculate and join)\n",
    "* [Calculated Fields](#AddDataColumns)\n",
    "* [Join Tables](#DataJoining)\n",
    "\n",
    "\n",
    "\n",
    "## Data Check and Export\n",
    "\n",
    "\n",
    "## [Data Check List](#Issue_Table1)\n",
    "The main table of check issues, \n",
    "one issue per row, \n",
    "\n",
    "\n",
    "* [Will_this_project_be_included_in_the_Project_Book](#Will_this_project_be_included_in_the_Project_Book)\n",
    "* [Does_project_cost_exceed_Minor_Program_limits](#Does_project_cost_exceed_Minor_Program_limits)\n",
    "* [Is_Major_Damage_or_Mobility_Subcategory_Identified (Obselete)](#Is_Major_Damage_or_Mobility_Subcategory_Identified)\n",
    "* [Is_Planned_Project_RTL_in_a_FY_that_can_be_programmed_in_future_SHOPP_cycles](#Is_Planned_Project_RTL_in_a_FY_that_can_be_programmed_in_future_SHOPP_cycles)\n",
    "* [Is_the_PID_cycle_consistent_with_the_project_status_and_RTL](#Is_the_PID_cycle_consistent_with_the_project_status_and_RTL)\n",
    "* [Is_PIP_uploaded_(Active_and_Complete_PIDs)](#Is_PIP_uploaded_(Active_and_Complete_PIDs))\n",
    "* [Is_District_Director_Approval_Date_in_the_Future](#Is_District_Director_Approval_Date_in_the_Future)\n",
    "* [Is_the_EA_or_Project_ID_repeated_in_the_AM_tool](#Is_the_EA_or_Project_ID_repeated_in_the_AM_tool)\n",
    "* [Does_project_include_performance_related_to_each_location](#Does_project_include_performance_related_to_each_location)\n",
    "* [Is_Performance_tab_Complete](#Is_Performance_tab_Complete)\n",
    "* [Is_at_least_one_performance_activities_related_to_the_Activity_Category_of_planned_project](#Is_at_least_one_performance_activities_related_to_the_Activity_Category_of_planned_project)\n",
    "* [Is_Long_Lead_Project_Cost_and_RTL_completed_and_consistent](#Is_Long_Lead_Project_Cost_and_RTL_completed_and_consistent)\n",
    "* [Are_all_Project_Locations_(PM)_Valid](#Are_all_Project_Locations_(PM)_Valid)\n",
    "* [Is_Drainage_Worksheet_Complete](#Is_Drainage_Worksheet_Complete)\n",
    "* [Are_all_conditions_selected_for_bridge_replacements](#Are_all_conditions_selected_for_bridge_replacements)\n",
    "* [Does_Bridge_Worksheet_need_updates](#Does_Bridge_Worksheet_need_updates)\n",
    "* [Does_the_Plan_Year_in_the_Pavement_Worksheet_match_the_Project_RTL](#Does_the_Plan_Year_in_the_Pavement_Worksheet_match_the_Project_RTL)\n",
    "* [Is_Pavement_Worksheet_Complete](#Is_Pavement_Worksheet_Complete)\n",
    "* [Is_the_Pavement_Work_Limits_Direction_in_the_Pavement_Worksheet_complete (Obselete)](#Is_the_Pavement_Work_Limits_Direction_in_the_Pavement_Worksheet_complete)\n",
    "* [Is_TMS_Worksheet_Complete](#Is_TMS_Worksheet_Complete)\n",
    "* [Does_the_RTL_Plan_Year_in_the_TMS_Worksheet_match_the_Project_RTL](#Does_the_RTL_Plan_Year_in_the_TMS_Worksheet_match_the_Project_RTL)\n",
    "* [Do_SHOPP_project_data_in_the_AM_Tool_match_CTIPS_data](#Do_SHOPP_project_data_in_the_AM_Tool_match_CTIPS_data)\n",
    "* [Is_PID_completed_and_uploaded_for_current_SHOPP_Candidates](#Is_PID_completed_and_uploaded_for_current_SHOPP_Candidates)\n",
    "* [Is_CCE_uploaded](#Is_CCE_uploaded)\n",
    "* [LL_not_in_POR](#LL_not_in_POR)\n",
    "* [Is_Pavement_limits_repeating_in_the_same_project](#Is_Pavement_limits_repeating_in_the_same_project)\n",
    "* [Repeated_Bridge_within_the_same_project](#Repeated_Bridge_within_the_same_project)\n",
    "* [Is_TMS_Asset_repeating_in_the_same_project](#Is_TMS_Asset_repeating_in_the_same_project)\n",
    "* [Repeated_Culvert_within_the_same_project](#Repeated_Culvert_within_the_same_project)\n",
    "* [Check_Flag](#Check_Flag)\n",
    "* [Project_missing_AMT_ID](#Project_missing_AMT_ID)\n",
    "\n",
    "* Is the Pavement Worksheet using updated inventory and condition data (2019 APCS) for planned projects?\n",
    "* Is the Drainage Worksheet using updated inventory and condition data (Sept 2021 or later) for planned projects?\n",
    "* Is the TMS Worksheet using updated inventory and condition data (June 2021 or later) for planned projects?\n",
    "* Is this project in the Project Book but not in the PID Workplan?  (Applies to non-reservation planned projects in years 6 or 7, and to Long Lead planned projects with PA&ED allocation in year 4.)\n",
    "* Is this project in the PID Workplan but not in the Project Book?  (Applies to non-reservation planned projects in years 6 or 7, and to Long Lead planned projects with PA&ED allocation in year 4.)\n",
    "* For planned projects, does the Performance Tab include a response to \"Is any location within the project limits Ped/Bike accessible?\"\n",
    "* Does the CCA date in the AM Tool match PRSM?\n",
    "* Does the CCA section including performance need to be completed?  (Applies to projects with CCA date on or after July 1, 2020)\n",
    "\n",
    "## [New Checks](#NewChecks)\n",
    "* 'Check Safety Comment'\n",
    "#Check comments about ActID of E55 and E58\n",
    "#if ActID == E55 and E58 and the comments include: 'HQ added the activity and District needs to review it.', mark the project\n",
    "* 'TMS data update needed for the project?'\n",
    "\n",
    "## [Internal Checks](#InteralChecks)\n",
    "* Does amendment date needs to be removed?\n",
    "* Need to add Resource in the PID workplan?\n",
    "* PRG section needs amendment date?\n",
    "* PPC section needs amendment date?\n",
    "\n",
    "## [Export Internal Check Summary](#Export_internal_check_summary)\n",
    "* internal check summary (csv)\n",
    "\n",
    "\n",
    "## [Export Check Summary](#Export_Table1)\n",
    "* data check summary matrix (csv)\n",
    "* data check summary punchlist (csv, tableau datasource)\n",
    "* project missing AMT_ID (csv)\n",
    "\n",
    "\n",
    "## [Export repeated EA or EFIS](#Export_repated_EA_EFIS)\n",
    "* repeated EA (csv)\n",
    "* repeated EFIS (csv)\n",
    "\n",
    "## [Export repeated assets](#Export_repeated_assets)\n",
    "* repeated assets (csv, tableau datasource)\n",
    "\n",
    "\n",
    "## [Export Projectbook](#Export_ProjectBook)\n",
    "* Project book for District asset manager (csv, tableau datasource)\n",
    "* Project book for HQ internal use (csv)\n",
    "\n",
    "\n",
    "## [Export Projectbook Check Sumary Key Dates](#Export_KeyDates)\n",
    "\n",
    "\n",
    "\n",
    "## [Final Clean Up](#FinalCleanUp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-mozambique",
   "metadata": {},
   "source": [
    "# Import common modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fundamental-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "\n",
    "# import requests\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "second-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intellectual-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show dataframe without skip column\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acquired-istanbul",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the Extract API 2.0, please save the output as .hyper format\n"
     ]
    }
   ],
   "source": [
    "# from config_datasource import *\n",
    "# from projectbookcheck_utilityfunction import *\n",
    "from constants import *\n",
    "import projectbookcheck_utilityfunction as uf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-aspect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "comic-disaster",
   "metadata": {},
   "source": [
    "# General Approach\n",
    "\n",
    "use SHOPP raw data as basis for data checks. \n",
    "Each project only occupies one line\n",
    "\n",
    "can expand columns, only if it will not create duplicate rows in the SHOPP raw dataset. \n",
    "\n",
    "\n",
    "Question: what is the entire list of projects, how to include projects without AMT_ID (SHOPP ID)\n",
    "Ans: use raw data, check missing SHOPP ID seperately \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-museum",
   "metadata": {},
   "source": [
    "# Data update procedure\n",
    "\n",
    "data schema: name, data type, default value if missing\n",
    "\n",
    "Option 1: Keep Excel column header fixed (number and sequence)\n",
    "\n",
    "Option 2: Maintain a dictionary of Excel column name and dataframe column name\n",
    "\n",
    "\n",
    "\n",
    "# Check update procedure\n",
    "\n",
    "Create utility function in python, in seperate module\n",
    "add the additional check in the main ETL module\n",
    "update the final data visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-celebrity",
   "metadata": {},
   "source": [
    "# Data clean process\n",
    "\n",
    "* funding amount: remove dollar sign, \n",
    "* fill missing value, string, numerical, \n",
    "* remove leading single quote for string value\n",
    "* strip off leading and trailing space \n",
    "\n",
    "* regulate column names\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-theory",
   "metadata": {},
   "source": [
    "<a id='GlobalConstants'></a>\n",
    "## Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sorted-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_FY = uf.fiscalyear(datetime.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-hindu",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "distinct-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#override to get the live data\n",
    "# DATA_SOURCE_TYPE = 'live'\n",
    "\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "\n",
    "# PROJECTBOOKCHECK_INPUT_FOLDER = r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_DataChecksSupport\\dev\\input'\n",
    "# PROJECTBOOKCHECK_HTTPSERVER_FOLDER = r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_DataChecksSupport\\dev\\output'\n",
    "# PROJECTBOOKCHECK_OUTPUT_FOLDER = r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_DataChecksSupport\\dev\\output'\n",
    "\n",
    "# LOG_FILE = r'\\\\ct.dot.ca.gov\\dfshq\\DIROFC\\Asset Management\\4e Project Book\\Projectbook_DataChecksSupport\\dev\\log\\ProjectBookExport.log'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-ready",
   "metadata": {},
   "source": [
    "<a id='Read_Data'></a>\n",
    "\n",
    "# Read Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "suitable-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DO NOT USE LIVE DATA, USE THE 5AM DATA LAKE ARCHIVES INSTEAD:\n",
    "# # for live data reading code, go to get_live_data.py\n",
    "# #it takes about 5 minutes to read live data, instead of 0.5 minute to read archived csv files.\n",
    "\n",
    "# if DATA_SOURCE_TYPE == 'live':\n",
    "\n",
    "#     import urllib\n",
    "\n",
    "#     #get live data\n",
    "#     dict_resource = {\n",
    "#         'TenYrShopp_RawData_' : \"http://10.56.12.86/pirs/tenyrshopp/Raw_data.cfm?selectdistrict=all&selectcounty=all&Route=&program=all&pid_cycle=all&shopp_yr=all&tenyearshopp=all&EA=&pID=&projectID=&proj_prog=all&report=Rawdata\"\n",
    "#         ,\n",
    "#         'TenYrShopp_PerfM_Raw_Data_': \"http://10.56.12.86/pirs/tenyrshopp/Raw_data_PerfM.cfm?selectdistrict=all&selectcounty=all&Route=&program=all&pid_cycle=all&shopp_yr=all&tenyearshopp=all&EA=&pID=&projectID=&proj_prog=all&fsection=all\"\n",
    "#         ,\n",
    "#         'Rawdata_Pavement_Worksheet_' : \"http://10.56.12.86/pirs/tenyrshopp/Rawdata_Pavement_WS.cfm?Program=SHOPP&fsection=all\"\n",
    "#         ,\n",
    "#         'Rawdata_Drainage_Worksheet_' : \"http://10.56.12.86/pirs/tenyrshopp/Rawdata_Drainage_WS.cfm?Program=SHOPP&fsection=all\"\n",
    "#         ,\n",
    "#         'Rawdata_Bridge_Worksheet_' : \"http://10.56.12.86/pirs/tenyrshopp/Rawdata_Bridge_WS.cfm?Program=SHOPP&fsection=all\"\n",
    "#         ,\n",
    "#         'Rawdata_TMS_Worksheet_' : \"http://10.56.12.86/pirs/tenyrshopp/Rawdata_TMS_WS.cfm?Program=SHOPP&fsection=all\"\n",
    "#         ,\n",
    "#         'PAC_Perfomrance_RawData_all_' : \"http://10.56.12.86/pirs/tenyrshopp/PAC_Performance_RawData.cfm?District=All\"\n",
    "#         ,\n",
    "#         'Project_Postmile_Check_' : \"http://10.56.12.86/pirs/TenYrShopp/project_Locations.cfm?District=all&view=All&program=All\",\n",
    "\n",
    "#         'Programming_Summary_' : \"http://10.56.12.86/pirs/tenyrshopp/?District=All&program=All&fsection=All&pID=&Placeholder=All&SType=A&page=RawdataProg&report=RawdataProg&selectdistrict=All&getreport=yes&submit=Get+Report\",\n",
    "\n",
    "#         'Minor_Project_Details_Raw_Data_' : 'http://10.56.12.86/pirs/tenyrshopp/?select&Route=&fPType=All&fAllocated=All&fAwarded=All&program=All&pID=&ProjectID=&EA=&FYInUse=&page=RawdataM&report=RawdataM&selectdistrict=All&getreport=yes&submit=Get+Report',\n",
    "\n",
    "#         'HM_Project_Details_Raw_Data_' : 'http://10.56.12.86/pirs/tenyrshopp/?selectcounty=All&Route=&program=All&pID=&ProjectID=&EA=&FYInUse=&Placeholder=All&page=RawdataHM&report=RawdataHM&selectdistrict=All&getreport=yes&submit=Get+Report'\n",
    "#     }\n",
    "\n",
    "#     sleep = 1\n",
    "\n",
    "#     filename = 'TenYrShopp_RawData_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_SHOPP_raw_data = pd.read_html(response.read())[-1]\n",
    "\n",
    "#     time.sleep(sleep)\n",
    "#     filename = 'TenYrShopp_PerfM_Raw_Data_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_perf_raw_data = pd.read_html(response.read())[-1]\n",
    "\n",
    "\n",
    "\n",
    "#     time.sleep(sleep)\n",
    "#     filename = 'Rawdata_Pavement_Worksheet_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_pav_raw_data = pd.read_html(response.read())[-1]\n",
    "#     df_pav_raw_data.columns = df_pav_raw_data.columns.droplevel()\n",
    "#     df_pav_raw_data.columns = df_pav_raw_data.columns.droplevel()\n",
    "\n",
    "#     time.sleep(sleep)\n",
    "#     filename = 'Rawdata_Drainage_Worksheet_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_drain_raw_data = pd.read_html(response.read())[-1]\n",
    "\n",
    "\n",
    "#     time.sleep(sleep)    \n",
    "#     filename = 'Rawdata_Bridge_Worksheet_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_brg_raw_data = pd.read_html(response.read())[-1]\n",
    "#     df_brg_raw_data.columns = df_brg_raw_data.columns.droplevel()\n",
    "\n",
    "\n",
    "#     time.sleep(sleep)    \n",
    "#     filename = 'Rawdata_TMS_Worksheet_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_tms_raw_data = pd.read_html(response.read())[-1]\n",
    "\n",
    "\n",
    "#     time.sleep(sleep)    \n",
    "#     filename = 'Project_Postmile_Check_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_pm_check = pd.read_html(response.read())[-1]\n",
    "\n",
    "\n",
    "#     time.sleep(sleep)\n",
    "#     filename = 'Programming_Summary_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_program_summary = pd.read_html(response.read())[-1]\n",
    "\n",
    "\n",
    "#     time.sleep(sleep)    \n",
    "#     filename = 'HM_Project_Details_Raw_Data_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_HM_raw_data = pd.read_html(response.read())[-1]\n",
    "\n",
    "\n",
    "#     time.sleep(sleep)    \n",
    "#     filename = 'Minor_Project_Details_Raw_Data_'\n",
    "#     url = dict_resource[filename]\n",
    "#     with urllib.request.urlopen(url) as response:\n",
    "#         df_Minor_raw_data = pd.read_html(response.read())[-1]\n",
    "        \n",
    "        \n",
    "#     DATA_HHMM = datetime.now().strftime(\"%H%M\")\n",
    "# else:\n",
    "#     print('skip getting live data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "artificial-handy",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\output\\\\DATALAKE\\\\TenYrShopp_RawData_.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-baa645f2dadf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mDATA_SOURCE_TYPE\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'csv'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'TenYrShopp_RawData_'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdf_SHOPP_raw_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'{}\\{}{}.csv'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATALAKE_FOLDER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFile_TimeStamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'TenYrShopp_PerfM_Raw_Data_'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dataprep\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.\\\\output\\\\DATALAKE\\\\TenYrShopp_RawData_.csv'"
     ]
    }
   ],
   "source": [
    "File_TimeStamp = '' #always read the latest timestamped file\n",
    "\n",
    "if DATA_SOURCE_TYPE == 'csv':\n",
    "    filename = 'TenYrShopp_RawData_'\n",
    "    df_SHOPP_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp))\n",
    "\n",
    "    filename = 'TenYrShopp_PerfM_Raw_Data_'\n",
    "    df_perf_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp))\n",
    "\n",
    "    filename = 'Rawdata_Bridge_Worksheet_'\n",
    "    df_brg_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), skiprows = [0], header = 0)\n",
    "\n",
    "    filename = 'Rawdata_Pavement_Worksheet_'\n",
    "    df_pav_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), skiprows = [0], header = 1)\n",
    "\n",
    "    filename = 'Rawdata_Drainage_Worksheet_'\n",
    "    df_drain_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), header = 0)\n",
    "\n",
    "    filename = 'Rawdata_TMS_Worksheet_'\n",
    "    df_tms_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), header = 0)\n",
    "\n",
    "    filename = 'Project_Postmile_Check_'\n",
    "    df_pm_check = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), header = 0)\n",
    "\n",
    "    filename = 'Programming_Summary_'\n",
    "    df_program_summary = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp), header = 0)\n",
    "\n",
    "\n",
    "    filename = 'Minor_Project_Details_Raw_Data_'\n",
    "    df_Minor_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp))\n",
    "\n",
    "\n",
    "    filename = 'HM_Project_Details_Raw_Data_'\n",
    "    df_HM_raw_data = pd.read_csv(r'{}\\{}{}.csv'.format(DATALAKE_FOLDER, filename, File_TimeStamp))\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print('skip getting csv data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-jamaica",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'TenYrShopp_RawData_'\n",
    "path_to_file = r'{}\\{}.csv'.format(DATALAKE_FOLDER, filename)\n",
    "t = os.path.getmtime(path_to_file)\n",
    "\n",
    "# File_TimeStamp = datetime.fromtimestamp(t).strftime(\"%m-%d-%Y_%H-%M\")\n",
    "Data_TimeStamp = datetime.fromtimestamp(t).strftime(\"%m-%d-%Y %H:%M:%S\")\n",
    "\n",
    "TARGETDATE = datetime.fromtimestamp(t).strftime(\"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-marketing",
   "metadata": {},
   "source": [
    "## read candidate project ID list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Programmed_Projects_check_postmile&performance.xlsx'\n",
    "\n",
    "df_programmed_projects = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "industrial-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_programmed_projects['AMT_ID'] = df_programmed_projects['AMT_ID'].dropna()\n",
    "\n",
    "# df_programmed_projects['AMT_ID']= df_programmed_projects['AMT_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-reservation",
   "metadata": {},
   "source": [
    "## Read the AMT_ID with TMS data change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'AM tool Ids_TMS needs to update if in POR.xlsx'\n",
    "\n",
    "df_TMS_Datachange = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "sht = 'KeyDates'\n",
    "\n",
    "filename = 'GlobalParameters.xlsx'\n",
    "\n",
    "df_GlobalParameters = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), sheet_name =sht) \n",
    "\n",
    "TMS_Data_Date = df_GlobalParameters.loc[0,'TMS_Data_Date'].to_pydatetime().strftime(\"%m-%d-%Y\")\n",
    "\n",
    "TARGET_FY = df_GlobalParameters.loc[0,'TARGET_FY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "DD_Approval_Placeholder_Date = df_GlobalParameters.loc[0,'District Director Approval Placeholder Date'].to_pydatetime().strftime(\"%m/%d/%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DD_Approval_Placeholder_Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-handling",
   "metadata": {},
   "source": [
    "<a id='Minor_Raw_Data'></a>\n",
    "\n",
    "## Minor and HM Raw Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solid-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_Minor_raw_data['District'] = df_Minor_raw_data['District'].apply(uf.remove_punction)\n",
    "df_Minor_raw_data['District'] = df_Minor_raw_data['District'].astype(int)\n",
    "\n",
    "df_Minor_raw_data['Unique EA'] = df_Minor_raw_data.apply(uf.calc_unique_EA, axis = 1)\n",
    "\n",
    "dict_rename = {\n",
    "    'Project ID':'EFIS'\n",
    "              }\n",
    "df_Minor_raw_data = df_Minor_raw_data.rename(dict_rename, axis = 1)\n",
    "\n",
    "#conver EFIS to numeric, \n",
    "#filter null and 0 \n",
    "\n",
    "\n",
    "df_Minor_raw_data['EFIS'] = pd.to_numeric(df_Minor_raw_data['EFIS'], errors='coerce')\n",
    "df_Minor_raw_data = df_Minor_raw_data[(df_Minor_raw_data['EFIS'].notna()) & df_Minor_raw_data['EFIS'] != 0]\n",
    "\n",
    "#rename ID\n",
    "dict_rename = {\n",
    "    'ID': 'AMT_ID',\n",
    "    }\n",
    "df_Minor_raw_data= df_Minor_raw_data.rename(dict_rename, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_HM_raw_data['Unique EA'] = df_HM_raw_data.apply(uf.calc_unique_EA, axis = 1)\n",
    "\n",
    "df_HM_raw_data['EFIS'] = pd.to_numeric(df_HM_raw_data['EFIS'], errors='coerce')\n",
    "df_HM_raw_data = df_HM_raw_data[(df_HM_raw_data['EFIS'].notna()) & df_HM_raw_data['EFIS'] != 0]\n",
    "\n",
    "#rename ID\n",
    "dict_rename = {'ID': 'AMT_ID',}\n",
    "df_HM_raw_data= df_HM_raw_data.rename(dict_rename, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-exception",
   "metadata": {},
   "source": [
    "<a id='SHOPP_Raw_Data'></a>\n",
    "## SHOPP Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data.name = 'df_SHOPP_raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-nature",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "\n",
    "dict_rename_rawdata = {\n",
    "                       'County':'County TYP',\n",
    "                       'Route': 'Route TYP',\n",
    "                       'BackPM':'BackPM TYP',\n",
    "                       'AheadPM':'AheadPM TYP',\n",
    "                       'ID': 'AMT_ID',\n",
    "                       'Ten-Year Plan': 'Ten-Year Plan RD',\n",
    "                       'County.1' : 'County PRG',\n",
    "                       'Route.1' : 'Route PRG',\n",
    "                       'BackPM.1':'BackPM PRG',\n",
    "                       'AheadPM.1' : 'AheadPM PRG',\n",
    "                       'County.2' : 'County PCR',\n",
    "                       'Route.2' : 'Route PCR',\n",
    "                       'BackPM.2':'BackPM PCR',\n",
    "                       'AheadPM.2' : 'AheadPM PCR',\n",
    "                       'Activity Category': 'Activity'\n",
    "                      }\n",
    "df_SHOPP_raw_data = df_SHOPP_raw_data.rename(dict_rename_rawdata, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove leading puncture for target columns\n",
    "cols_strip = ['District','Route TYP','EA','EFIS','Route PRG','PPNO','Route PCR']\n",
    "for c in cols_strip :\n",
    "    df_SHOPP_raw_data[c] = df_SHOPP_raw_data[c].str.strip(\"'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cost_columns = [\n",
    "    'RW Cost ($K)',\n",
    "    'Const Cost ($K)',\n",
    "    'Support Cost ($)',\n",
    "    'TYP Total Project Cost ($K)',\n",
    "    'PAED ($K)',\n",
    "    'PSE ($K)',\n",
    "    'R/W ($K)',\n",
    "    'CONS ($K)',\n",
    "    'Prog Support Cost ($)',\n",
    "    'Prog RW Cost ($K)',\n",
    "    'Prog Const Cost ($K)',\n",
    "    'Prog Total Project Cost ($K)',\n",
    "    'PCR R/W Cap ($K)',\n",
    "    'PCR Const Cap ($K)',\n",
    "    'PCR Support Cost ($K)',\n",
    "    'PCR Total Cost ($K)',\n",
    "    'Project Cost In Use',\n",
    "    'Total LL Prog ($K)',\n",
    "    'LL PAED Cost ($K)',\n",
    "    'LL CONS Cap ($K)'\n",
    "               ]\n",
    "\n",
    "for c in cost_columns:\n",
    "    df_SHOPP_raw_data[c] = df_SHOPP_raw_data[c].apply(uf.curreny_to_float)\n",
    "    df_SHOPP_raw_data[c].fillna(0, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data clean \n",
    "#data type regulation\n",
    "#string/text data regulation\n",
    "\n",
    "df_SHOPP_raw_data['District'] =df_SHOPP_raw_data['District'].astype(int)\n",
    "\n",
    "df_SHOPP_raw_data['EFIS'] = pd.to_numeric(df_SHOPP_raw_data['EFIS'], errors='coerce')\n",
    "\n",
    "df_SHOPP_raw_data['Route PCR'] = df_SHOPP_raw_data['Route PCR'].astype(str)\n",
    "\n",
    "#data trimming\n",
    "#row trimming\n",
    "df_SHOPP_raw_data= df_SHOPP_raw_data[df_SHOPP_raw_data['District'] != 56]\n",
    "\n",
    "#column trimming\n",
    "df_SHOPP_raw_data.drop(['District Priority', 'PIR Performance Report'],\n",
    "  axis='columns', inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "#Question to be answered:\n",
    "#for EA Raw Data, the missing data is not null , but ''. Should we handle the missing data uniformly for string data?\n",
    "\n",
    "#TODO:\n",
    "#fill missing data\n",
    "#data quality check (checksum,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['CCA Date Miilestone (M600)'] = df_SHOPP_raw_data['CCA Date Miilestone (M600)'].apply(uf.regulate_timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civilian-belief",
   "metadata": {},
   "source": [
    "<a id='TenYrShopp_Perf_RawData'></a>\n",
    "## TenYrShopp_Perf_RawData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "dict_rename_perf_rawdata = {\n",
    "                           'ID': 'AMT_ID',\n",
    "#                             'ProjectedRTL FY': 'Projected RTL FY',\n",
    "    \n",
    "# 'ActID':'Performance_ActID',\n",
    "# 'Quantity':    'Performance_Quantity'\n",
    "              }\n",
    "\n",
    "df_perf_raw_data = df_perf_raw_data.rename(dict_rename_perf_rawdata, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_strip = ['EA','EFIS','PPNO']\n",
    "for c in cols_strip :\n",
    "    df_perf_raw_data[c] = df_perf_raw_data[c].str.strip(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-calendar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-economy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data clean \n",
    "#data type regulation\n",
    "\n",
    "df_perf_raw_data['Quantity'] = df_perf_raw_data['Quantity'].fillna(0)\n",
    "df_perf_raw_data['Assets in Good Cond'] = df_perf_raw_data['Assets in Good Cond'].fillna(0)\n",
    "df_perf_raw_data['Assets in Fair Cond'] = df_perf_raw_data['Assets in Fair Cond'].fillna(0)\n",
    "df_perf_raw_data['Assets in Poor Cond'] = df_perf_raw_data['Assets in Poor Cond'].fillna(0)\n",
    "df_perf_raw_data['New Assets Added'] = df_perf_raw_data['New Assets Added'].fillna(0)\n",
    "\n",
    "df_perf_raw_data['EFIS'] = pd.to_numeric(df_perf_raw_data['EFIS'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data trimming\n",
    "#row\n",
    "df_perf_raw_data= df_perf_raw_data[df_perf_raw_data['District'] != 56]\n",
    "#column\n",
    "df_perf_raw_data.drop(['PID Cycle', 'TYP','ProjectedSHOPP Cycle','RequestedRTL FY','DistrictPriority'],\n",
    "  axis='columns', inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data.name = 'df_perf_raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-tampa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-commerce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw = df_perf_raw_data.merge(df_SHOPP_raw_data, \n",
    "#                               how = 'outer', \n",
    "#                               left_on = 'AMT_ID', right_on = 'AMT_ID',\n",
    "#                              suffixes = ['_raw_perf', '_raw_data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-edition",
   "metadata": {},
   "source": [
    "<a id='Counties'></a>\n",
    "## Counties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Counties.xlsx'\n",
    "\n",
    "df_counties = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counties['Co. Name Abbr.'] = df_counties['Co. Name Abbr.'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counties.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counties.name = 'df_counties'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-neutral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_prog_county = df_perf_raw_prog_candidate.merge(df_counties, how = 'left', left_on = 'County', right_on = 'Co. Name Abbr.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no need for the following, already added to the df_perf_raw_data\n",
    "\n",
    "# #rename columns\n",
    "# dict_rename_4= {\n",
    "#                'Performance Objective':'Performance Objective Original', \n",
    "#               }\n",
    "\n",
    "# df_perf_raw_prog_county = df_perf_raw_prog_county.rename(dict_rename_4, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-healthcare",
   "metadata": {},
   "source": [
    "<a id='ProgrammingList'></a>\n",
    "## Programming List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "shts = ['2010 SHOPP',\n",
    "        '2012 SHOPP',\n",
    "        '2014 SHOPP',\n",
    "        '2016 SHOPP',\n",
    "        '2018 SHOPP',\n",
    "        '2020 SHOPP',\n",
    "        'Long Lead'\n",
    "        ]\n",
    "\n",
    "filename = 'Programming_list.xlsx'\n",
    "\n",
    "df_dict = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), sheet_name =shts) \n",
    "\n",
    "\n",
    "df_program = pd.DataFrame()\n",
    "\n",
    "for k, v in df_dict.items():\n",
    "#     print(type(v))\n",
    "#     print(k)\n",
    "    v['Table Names'] = k\n",
    "#     print(v.columns)\n",
    "    df_program = df_program.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns\n",
    "dict_rename_program = {\n",
    "#                         'EA':'EA', \n",
    "                        'EFIS':'EFIS_Program', \n",
    "#                         'PPNO':'PPNO Programming', \n",
    "                        'Total Capital & Support':'Total Capital & Support Cost',\n",
    "#                         'Route': 'Route Programming',\n",
    "                        'County': 'County_FullName'\n",
    "              }\n",
    "\n",
    "df_program = df_program.rename(dict_rename_program, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data clean \n",
    "#data type regulation\n",
    "# df_program['Dist'] = df_program['Dist'].astype(int)\n",
    "# df_program['EFIS'] = pd.to_numeric(df_program['EFIS'], errors='coerce')\n",
    "\n",
    "fillna_columns = ['Con Sup','RW Sup','PA&ED','PS&E', 'PA&ED', 'RW', 'Con']\n",
    "\n",
    "df_program[fillna_columns].fillna(0, inplace=True)\n",
    "df_program['Route'].fillna('Various', inplace=True)\n",
    "\n",
    "df_program['Support Cost'] = df_program['Con Sup']+df_program['RW Sup']+df_program['PA&ED']+df_program['PS&E']\n",
    "df_program['Capital Cost'] = df_program['Con']+df_program['RW']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program[['Con Sup','RW Sup','PA&ED','PS&E']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program['PA&ED'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove leading puncture for all string columns\n",
    "# df_program.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-twelve",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data augmentation\n",
    "#data transformation\n",
    "\n",
    "# df_program.dropna(subset = ['EFIS_Program'], inplace = True)\n",
    "df_program['FY'] = df_program['FY'].apply(uf.FY_cleanup)\n",
    "\n",
    "df_program['Begin Post Miles'] = df_program['Post Miles'].apply(lambda x: str(x).split('/')[0])\n",
    "df_program['End Post Miles'] = df_program['Post Miles'].apply(lambda x: str(x).split('/')[0] if '/' in str(x) else np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data trimming\n",
    "df_program.drop(['PM1BF', 'PM1B', 'PM1AF','PM1A',], axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program.name = 'df_program'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data sanity check\n",
    "#check duplicates\n",
    "#check null\n",
    "#check data type\n",
    "\n",
    "if df_program['EFIS_Program'].value_counts(dropna = False).max() > 1:\n",
    "    Print('Duplicate EFIS ID found, please check the source data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program['EFIS_Program'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-cancer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_counties[['County Name','Co. Name Abbr.']]\n",
    "temp.columns = ['County_FullName', 'County']\n",
    "\n",
    "df_program = pd.merge(df_program, temp, how='left', left_on = 'County_FullName', right_on = 'County_FullName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program.drop(['County_FullName'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-annex",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-maldives",
   "metadata": {},
   "source": [
    "<a id='SHOPP_Candidates'></a>\n",
    "## SHOPP candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-treaty",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'SHOPP candidates.xlsx'\n",
    "\n",
    "df_shopp_candidate = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rename_candidate = {\n",
    "                        'SHOPP ID':'AMT_ID', \n",
    "    'Advertised Year': 'Advertised Year_Candidate',\n",
    "    'Project Cost ($K)' : 'Project Cost ($K)_Candidate'\n",
    "              }\n",
    "\n",
    "df_shopp_candidate = df_shopp_candidate.rename(dict_rename_candidate, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-labor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shopp_candidate['AMT_ID'] = df_shopp_candidate['AMT_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['AMT_ID', 'Candidate Type','Advertised Year_Candidate','Project Cost ($K)_Candidate']\n",
    "df_shopp_candidate = df_shopp_candidate[export_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affecting-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shopp_candidate.name = 'df_shopp_candidate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_shopp_candidate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-server",
   "metadata": {},
   "source": [
    "<a id='Bridge_Inventory'></a>\n",
    "\n",
    "## Bridge Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-leather",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'GFP_BrInvList_AllDistricts.xlsx'\n",
    "\n",
    "df_bridge_inventory = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), skiprows = 4, header = [0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# filename = 'GFP_BrInvList_AllDistricts_March_2020_05042020.xlsx'\n",
    "\n",
    "# df_bridge_inventory = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), skiprows = 4, header = [0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = ['Bridge #', 'Deck Area, SF', 'Date', 'Health', 'Deck (58)',\n",
    "       'Super (59)', 'Sub (60)', 'Culv (62)', 'Scour', 'Seismic', 'Overall',\n",
    "       'VC', 'Permit', 'Rail Overall', 'Good, LF', 'Fair, LF', 'Poor, LF',\n",
    "       'Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bridge_inventory = df_bridge_inventory[keep_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "\n",
    "dict_rename_bridge_inventory = {'Bridge #':'BridgeNo',\n",
    "                               'Date':'Inspection Date', \n",
    "                               'Health': 'Bridge Health',\n",
    "                                'Scour':'Bridge Scour', \n",
    "                                'Seismic':'Bridge Seismic', \n",
    "                               'Deck (58)' : 'NBI Condition Ratings_Deck (58)',\n",
    "                                'Super (59)': 'NBI Condition Ratings_Super (59)',\n",
    "                                'Sub (60)':'NBI Condition Ratings_Sub (60)',\n",
    "                                'Culv (62)':'NBI Condition Ratings_Culv (62)',\n",
    "                                'Overall': 'Bridge Goods Movement_Overall',\n",
    "                                'VC': 'Bridge Goods Movement_VC',\n",
    "                                'Permit':'Bridge Goods Movement_Permit',\n",
    "                                'Rail Overall': 'Bridge Rail Upgrade_Rail Overall', \n",
    "                                'Good, LF': 'Bridge Rail Upgrade_Good, LF', \n",
    "                                'Fair, LF': 'Bridge Rail Upgrade_Fair, LF', \n",
    "                                'Poor, LF': 'Bridge Rail Upgrade_Poor, LF', \n",
    "                      }\n",
    "\n",
    "df_bridge_inventory.rename(dict_rename_bridge_inventory, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_to_good(value):\n",
    "    if value and value in ['Poor','Fair','Good']:\n",
    "        return value\n",
    "    else:\n",
    "        return 'Good'\n",
    "\n",
    "data_recondition_columns = ['Bridge Health','Bridge Scour','Bridge Seismic','Bridge Goods Movement_Overall',\n",
    "       'Bridge Goods Movement_VC', 'Bridge Goods Movement_Permit','Bridge Rail Upgrade_Rail Overall',]\n",
    "\n",
    "for c in data_recondition_columns:\n",
    "    df_bridge_inventory[c] = df_bridge_inventory[c].apply(default_to_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bridge_inventory.name = 'df_bridge_inventory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-dependence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bridge_inventory.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-prisoner",
   "metadata": {},
   "source": [
    "<a id='Bridge_Worksheet'></a>\n",
    "\n",
    "## Raw Data Bridge Worksheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-interest",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "#with manual edits\n",
    "\n",
    "dict_rename_bridge_worksheet = {\n",
    " 'ID': 'AMT_ID',\n",
    " 'Bridge №': 'BridgeNo',\n",
    " 'Work Type': 'WorkType',\n",
    " 'Brdige / TunnelWork Description': 'WorkDescription',\n",
    " 'Bridge /TunnelHealth Pre': 'Health Pre',\n",
    " 'Bridge /TunnelHealth Post': 'Health Post',\n",
    " 'BridgeScourPre': 'Scour_Pre',\n",
    " 'BridgeScourPost': 'Scour_Post',\n",
    " 'BridgeSeismicPre': 'Seismic_Pre',\n",
    " 'BridgeSeismicPost': 'Seismic_Post',\n",
    " 'BridgeGds MvmtPre': 'GdsMvmt_Pre',\n",
    " 'BridgeGds MvmtPost': 'GdsMvmt_Post',\n",
    " 'Exist(sf)': 'Deck_Exist(sf)',\n",
    " 'Additional(sf)': 'Deck_Additional(sf)',\n",
    " 'Y/N': 'Paint_Y/N',\n",
    " 'Condition': 'Paint_Condition',\n",
    " 'Paint Area(sf)': 'Paint Area(sf)',\n",
    " 'Y/N.1': 'ElectricalMechanical_Y/N',\n",
    " 'Condition.1': 'ElectricalMechanical_Condition',\n",
    " 'Area(sf)': 'ElectricalMechanical_Area(sf)',\n",
    " 'Y/N.2': 'ApproachSlab_Y/N',\n",
    " 'Replaced(sf)': 'ApproachSlab_Replaced(sf)',\n",
    " 'New(sf)': 'ApproachSlab_New(sf)',\n",
    " 'Y/N.3': 'Rail_Y/N',\n",
    " 'Good(lf)': 'Rail_Good(lf)',\n",
    " 'Fair(lf)': 'Rail_Fair(lf)',\n",
    " 'Poor(lf)': 'Rail_Poor(lf)',\n",
    " 'Additonal(lf)': 'Rail_Additonal(lf)',\n",
    " 'Post Good(lf)': 'Rail_Post Good(lf)',\n",
    " 'Post Fair(lf)': 'Rail_Post Fair(lf)',\n",
    " 'Post Poor(lf)': 'Rail_Post Poor(lf)',\n",
    " 'Post New(lf)': 'Rail_Post New(lf)',\n",
    " 'FishPassage(Y/N)': 'FishPassage(Y/N)',\n",
    "}\n",
    "\n",
    "df_brg_raw_data.rename(dict_rename_bridge_worksheet, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brg_raw_data.name = 'df_brg_raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-tanzania",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_brg_raw_data['Rail_Good(lf)'].fillna(0, inplace = True)\n",
    "df_brg_raw_data['Rail_Fair(lf)'].fillna(0, inplace = True)\n",
    "df_brg_raw_data['Rail_Poor(lf)'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brg_raw_data['Rail_Total(lf)'] = (df_brg_raw_data['Rail_Good(lf)'] \n",
    "                                             + df_brg_raw_data[ 'Rail_Fair(lf)'] \n",
    "                                             + df_brg_raw_data['Rail_Poor(lf)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-shipping",
   "metadata": {},
   "source": [
    "<a id='Pavement_Worksheet'></a>\n",
    "\n",
    "## Raw Data Pavement Worksheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-accounting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "#with manual editing\n",
    "\n",
    "dict_rename_pavement_worksheet = {\n",
    " 'ID': 'AMT_ID',\n",
    " 'Class': 'RoadwayClass',\n",
    " 'Green': 'TriditionalCondition_Green',\n",
    " 'Yellow': 'TriditionalCondition_Yellow',\n",
    " 'Blue': 'TriditionalCondition_Blue',\n",
    " 'Orange': 'TriditionalCondition_Orange',\n",
    " 'Red': 'TriditionalCondition_Red',\n",
    " 'Good': 'MAP21_Good',\n",
    " 'Fair': 'MAP21_Fair',\n",
    " 'Poor': 'MAP21_Poor',\n",
    " 'Total LaneMiles': 'Total LaneMiles',\n",
    " 'Green.1': 'TriditionalCondition_Post_Green',\n",
    " 'Yellow.1': 'TriditionalCondition_Post_Yellow',\n",
    " 'Blue.1': 'TriditionalCondition_Post_Blue',\n",
    " 'Orange.1': 'TriditionalCondition_Post_Orange',\n",
    " 'Red.1': 'TriditionalCondition_Post_Red',\n",
    " 'Good.1': 'MAP21_Post_Good',\n",
    " 'Fair.1': 'MAP21_Post_Fair',\n",
    " 'Poor.1': 'MAP21_Post_Poor',\n",
    "#  'Date': 'Date'\n",
    "                               }\n",
    "df_pav_raw_data.rename(dict_rename_pavement_worksheet, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-potter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pav_raw_data.name = 'df_pav_raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pav_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-williams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pav_raw_data['Plan Year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-canvas",
   "metadata": {},
   "source": [
    "<a id='Drainage_Worksheet'></a>\n",
    "## Raw Data Drainage Worksheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drain_raw_data.name = 'df_drain_raw_data'\n",
    "\n",
    "dict_drain_rename = {\n",
    " 'ID': 'AMT_ID',\n",
    " 'Data Date':'Data Date_Drainage'\n",
    "                               }\n",
    "df_drain_raw_data.rename(dict_drain_rename, axis = 1, inplace = True)\n",
    "\n",
    "cols = ['EA','EFIS','SYSNO','INETNO','OUTETNO']\n",
    "for c in cols: \n",
    "    df_drain_raw_data[c] = df_drain_raw_data[c].apply(uf.remove_punction)\n",
    "\n",
    "\n",
    "df_drain_raw_data['Data Date_Drainage'] = df_drain_raw_data['Data Date_Drainage'].apply(uf.regulate_timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#question to be answered: what to do with meanless or null values.\n",
    "#return null if DrainageWorksheet_SYSNO,DrainageWorksheet_INETNO,DrainageWorksheet_OUTETNO are null or empty\n",
    "\n",
    "# df_drain_raw_data['Unique Culvert ID'] = (df_drain_raw_data['SYSNO'] + \"_\"\n",
    "#                                           + df_drain_raw_data['INETNO'] + \"_\"\n",
    "#                                          + df_drain_raw_data['OUTETNO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_drain_raw_data['SYSNO'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_drain_unique_ID(df):\n",
    "    if pd.isnull(df['SYSNO']) or pd.isnull(df['INETNO']) or pd.isnull(df['OUTETNO']):\n",
    "        return None\n",
    "    else:\n",
    "        return (df['SYSNO'] + \"_\"+ df['INETNO'] + \"_\"+ df['OUTETNO'])\n",
    "df_drain_raw_data['Unique Culvert ID'] = df_drain_raw_data.apply(calc_drain_unique_ID, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_drain_raw_data.select_dtypes(include=[object]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (columnName, columnData) in df_drain_raw_data.select_dtypes(include=[object]).iteritems():\n",
    "#     df_drain_raw_data[columnName]=columnData.apply(remove_punction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_drain_ws = df_drain_raw_data.merge(df_SHOPP_raw_data, how = 'left', \n",
    "#                   left_on = ['AMT_ID', 'Date'], \n",
    "#                   right_on = ['AMT_ID', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romance-patio",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "virgin-conclusion",
   "metadata": {},
   "source": [
    "<a id='TMS_Worksheet'></a> \n",
    "## Raw Data TMS Worksheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_TMS_rename = {\n",
    " 'ID': 'AMT_ID',\n",
    " 'Data Date':'Data Date_TMS'\n",
    "                               }\n",
    "df_tms_raw_data.rename(dict_TMS_rename, axis = 1, inplace = True)\n",
    "\n",
    "# df_tms_ws = df_tms_raw_data.merge(df_SHOPP_raw_data, how = 'left', \n",
    "#                   left_on = ['AMT_ID', 'Date'], \n",
    "#                   right_on = ['AMT_ID', 'Date'])\n",
    "\n",
    "df_tms_raw_data.name = 'df_tms_raw_data'\n",
    "\n",
    "df_tms_raw_data.shape\n",
    "\n",
    "\n",
    "\n",
    "df_tms_raw_data['Data Date_TMS'] = df_tms_raw_data['Data Date_TMS'].apply(uf.regulate_timestamp_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-processor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "classified-boost",
   "metadata": {},
   "source": [
    "<a id='Postmile_Check'></a>\n",
    "## Postmile Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-recycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_PM_ck_rename = {\n",
    " 'ID': 'AMT_ID',\n",
    " '№': 'No'                            }\n",
    "df_pm_check.rename(dict_PM_ck_rename, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pm_check['District'] = df_pm_check['District'].str.strip(\"'\")\n",
    "df_pm_check['District'] =df_pm_check['District'].astype(int)\n",
    "df_pm_check = df_pm_check[df_pm_check['District']!= 56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pm_check = df_pm_check[df_pm_check['Program'] == 'SHOPP']\n",
    "df_pm_check['AMT_ID'] = df_pm_check['AMT_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "differential-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pm_check.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pm_check.columns = ['No', 'District', 'AMT_ID', 'EA', 'EFIS', 'Location',\n",
    "#        'Section', 'County', 'Route', 'BackPM', 'AheadPM',\n",
    "#        'Alignment', 'Valid PM', 'Activity Category', 'Program',\n",
    "#        'BackPMLatitude', 'BackPMLongitude',\n",
    "#        'BackPMAssemblyDistrict', 'BackPMCongressDistrict',\n",
    "#        'BackPMSenateDistrict', 'AheadPMLatitude',\n",
    "#        'AheadPMLongitude', 'AheadPMAssemblyDistrict',\n",
    "#        'AheadPMCongressDistrict', 'AheadPMSenateDistrict',\n",
    "#        'AssemblyDistrict(s)', 'CongressDistrict(s)',\n",
    "#        'SenateDistrict(s)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pm_check.name = 'df_pm_check'\n",
    "df_pm_check.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataCheck_perf_loc = df_dataCheckFlow_all_ws.merge(df_pm_check, how = 'outer', \n",
    "#                   left_on = ['AMT_ID','Date'], \n",
    "#                   right_on = ['AMT_ID', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataCheck_perf_loc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataCheck_perf_loc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-ensemble",
   "metadata": {},
   "source": [
    "<a id='PID_Workload'></a>\n",
    "\n",
    "## 2022 PID workload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PID_workload = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = 'PID SHOPP WL.xlsx'\n",
    "shts = ['PID workload', 'ColumnHeaderDictionary']\n",
    "\n",
    "dict_df = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), sheet_name = shts) \n",
    "\n",
    "df_PID_workload = dict_df['PID workload']\n",
    "df_rename = dict_df['ColumnHeaderDictionary'].iloc[:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns \n",
    "\n",
    "# dict_rename_PID_workplan = {'SHOPP Tool ID': 'AMT_ID',\n",
    "#                                }\n",
    "\n",
    "dict_col_rename = dict(zip(df_rename.OriginalName, df_rename.NewName))\n",
    "df_PID_workload.rename(dict_col_rename, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PID_workload['District'] = df_PID_workload['District'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PID_workload.name = 'df_PID_workload'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_PID_workload.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-feelings",
   "metadata": {},
   "source": [
    "<a id='Check_Exceptions'></a>\n",
    "## Check Exceptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Checks_exception.xlsx'\n",
    "\n",
    "df_ck_exceptions = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ck_exceptions.name = 'df_ck_exceptions'\n",
    "df_ck_exceptions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rename_ck_exception = {'Exception ID': 'AMT_ID',\n",
    "                               }\n",
    "df_ck_exceptions.rename(dict_rename_ck_exception, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataCheckwException = df_dataCheck_PID.merge(df_ck_exceptions, how = 'outer', \n",
    "#                   left_on = ['AMT_ID'], \n",
    "#                   right_on = ['AMT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df_ck_exceptions.shape)\n",
    "# print(df_dataCheckwException.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question\n",
    "#we will not ensure only one entry in the exception\n",
    "#we need to consider all exception\n",
    "#  df_ck_exceptions[(df_ck_exceptions['AMT_ID'] == AMT_ID) & ( df_ck_exceptions['Type of Exception'] =='Repeated Culvert')].shape[0]>0:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-cookie",
   "metadata": {},
   "source": [
    "<a id='Project_Obselete'></a>\n",
    "## Project to be obseleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Combined projects.xlsx'\n",
    "\n",
    "df_project_obselete = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_project_obselete.name = 'df_project_obselete'\n",
    "# df_project_obselete.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_rename_project_obselete = {'TOOL ID to obsolete': 'AMT_ID to obsolete',\n",
    "                               }\n",
    "df_project_obselete.rename(dict_rename_project_obselete, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataCheck_combined =  df_dataCheckwException.merge(df_project_obselete, how = 'left', \n",
    "#                   left_on = ['AMT_ID'], \n",
    "#                   right_on = ['TOOL ID to obsolete'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_project_obselete['TOOL ID to obsolete'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-comedy",
   "metadata": {},
   "source": [
    "<a id='Project_Detail_Report'></a>\n",
    "## Project Detail Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'project_detail_report_nickname.xlsx'\n",
    "\n",
    "# df_project_detail =pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), sheet_name = 'project_detail_report') \n",
    "\n",
    "\n",
    "\n",
    "# shts = ['project_detail_report','Instruction']\n",
    "\n",
    "# dict_df = pd.read_excel(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename), sheet_name = shts) \n",
    "\n",
    "# df_project_detail = dict_df['project_detail_report']\n",
    "\n",
    "# datatimestamp = dict_df['Instruction'].iloc[0,0]\n",
    "# df_project_detail['QMRS Report Date'] = datatimestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Project Detail Report.csv'\n",
    "\n",
    "df_project_detail =pd.read_csv(r'{}\\{}'.format(PROJECTBOOKCHECK_INPUT_FOLDER, filename)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_cca_date(df):\n",
    "    \n",
    "    if pd.isna(df['Cca Finish Date']):\n",
    "        return np.nan\n",
    "    \n",
    "    dt = datetime.strptime(df['Cca Finish Date'], '%d-%b-%y') \n",
    "    \n",
    "    if dt < datetime.strptime('01-01-1978', '%M-%d-%Y'):\n",
    "        dt = datetime(dt.year+100, dt.month, dt.day, dt.hour, dt.minute, dt.second, dt.microsecond, dt.tzinfo)\n",
    "    return dt.strftime(\"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_project_detail['Cca Finish Date'] = df_project_detail.apply(cleanup_cca_date, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_project_detail.name = 'df_project_detail'\n",
    "# df_project_detail.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rename_project_detail = {'Project ID': 'EFIS',\n",
    "                               }\n",
    "df_project_detail.rename(dict_rename_project_detail, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_project_detail['EFIS'] = pd.to_numeric(df_project_detail['EFIS'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataCheckwNickname = df_dataCheck_combined.merge(df_project_detail, how = 'left', \n",
    "#                   left_on = ['EFIS'], \n",
    "#                   right_on = ['Project ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-authorization",
   "metadata": {},
   "source": [
    "<a id='Programming_Summary'></a>\n",
    "## Programming Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-lighting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data trimming\n",
    "#row\n",
    "df_program_summary = df_program_summary[df_program_summary['Program'] == 'SHOPP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporary for debug\n",
    "df_program_summary['ID'] = df_program_summary['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_rename_program_summary = {'ID': 'AMT_ID',\n",
    "                               }\n",
    "df_program_summary.rename(dict_rename_program_summary, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-convention",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_summary.dropna(subset = ['District'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_summary['Performance Value'] = df_program_summary['Performance Value'].astype(str)\n",
    "df_program_summary['Performance Value'].fillna('N/A', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove leading puncture for all string columns\n",
    "col_strip=['EA', 'EFIS',  'Program Code',]\n",
    "for c in col_strip:\n",
    "    df_program_summary[c] = df_program_summary[c].str.strip(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_summary['EFIS'] = pd.to_numeric(df_program_summary['EFIS'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program_summary['EFIS'].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program_summary['AMT_ID'] = df_program_summary['AMT_ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-killing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_summary['Performance Value'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_summary['Performance Value'] = df_program_summary['Performance Value'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_summary.name = 'df_program_summary'\n",
    "# df_program_summary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-blend",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert percentage to actual quantity\n",
    "\n",
    "def percent_to_quantity(df, col, subtotal_col):\n",
    "    if isinstance(df[c], str) and '%' in df[c]:\n",
    "        return (float(df[c].strip('%'))/100) * df[subtotal_col]\n",
    "    else:\n",
    "        return float(df[c])\n",
    "\n",
    "ck_cols = [ 'Pre-Good', 'Pre-Fair', 'Pre-Poor',]\n",
    "\n",
    "for c in ck_cols:\n",
    "    df_program_summary[c] = df_program_summary.apply(percent_to_quantity, args=(c,'Pre-Total'), axis = 1)\n",
    "\n",
    "ck_cols = [ 'Post Good', 'New', 'Post Good+New', 'Post-Fair',\n",
    "       'Post-Poor']\n",
    "\n",
    "for c in ck_cols:\n",
    "    df_program_summary[c] = df_program_summary.apply(percent_to_quantity, args=(c,'Post-Total'), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-defensive",
   "metadata": {},
   "source": [
    "<a id='AddDataColumns'></a>\n",
    "## Calculate and join additional fields\n",
    "\n",
    "- Section\n",
    "- County\n",
    "- Route\n",
    "- End Postmile\n",
    "- Begin Postmile\n",
    "\n",
    "\n",
    "* Advertised Year\n",
    "* Last Year FY POR\n",
    "* Last Year of Fiscal Year\n",
    "* Will this project be included in the Project Book?\n",
    "* AM Tool RTL (Section in Use)\n",
    "* FY\n",
    "* Activity (group)\n",
    "\n",
    "* Total Project Cost ($K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-waters",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['EA_'] = df_SHOPP_raw_data['EA'].apply(lambda x: \"'\" + x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-affect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'FY','Total Capital & Support Cost' to raw data\n",
    "df_SHOPP_raw_data.drop(columns =['EFIS_Program','FY','Total Capital & Support Cost','Capital Cost', 'Support Cost'], inplace=True, errors = 'ignore')\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, df_program[['EFIS_Program','FY','Total Capital & Support Cost','Capital Cost', 'Support Cost' ]], \n",
    "                             how = 'left', left_on = 'EFIS', right_on='EFIS_Program')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-salmon",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_and_convert_FY(FY):\n",
    "    #get the last two number of FY string and convert to 4 digit FY value, \n",
    "    #for na value, fill 0\n",
    "    if FY is np.nan:\n",
    "        return 0\n",
    "#     elif int(FY[-2:]) == 0:\n",
    "#         return 0\n",
    "    else:\n",
    "        return int(FY[-2:])+2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['Target RTL FY Number'] = df_SHOPP_raw_data['Target RTL FY'].apply(calc_and_convert_FY)\n",
    "\n",
    "df_SHOPP_raw_data['Requested RTL FY Number'] = df_SHOPP_raw_data['Requested RTL FY'].apply(calc_and_convert_FY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['Target RTL FY Number'].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this logic needs to consider the programming list\n",
    "df_SHOPP_raw_data['Section'] = df_SHOPP_raw_data.apply(uf.cal_section_in_use, axis=1)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reconcile: \n",
    "# AMT_ID = 16813\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID][['AMT_ID','Section',\n",
    "# 'CCA Date Miilestone (M600)' ,'PCR SHOPP Amendment Date' ,\n",
    "# 'SHOPP Amendment Date' ,'EFIS_Program' ,'Long Lead' ,\n",
    "\n",
    "# 'Resourced In PID WP' ,'Requested RTL FY' ,'Dist Dir Appr','PID Uploaded' ,\n",
    "# 'LL RTL FY PRG' ,\n",
    "\n",
    "# ] ]#needs to be TYP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-diabetes",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['County'] = df_SHOPP_raw_data.apply(uf.county_to_use, axis=1)\n",
    "# df_SHOPP_raw_data['County'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['Route'] = df_SHOPP_raw_data.apply(uf.route_to_use, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-railway",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['Begin Postmile'] = df_SHOPP_raw_data.apply(uf.beginPM_to_use, axis=1)\n",
    "df_SHOPP_raw_data['End Postmile'] = df_SHOPP_raw_data.apply(uf.endPM_to_use, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['Unique EA'] = df_SHOPP_raw_data.apply(uf.calc_unique_EA, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['Activity (group)'] = df_SHOPP_raw_data['Activity'].apply(uf.calc_activity_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['Activity Book'] = df_SHOPP_raw_data['Activity'].apply(uf.calc_activity_book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['AM Tool RTL (Section in Use)'] = df_SHOPP_raw_data.apply(uf.calc_SIU_RTL, axis=1)\n",
    "\n",
    "df_SHOPP_raw_data['AM Tool RTL (Section in Use)'].fillna('00', inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-actress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['Advertised Year'] = df_SHOPP_raw_data.apply(\n",
    "    lambda x: x['AM Tool RTL (Section in Use)'] if pd.isnull(x['FY']) else x['FY'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data[['AM Tool RTL (Section in Use)','FY','Advertised Year','Last Year of Fiscal Year', 'PID Cycle']].head()\n",
    "# 'Last Year of FY POR', 'Plan Year','RTL Plan Year'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-syracuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last Year of Fiscal Year\n",
    "\n",
    "# cal ='''\n",
    "# FLOAT(Right([Advertised Year],2))\n",
    "# '''\n",
    "\n",
    "df_SHOPP_raw_data['Last Year of Fiscal Year'] = df_SHOPP_raw_data['Advertised Year'].str[-2:].astype(int)+2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-skating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['Last Year of Fiscal Year'] != df_SHOPP_raw_data['Last Year FY POR'] ][['AMT_ID','Last Year of Fiscal Year','Last Year FY POR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == 11296][]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-platform",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last Year FY POR\n",
    "\n",
    "# cal ='''\n",
    "# FLOAT(Right([AM Tool RTL (Section in Use)],2))\n",
    "# '''\n",
    "\n",
    "df_SHOPP_raw_data['Last Year FY POR'] = df_SHOPP_raw_data['AM Tool RTL (Section in Use)'].str[-2:].astype(int)+2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attached-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['Last Year FY POR'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-nightlife",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Planning or Post-Planning\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# IF(isnull([EFIS Programmed Projects]) and isnull([SHOPP Amendment Date])) Then \"Planning\"\n",
    "# ELSE \"Post-Planning\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "#Loren: is it possible logic does not compare well between different sources? Currently EFIS is converted as integer.\n",
    "\n",
    "ck_col = 'Planning or Post-Planning'\n",
    "def calc_planning_stage(df):\n",
    "    if pd.isnull(df['EFIS_Program']) and pd.isnull(df['SHOPP Amendment Date']):\n",
    "        return 'Planning' \n",
    "        \n",
    "    elif df['SHOPP Amendment Date'] == DD_Approval_Placeholder_Date:\n",
    "        return 'Planning'\n",
    "        \n",
    "    else:\n",
    "        return 'Post-Planning'\n",
    "    \n",
    "df_SHOPP_raw_data[ck_col]= df_SHOPP_raw_data.apply(calc_planning_stage, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #UnitTest\n",
    "\n",
    "# AMT_IDs= [20275,\n",
    "# 20277,\n",
    "# 20274,\n",
    "# 20620,\n",
    "# ]\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'].isin(AMT_IDs)][['AMT_ID','EFIS','SHOPP Amendment Date','Dist Dir Appr','Section','Planning or Post-Planning']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-extra",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SB-1 Priority\n",
    "\n",
    "# IF([Activity (group)]=\"SB-1\") Then \"Yes\"\n",
    "# Else \" \"\n",
    "# END\n",
    "\n",
    "\n",
    "df_SHOPP_raw_data['SB-1 Priority'] = df_SHOPP_raw_data['Activity'].apply(uf.calc_SB1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-batch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long Lead?\n",
    "\n",
    "# cal = '''\n",
    "# If([Last Year of Fiscal Year]>24 and [Long Lead]=\"Y\") THEN \"Yes\" Else \"No\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "def verify_longlead(df):\n",
    "    if df['Last Year of Fiscal Year'] > TARGET_FY + 5 and df['Long Lead'] == \"Y\": \n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'No'\n",
    "\n",
    "df_SHOPP_raw_data['Active Long Lead']= df_SHOPP_raw_data.apply(verify_longlead, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "posted-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df_SHOPP_raw_data['Target RTL FY'].str[-2:].astype(str)\n",
    "\n",
    "# df_SHOPP_raw_data['Requested RTL FY'].value_counts(dropna = False)\n",
    "\n",
    "df_SHOPP_raw_data['PA&ED FY Number'] = df_SHOPP_raw_data.apply(uf.calc_PAED_FY, axis = 1).astype(int)\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['Target RTL FY'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data=df_SHOPP_raw_data_copy\n",
    "\n",
    "# df_SHOPP_raw_data_copy = df_SHOPP_raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_include_5year_POR(df):\n",
    "    if df['Ten-Year Plan RD'] == 9999:\n",
    "        return  'No'\n",
    "    elif(df['Last Year FY POR']>TARGET_FY and df['Last Year FY POR']<TARGET_FY + 6) :   \n",
    "        if df['Activity (group)'] == 'Reservation' and pd.isnull(df['SHOPP Amendment Date']): \n",
    "            return  'Yes' \n",
    "        else: \n",
    "            return 'No' \n",
    "    elif (df['Last Year FY POR']>TARGET_FY + 5 and df['Last Year FY POR']<TARGET_FY + 11) : \n",
    "        if(df['Long Lead'] == \"Y\") and (df['Section'] == \"PRG\")  :\n",
    "             return  'Yes'\n",
    "        elif pd.isnull(df['SHOPP Amendment Date']): \n",
    "             return  'Yes' \n",
    "        else: \n",
    "            return 'No' \n",
    "    else: \n",
    "        return 'No'\n",
    "\n",
    "df_SHOPP_raw_data['Include 5-year POR?'] = df_SHOPP_raw_data.apply(calc_include_5year_POR, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-carrier",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "final-finance",
   "metadata": {},
   "source": [
    "<a id='DataJoining'></a>\n",
    "## Data Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add performance entry counts to raw data \n",
    "#Done: count unique location based on column 'Location'\n",
    "temp = df_perf_raw_data.groupby(['AMT_ID','Section'])['Location'].nunique().reset_index(name = 'perf_entry_count')\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['perf_entry_count'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp, how = 'left', \n",
    "                  left_on = ['AMT_ID','Section'], \n",
    "                  right_on = ['AMT_ID','Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Candidate Type to raw data \n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['Candidate Type', 'Advertised Year_Candidate','Project Cost ($K)_Candidate'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, df_shopp_candidate[['AMT_ID','Candidate Type', 'Advertised Year_Candidate','Project Cost ($K)_Candidate']], \n",
    "                             how = 'left', left_on = 'AMT_ID', right_on='AMT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add MPO/RTPA to raw data \n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['MPO/RTPA','County Name',],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, df_counties[['Co. Name Abbr.','County Name','MPO/RTPA']].drop_duplicates(), \n",
    "                             how = 'left', left_on = 'County', right_on='Co. Name Abbr.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find number of drainage worksheet entries for each project\n",
    "temp = df_drain_raw_data.groupby(['AMT_ID','Section'])['AMT_ID'].count().reset_index(name = 'No of Drainage Entries')\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['No of Drainage Entries'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp, how = 'left', \n",
    "                  left_on = ['AMT_ID','Section'], \n",
    "                  right_on = ['AMT_ID','Section'])\n",
    "\n",
    "df_SHOPP_raw_data['No of Drainage Entries'].fillna(0, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-cyprus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data['drainage_in_performance'] = df_perf_raw_data['Performance Objective'].str[:20] == 'Drainage Restoration'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_perf_raw_data.groupby(['AMT_ID','Section'])['drainage_in_performance'].agg(max).reset_index()\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['drainage_in_performance'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID','Section','drainage_in_performance']], how = 'left', \n",
    "                  left_on = ['AMT_ID','Section'], \n",
    "                  right_on = ['AMT_ID','Section'])\n",
    "\n",
    "df_SHOPP_raw_data['drainage_in_performance'].fillna(False, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data[(df_perf_raw_data['AMT_ID'] == 9000) & (df_perf_raw_data['Section'] == 'PPC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-programming",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add drainage cleaned dates\n",
    "temp = df_drain_raw_data[~df_drain_raw_data['Cleaned date'].isna()]\n",
    "# temp['Cleaned date'] = temp['Cleaned date'].apply(lambda x: x.strftime(\"%m-%d-%Y\"))\n",
    "\n",
    "temp_group = temp.groupby(['AMT_ID','Section',])['Cleaned date'].agg(set).reset_index(name = 'Cleaned Dates')\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['Cleaned Dates'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp_group[['AMT_ID','Section','Cleaned Dates']], \n",
    "                             how = 'left', \n",
    "                  left_on = ['AMT_ID','Section'], \n",
    "                  right_on = ['AMT_ID','Section'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-classroom",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-aviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if performance objective has pavement\n",
    "\n",
    "temp = df_perf_raw_data[df_perf_raw_data['Performance Objective'].str[:8] == \"Pavement\"].groupby(['AMT_ID','Section']).first().reset_index()\n",
    "\n",
    "temp['Performance Objective has Pavement'] = 'Yes'\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['Performance Objective has Pavement'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID','Section','Performance Objective has Pavement']], \n",
    "                             how= 'left', left_on = ['AMT_ID', 'Section'], right_on = ['AMT_ID','Section'])\n",
    "\n",
    "df_SHOPP_raw_data['Performance Objective has Pavement'].fillna('No', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data[df_perf_raw_data['Performance Objective'].str[:8] == \"Pavement\"].groupby(['AMT_ID','Section']).agg(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if performance objective has TMS\n",
    "\n",
    "temp =df_perf_raw_data[df_perf_raw_data['Performance Objective'].isin([\"Transportation Management Systems\", 'Transportation Management System Structures'])].groupby(['AMT_ID','Section']).first().reset_index()\n",
    "\n",
    "temp['Performance Objective has TMS'] = 'Yes'\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns =['Performance Objective has TMS'],inplace=True, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID','Section','Performance Objective has TMS']], \n",
    "                             how= 'left', left_on = ['AMT_ID', 'Section'], right_on = ['AMT_ID','Section'])\n",
    "\n",
    "df_SHOPP_raw_data['Performance Objective has TMS'].fillna('No', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add pavement plan year to df_SHOPP_raw_data\n",
    "\n",
    "temp =df_pav_raw_data.groupby(['AMT_ID','Section',])['Plan Year'].agg(Pavement_PlanYear='first', No_Pavement_PlanYear='nunique').reset_index()\n",
    "df_SHOPP_raw_data.drop(columns=['Pavement_PlanYear', 'No_Pavement_PlanYear'], inplace=True, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp, how= 'left', \n",
    "                             left_on = ['AMT_ID', 'Section'], right_on = ['AMT_ID','Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-nudist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp =df_pav_raw_data.groupby(['AMT_ID','Section',])['Plan Year'].agg(Pavement_PlanYears=set, No_Pavement_PlanYear='nunique').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-harvest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp[temp['No_Pavement_PlanYear']> 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-writer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #add pavement plan year to df_SHOPP_raw_data\n",
    "\n",
    "\n",
    "# temp =df_pav_raw_data.groupby(['AMT_ID','Section',])['Plan Year'].agg(Pavement_PlanYears=set, No_Pavement_PlanYear='nunique').reset_index()\n",
    "# df_SHOPP_raw_data.drop(columns=['Pavement_PlanYears', 'No_Pavement_PlanYear'], inplace=True, errors='ignore')\n",
    "\n",
    "# df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp, how= 'left', \n",
    "#                              left_on = ['AMT_ID', 'Section'], right_on = ['AMT_ID','Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pav_raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add pavement direction check to df_SHOPP_raw_data\n",
    "temp = df_pav_raw_data[df_pav_raw_data['Direction'].isna()]\n",
    "#Question do you mean to catch the project with pavement direction missing? data missing in the original dataset. \n",
    "# Left     1451\n",
    "# Right    1428\n",
    "# NaN        49\n",
    "\n",
    "temp['Direction'].fillna('Direction Info Missing', inplace= True) \n",
    "temp = temp.groupby(['AMT_ID', 'Section'])['Direction'].first().reset_index()\n",
    "temp.columns = ['AMT_ID', 'Section','Direction_check']\n",
    "df_SHOPP_raw_data.drop(columns=['Direction_check'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID', 'Section','Direction_check']], \n",
    "                             how= 'left', left_on = ['AMT_ID','Section'], right_on = ['AMT_ID', 'Section'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to get all scenarios of the pavement, how to deal with nan values\n",
    "# skip check for nan data of PCR Scenario\n",
    "df_pav_raw_data['PCRScenarioNo'] = df_pav_raw_data['PCRScenarioNo'].astype(str)\n",
    "temp = df_pav_raw_data[df_pav_raw_data['PCRScenarioNo']!= 'nan'].groupby(['AMT_ID', 'Section'])['PCRScenarioNo'].agg(first_PCRScenarioNo='first', count_PCRScenarioNo='nunique').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-locator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp['PCRScenarioNos'] = temp['PCRScenarioNo'].apply(','.join)\n",
    "# temp['PCRScenarioNo_len'] = temp['PCRScenarioNo'].apply(len)\n",
    "temp[temp['count_PCRScenarioNo']> 1]\n",
    "# df_SHOPP_raw_data.drop(columns=['PCRScenarioNo'],inplace=True , errors='ignore')\n",
    "\n",
    "# df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID', 'Section','PCRScenarioNo']], \n",
    "#                              how= 'left', left_on = ['AMT_ID','Section'], right_on = ['AMT_ID', 'Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = df_pav_raw_data.groupby(['AMT_ID', 'Section'])['PCRScenarioNo'].first().reset_index()\n",
    "\n",
    "# df_SHOPP_raw_data.drop(columns=['PCRScenarioNo'],inplace=True , errors='ignore')\n",
    "\n",
    "# df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID', 'Section','PCRScenarioNo']], \n",
    "#                              how= 'left', left_on = ['AMT_ID','Section'], right_on = ['AMT_ID', 'Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data.drop(columns=['first_PCRScenarioNo','count_PCRScenarioNo'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID', 'Section','first_PCRScenarioNo','count_PCRScenarioNo']], \n",
    "                             how= 'left', left_on = ['AMT_ID','Section'], right_on = ['AMT_ID', 'Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-samoa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-burton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tms_raw_data['Data Date_TMS'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data date for TMS is unique for each project and section. no need to check all \n",
    "temp =df_tms_raw_data.groupby(['AMT_ID','Section',])['Data Date_TMS'].first().reset_index()\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['Data Date_TMS'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID', 'Section','Data Date_TMS']], \n",
    "                             how= 'left', left_on = ['AMT_ID','Section'], right_on = ['AMT_ID', 'Section'])\n",
    "\n",
    "# 'Data Date_Drainage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data date for drainage is unique for each project and section. no need to check al\n",
    "\n",
    "temp =df_drain_raw_data.groupby(['AMT_ID','Section',])['Data Date_Drainage'].first().reset_index()\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['Data Date_Drainage'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID', 'Section','Data Date_Drainage']], \n",
    "                             how= 'left', left_on = ['AMT_ID','Section'], right_on = ['AMT_ID', 'Section'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-coordinate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-second",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data.drop(columns =['Cca Finish Date', 'Cca Percent Comp'],inplace=True, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, df_project_detail[['EFIS','Cca Finish Date', 'Cca Percent Comp']], \n",
    "                             how= 'left', left_on = ['EFIS'], right_on = ['EFIS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-skill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preliminary-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp = df_perf_raw_data[(df_perf_raw_data['ActID'] == 'H32') & (~df_perf_raw_data['Quantity'].isna())].groupby(['AMT_ID', 'Section']).first().reset_index()\n",
    "\n",
    "def ck_ActID_H32(df):\n",
    "    if (df['ActID'] == 'H32' and pd.notnull(df['Quantity'])):\n",
    "        return 'OK'\n",
    "    else:\n",
    "        return 'NG'\n",
    "\n",
    "temp['ck_ActID_H32'] = temp.apply(ck_ActID_H32, axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #debug: there are multiple activities for each projec tand section. Needs to create a list or a concat string before join with SHOPP raw data\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns =['ck_ActID_H32'],inplace=True, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID','Section','ck_ActID_H32']], \n",
    "                             how= 'left', left_on = ['AMT_ID', 'Section'], right_on = ['AMT_ID','Section'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-warrior",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add TMS plan year to df_SHOPP_raw_data\n",
    "\n",
    "temp =df_tms_raw_data.groupby(['AMT_ID','Section',])['RTL Plan Year'].agg(TMS_PlanYear='first', No_TMS_PlanYear='nunique').reset_index()\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns=['TMS_PlanYear', 'No_TMS_PlanYear'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp, how= 'left', \n",
    "                             left_on = ['AMT_ID', 'Section'], right_on = ['AMT_ID','Section'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add columns from df_PID_workload\n",
    "df_SHOPP_raw_data.drop(columns=['PID Status'],inplace=True , errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, df_PID_workload[['AMT_ID','PID Status']], \n",
    "                             how = 'left', left_on = 'AMT_ID', right_on='AMT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Valid PM to df_SHOPP_raw_data\n",
    "\n",
    "#question to be answered: please confirm: confirmed by manpaul and loren\n",
    "# the Section information is missing for any location other than \"Primary Location\",which cause original logic to skip this row\n",
    "# '''The logic is updated such that it will check the PM for entire project, regardless of the section. If any invalid PM exists in a project, it will be flagged.'''\n",
    "\n",
    "temp =df_pm_check.groupby(['AMT_ID'])['Valid PM'].agg(list).reset_index()\n",
    "temp['PM_Check'] = temp['Valid PM'].apply(lambda x: \"Has Invalid PM\" if 'No' in x else 'OK')\n",
    "\n",
    "df_SHOPP_raw_data.drop(['PM_Check'], axis=1, errors='ignore')\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID','PM_Check']], \n",
    "                                            how = 'left',\n",
    "                                            left_on = ['AMT_ID'],\n",
    "                                            right_on = ['AMT_ID'])\n",
    "df_SHOPP_raw_data['PM_Check'].fillna('OK', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-homework",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['PM_Check'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-arthur",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-perspective",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add obsolete project\n",
    "\n",
    "temp =df_project_obselete.groupby(['AMT_ID to obsolete']).first().reset_index()\n",
    "df_SHOPP_raw_data.drop(['AMT_ID to obsolete'], axis=1, errors='ignore')\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID to obsolete']], \n",
    "                how = 'left', left_on = 'AMT_ID', right_on = 'AMT_ID to obsolete')\n",
    "\n",
    "df_SHOPP_raw_data['AMT_ID to obsolete'] = df_SHOPP_raw_data['AMT_ID to obsolete'].apply(lambda x: 'Not Obselete' if pd.isnull(x) else 'Obselete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a column of program_summary_performance_value_sum\n",
    "\n",
    "temp = df_program_summary.groupby(['AMT_ID','Section',])['Performance Value'].sum().reset_index()\n",
    "dict_rename = {'Performance Value':'Performance Value Sum'}\n",
    "temp = temp.rename(dict_rename, axis = 1)\n",
    "\n",
    "\n",
    "#delete column 'Performance Value Sum' if exists\n",
    "# add 'Performance Value Sum' to raw data via merge\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns =['Performance Value Sum'],inplace=True, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID','Section','Performance Value Sum']], \n",
    "                             how= 'left', left_on = ['AMT_ID', 'Section'], right_on = ['AMT_ID', 'Section'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add exception type to each project \n",
    "\n",
    "#question to be answered, should we maintain max one exception for each project entry\n",
    "#This logic extend the type of exception strings\n",
    "temp = df_ck_exceptions.groupby(['AMT_ID'])['Type of Exception'].apply(','.join).reset_index()\n",
    "\n",
    "df_SHOPP_raw_data.drop(columns =['Type of Exception'],inplace=True, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp[['AMT_ID','Type of Exception']], \n",
    "                             how= 'left', left_on = ['AMT_ID'], right_on = ['AMT_ID'])\n",
    "\n",
    "df_SHOPP_raw_data['Type of Exception'].fillna('No Related Exception', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 'Nickname' into df_SHOPP_raw_data\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, df_project_detail[['EFIS','Nickname']], how ='left', left_on = 'EFIS', right_on = 'EFIS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-cooper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data['Shopp Tool Cost to use'] = df_SHOPP_raw_data.apply(uf.calc_SHOPP_tool_cost, axis = 1)\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['Shopp Tool Cost to use'].isna()]\n",
    "\n",
    "df_SHOPP_raw_data['Shopp Tool Cost to use'].fillna(0, inplace= True)\n",
    "\n",
    "#Question: how to handle this 7 null values \n",
    "#mara: fill null with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_SHOPP_raw_data['Total Project Cost ($K)'] = df_SHOPP_raw_data.apply(uf.calc_total_project_cost, axis = 1)\n",
    "\n",
    "# df_SHOPP_raw_data['Total Project Cost ($K)'].isna().any()\n",
    "df_SHOPP_raw_data['Total Project Cost ($K)'].fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-guide",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Project Cost ($K)\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# {Fixed [SHOPP ID], [EFIS ],[Date], [Advertised Year], [District]:Max([Total Project Cost ($K)])}\n",
    "# '''\n",
    "\n",
    "#Question: I do not understand this logic, why group SHOPP ID,  EFIS, Advertised Year and District, why get the max\n",
    "#Mara: just get the Total Project Cost ($K) for each SHOPP ID\n",
    "\n",
    "df_SHOPP_raw_data['Project Cost ($K)'] = df_SHOPP_raw_data['Total Project Cost ($K)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconcile: the total project cost can not be nan \n",
    "\n",
    "\n",
    "AMT_ID= 22171\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID][['AMT_ID','Section','EFIS','EFIS_Program','Shopp Tool Cost to use',\n",
    "                                                          'Long Lead','Total Project Cost ($K)','TYP Total Project Cost ($K)','LL PAED Cost ($K)','Project Cost ($K)']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12+ np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program[df_program['EFIS_Program'] == 319000045]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-fitness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-chambers",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "balanced-hunger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "individual-water",
   "metadata": {},
   "source": [
    "<a id='Issue_Table1'></a>\n",
    "# Issue Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-crash",
   "metadata": {},
   "source": [
    "<a id='Will_this_project_be_included_in_the_Project_Book'></a> \n",
    "### Will this project be included in the Project Book?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will this project be included in the Project Book?\n",
    "\n",
    "# cal ='''\n",
    "# IF(Isnull([EFIS Programmed Projects]))Then \n",
    "#     If [Ten-Year Plan RD]=9999 Then 'No'\n",
    "#     ELSEIF (Isnull([2020 Candidates])) Then\n",
    "#         If ([Last Year FY POR]>19 and [Last Year FY POR]<25) Then \n",
    "#             If(([Activity (group)]='Reservation') and (Isnull([SHOPP Amendment Date]))) Then 'Yes' \n",
    "#             Else 'No' \n",
    "#             End\n",
    "#         ElseIF ([Last Year FY POR]>24 and [Last Year FY POR]<30) Then\n",
    "#             ( IF([Long Lead]=\"Y\") Then   'Yes'\n",
    "#               ELSEIf((Isnull([SHOPP Amendment Date]) )) Then 'Yes' \n",
    "#               Else 'No' \n",
    "#               End)\n",
    "#         Else 'No'\n",
    "#         End\n",
    "#     Else \"Yes\"\n",
    "#     END\n",
    "# Elseif ([Last Year of Fiscal Year]=0) Then'Yes'\n",
    "# Elseif ([Last Year of Fiscal Year]<20) Then\"No\"\n",
    "# Else 'Yes'\n",
    "# END\n",
    "# '''\n",
    "\n",
    "#Question: Should use hard coded FY limit or use FY offset from current FY\n",
    "#Mara: to confirm\n",
    "def ck_include_in_projectbook(df):\n",
    "    \n",
    "    if pd.isnull(df['EFIS_Program']): #it is not programmed project\n",
    "        if df['Ten-Year Plan RD']==9999:\n",
    "            return 'No'\n",
    "        elif not pd.isnull(df['Candidate Type']): \n",
    "            return 'Yes'\n",
    "        else:\n",
    "            if (df['Last Year FY POR']>(TARGET_FY) and df['Last Year FY POR']<(TARGET_FY + 6)): \n",
    "                if ((df['Activity (group)']=='Reservation') and (pd.isnull(df['SHOPP Amendment Date']))):\n",
    "                    return 'Yes'\n",
    "                else:\n",
    "                    return 'No'\n",
    "            elif  (df['Last Year FY POR']>(TARGET_FY + 5) and df['Last Year FY POR']<(TARGET_FY + 11)):\n",
    "                if df['Long Lead'] == 'Y':\n",
    "                    return 'Yes'\n",
    "                elif pd.isnull(df['SHOPP Amendment Date']):\n",
    "                    return 'Yes'\n",
    "                else:\n",
    "                    return 'No'\n",
    "            else:\n",
    "                return 'No'\n",
    "    elif df['Last Year of Fiscal Year'] == 0: #information missing, 0 used to fill space\n",
    "        return 'Yes'\n",
    "    elif df['Last Year of Fiscal Year'] < TARGET_FY + 1:\n",
    "        return 'No'\n",
    "    else:\n",
    "        return 'Yes'\n",
    "\n",
    "\n",
    "df_SHOPP_raw_data['Will this project be included in the Project Book?'] = df_SHOPP_raw_data.apply(ck_include_in_projectbook, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "AMT_ID= 23064\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID][['AMT_ID','EFIS','EFIS_Program', 'Ten-Year Plan RD',\n",
    "                                                           'Section','PCR SHOPP Amendment Date','Requested RTL FY','PCR RTL',\n",
    "                                                           'AM Tool RTL (Section in Use)',\n",
    "                                                          'Last Year FY POR',\n",
    "                                                          'Last Year of Fiscal Year',\n",
    "                                                          'Activity (group)',\n",
    "                                                          'SHOPP Amendment Date',\n",
    "                                                          'Long Lead','Will this project be included in the Project Book?']]\n",
    "\n",
    "# df_program[df_program['EFIS'] ==  317000065.0]\n",
    "\n",
    "# ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['Will this project be included in the Project Book?'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-welding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "educational-spelling",
   "metadata": {},
   "source": [
    "<a id='Does_project_cost_exceed_Minor_Program_limits'></a>\n",
    "### Does project cost exceed Minor Program limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does project cost exceed Minor Program limits ($1,250K)?\n",
    "\n",
    "# cal ='''\n",
    "# IF([Planning or Post-Planning]=\"Post-Planning\") Then \"OK\"\n",
    "# ELSEIF([Activity]= \"Relinquishment\") Then \"OK\"\n",
    "# ELSEIF([Activity (group)]=\"Reservation\") Then \n",
    "#     (IF [Project Cost ($K)]>333 Then \"OK\" \n",
    "#     Else 'Please review project cost, it is below Minor Program limits' \n",
    "#     End)\n",
    "# ELSEIF[Project Cost ($K)]>1250 Then \"OK\" \n",
    "# Else 'Please review project cost, it is below Minor Program limits' \n",
    "# END\n",
    "# '''\n",
    "\n",
    "def ck_minor_program_limit(df):\n",
    "    \n",
    "    if df['Ten-Year Plan RD']==9999:\n",
    "        return 'OK'\n",
    "    else:\n",
    "        if(df['Planning or Post-Planning']==\"Post-Planning\"):\n",
    "            return \"OK\"\n",
    "        elif (df['Activity']== \"Relinquishment\"):\n",
    "            return \"OK\"\n",
    "        elif(df['Activity (group)']==\"Reservation\"):\n",
    "            if df['Project Cost ($K)']>333:\n",
    "                return \"OK\" \n",
    "            else:\n",
    "                return 'The reservation project cost (${:.0f}k) is less than Minor Program minimum limits (${}k). AMT_ID: {}'.format(df['Project Cost ($K)'],df['Section'], 333, df['AMT_ID']) \n",
    "        elif df['Project Cost ($K)']>1250:\n",
    "            return \"OK\" \n",
    "        else:\n",
    "            return 'The project cost (${:.0f}k) in the {} section is less than Minor Program minimum limits (${}k). AMT_ID: {}'.format(df['Project Cost ($K)'],df['Section'], 1250, df['AMT_ID']) \n",
    "#             return 'For {} section in project {}: Please review project cost. The cost (${:.0f}k) is less than Minor Program minimum limits  (${}k).'.format(df['Section'],df['AMT_ID'],df['Project Cost ($K)'], 1250) \n",
    "\n",
    "df_SHOPP_raw_data['Does project cost exceed Minor Program limits ($1,250K)?'] = df_SHOPP_raw_data.apply(ck_minor_program_limit, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['Does project cost exceed Minor Program limits ($1,250K)?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-peace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "# AMT_ID = 19612\n",
    "\n",
    "\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID][['AMT_ID','Planning or Post-Planning','Activity','Activity (group)',\n",
    "#                                                             'Project Cost ($K)','Total Project Cost ($K)',\n",
    "#                                                            'Does project cost exceed Minor Program limits ($1,250K)?']]\n",
    "\n",
    "# ############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-elevation",
   "metadata": {},
   "source": [
    "<a id='Is_Major_Damage_or_Mobility_Subcategory_Identified'></a>\n",
    "\n",
    "### Is Major Damage or Mobility Subcategory Identified (Obsoleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is Major Damage or Mobility Subcategory Identified?\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# If([Activity]=\"Mobility\" or [Activity]=\"Major Damage\") Then \"Please identify Major Damage or Mobility Sub-Category\"\n",
    "# Else \"OK\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "\n",
    "def ck_mobility_major_damange(df):\n",
    "    if df['Activity'] == 'Mobility' or df['Activity'] == 'Major Damage':\n",
    "        return 'For {} section in project {}: Please identify Major Damage or Mobility Sub-Category. The current activity is {}.'.format(df['Section'],df['AMT_ID'],df['Activity'])\n",
    "    else:\n",
    "        return 'OK'\n",
    "\n",
    "df_SHOPP_raw_data['Is Major Damage or Mobility Subcategory Identified?'] = df_SHOPP_raw_data.apply(ck_mobility_major_damange, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['Is Major Damage or Mobility Subcategory Identified?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "AMT_ID = 22053  \n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID][['AMT_ID' ,'Activity',]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-bulgaria",
   "metadata": {},
   "source": [
    "<a id='Is_Planned_Project_RTL_in_a_FY_that_can_be_programmed_in_future_SHOPP_cycles'></a>\n",
    "\n",
    "### Is Planned Project RTL in a FY that can be programmed in future SHOPP cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is Planned Project RTL in a FY that can be programmed in future SHOPP cycles?\n",
    "\n",
    "# cal ='''\n",
    "#     IF ([Activity (group)]=\"Reservation\") then \n",
    "#         (IF ([Planning or Post-Planning]=\"Planning\" \n",
    "#             and ISNULL([SHOPP Amendment Date])\n",
    "#             and [Last Year of Fiscal Year]<21) Then \n",
    "#             \"Please review. RTL inconsistent with project status. Project should be programmed or RTL should be 2019/20 or later\" \n",
    "#         Else \"OK\" \n",
    "#         END)\n",
    "#     ELSEIF ([Planning or Post-Planning]=\"Post-Planning\") Then \"OK\"\n",
    "#     Elseif ([Last Year of Fiscal Year]>24)Then \"OK\" \n",
    "#     Elseif(ISnull([2020 Candidates])) Then \"Please review Projected RTL, only planned reservation projects may have a RTL prior to 2024/25\"\n",
    "#     Else \"OK\"\n",
    "#     END\n",
    "# '''\n",
    "\n",
    "\n",
    "def ck_project_programmability(df):\n",
    "    # question to be answered: manpaul: if there is a DDA date, then the project is programmed (post-planning)\n",
    "    # manpaul: if the request RTL FY < 25 and missing  'SHOPP Amendment Date', flag the project\n",
    "    if df['Ten-Year Plan RD']==9999:\n",
    "          return 'OK'\n",
    "    else:\n",
    "        if (df['Activity (group)'] == \"Reservation\"): \n",
    "            if (df['Planning or Post-Planning'] == \"Planning\" \n",
    "                and pd.isnull(df['SHOPP Amendment Date'])\n",
    "                and df['Last Year of Fiscal Year']< CURRENT_FY) : \n",
    "                return \"For {} section in project {}: Please review. RTL inconsistent with project status. For this {} Project, it is not programmed and RTL ({}) is not the current FY ({}) or later\".format(df['Section'],df['AMT_ID'],df['Last Year of Fiscal Year'], df['Activity (group)'], CURRENT_FY) \n",
    "            else: \n",
    "                return \"OK\" \n",
    "        elif  (df['Planning or Post-Planning'] == \"Post-Planning\") : \n",
    "            return \"OK\"\n",
    "        elif  (df['Last Year of Fiscal Year']> TARGET_FY + 5): \n",
    "            return \"OK\" \n",
    "        elif pd.isnull(df['Candidate Type']) or pd.isnull(df['SHOPP Amendment Date']):\n",
    "            return \"For {} section in project {}: Please review Projected RTL. This project is a {} project, which is not a reservation project. It is not programmed and the last year of fiscal year is {}, which not later than {}.\".format(df['Section'],df['AMT_ID'],df['Activity (group)'],df['Last Year of Fiscal Year'], df['Last Year of Fiscal Year']+ 5 )  \n",
    "        else: \n",
    "            return \"OK\"\n",
    "\n",
    "df_SHOPP_raw_data['Is Planned Project RTL in a FY that can be programmed in future SHOPP cycles?'] = df_SHOPP_raw_data.apply(ck_project_programmability, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['Is Planned Project RTL in a FY that can be programmed in future SHOPP cycles?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-committee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-forward",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "# AMT_ID = 22053  #this should be flagged, but not flagged since it is in 'post planning'\n",
    "# # AMT_ID = 14153 #a complicated project with combine action, might require different logic\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID][['AMT_ID' ,'EFIS',\n",
    "# 'Candidate Type' ,\n",
    "# 'Activity (group)' ,\n",
    "# 'Planning or Post-Planning' ,\n",
    "# 'SHOPP Amendment Date' ,\n",
    "# 'Last Year of Fiscal Year' ,\n",
    "# 'Is Planned Project RTL in a FY that can be programmed in future SHOPP cycles?' ,]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-impossible",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-timeline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-minneapolis",
   "metadata": {},
   "source": [
    "<a id='Is_the_PID_cycle_consistent_with_the_project_status_and_RTL'></a>\n",
    "\n",
    "### Is the PID cycle consistent with the project status and RTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is the PID cycle consistent with the project status and RTL?\n",
    "\n",
    "# cal ='''\n",
    "# IF ([Planning or Post-Planning]=\"Post-Planning\") Then \"OK\"\n",
    "#     ElseIf([Activity (group)]=\"Reservation\") Then \"OK\"\n",
    "#     Elseif [Long Lead]=\"Y\" Then \"OK\"\n",
    "#     ELSEIF  (([Last Year of Fiscal Year]=([PID Cycle]-2000+3) or[Last Year of Fiscal Year]=([PID Cycle]-2000+4))) Then \"OK\"\n",
    "#          Else \"Please review PID cycle according to RTL year\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "ck_name = 'Is the PID cycle consistent with the project status and RTL?'\n",
    "def ck_PID_cycle(df):\n",
    "    if df['Ten-Year Plan RD']==9999:\n",
    "        return 'OK'\n",
    "    else:\n",
    "        if (df['Planning or Post-Planning'] == \"Post-Planning\") : \n",
    "            return \"OK\"\n",
    "        elif (df['Activity (group)'] == \"Reservation\") : \n",
    "            return \"OK\"\n",
    "        elif  df['Long Lead'] == \"Y\" : \n",
    "            return \"OK\"\n",
    "\n",
    "        elif pd.isnull(df['PID Cycle']): \n",
    "            return 'The PID Cycle year is missing or needs to be updated to correspond to the project RTL year. AMT_ID: {}'.format(df['AMT_ID'])\n",
    "\n",
    "        elif  ((df['Last Year of Fiscal Year'] == (df['PID Cycle']+3) \n",
    "                or df['Last Year of Fiscal Year'] == (df['PID Cycle']+4))) : \n",
    "            return \"OK\"\n",
    "        else: \n",
    "            return 'Please review PID cycle according to RTL year. This project is not programmed. It is not a Reservation project nor a long lead. The Last Year of Fiscal Year is {}, which is not consistant with the PID cycle year ({:.0f}). AMT_ID: {}'.format(df['Last Year of Fiscal Year'], df['PID Cycle'],df['AMT_ID'])\n",
    "\n",
    "        \n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_PID_cycle, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-easter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-envelope",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "# AMT_ID = 22053  #this project should be flagged, but it is designated as 'post planning'\n",
    "# # 13330\n",
    "# # 17498\n",
    "# # 18156\n",
    "\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID][['AMT_ID','EFIS','SHOPP Amendment Date','Requested SHOPP Cycle','PID Cycle',\n",
    "# 'Activity (group)' ,\n",
    "# 'Planning or Post-Planning' ,\n",
    "# 'Long Lead' ,\n",
    "# 'Last Year of Fiscal Year' ,'Is the PID cycle consistent with the project status and RTL?' ,]]\n",
    "\n",
    "# ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-restoration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-grant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "inner-church",
   "metadata": {},
   "source": [
    "<a id='Is_PIP_uploaded_(Active_and_Complete_PIDs)'></a>\n",
    "\n",
    "### Is PIP uploaded (Active and Complete PIDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suitable-conducting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is PIP uploaded (Active and Complete PIDs)?\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# If(Isnull([2020 Candidates])) Then \"OK\"\n",
    "# ELSeIf([PID Status]=\"Active\" or [PID Status]=\"Completed\" and isnull([PIP Uploaded])) Then \n",
    "#     If isnull([PID Uploaded]) then \"PIP needs to be uploaded\" \n",
    "#     Else \"OK\" \n",
    "#     END\n",
    "# Else \"OK\"\n",
    "# End\n",
    "# '''\n",
    "\n",
    "ck_name = 'Is Project Initiation Proposal (PIP) uploaded? Applies to projects with Active and  Complete PIDs.'\n",
    "\n",
    "def ck_PIP_uploaded(df):\n",
    "    \n",
    "    if pd.notnull(df['Candidate Type']): #it is on the candidate list, skip check\n",
    "        return \"OK\"\n",
    "    elif df['PID Status'] == \"Active\" or df['PID Status'] == \"Completed\": \n",
    "        if pd.isnull(df['PID Uploaded']) and pd.isnull(df['PIP Uploaded']) : \n",
    "            return 'Please upload a PIP or PID document. AMT_ID: {}'.format(df['AMT_ID'])\n",
    "        else: \n",
    "            return \"OK\" \n",
    "    else: \n",
    "        return \"OK\"\n",
    "\n",
    "\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_PIP_uploaded, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data[ck_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Unit Test#####\n",
    "\n",
    "AMT_ID = 18990\n",
    "\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID][['AMT_ID','EFIS','SHOPP Amendment Date',\n",
    "                                                          'Requested SHOPP Cycle','PID Cycle',\n",
    "'PID Uploaded','PIP Uploaded','PID Status','Candidate Type',\n",
    "'Planning or Post-Planning' ,\n",
    "'Long Lead' ,\n",
    "'Last Year of Fiscal Year' ,ck_name,]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-syndication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['PID Uploaded'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-yukon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['PIP Uploaded'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-southwest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "economic-regression",
   "metadata": {},
   "source": [
    "<a id='Is_District_Director_Approval_Date_in_the_Future'></a>\n",
    "\n",
    "### Is District Director Approval Date in the Future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-reset",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is District Director Approval Date in the Future?\n",
    "\n",
    "# cal ='''\n",
    "# If(DATE([Dist Dir Appr])>Date([Date])) Then \"Date needs to be removed or corrected to actual PID signature date\"\n",
    "# Else \"OK\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "\n",
    "# question to be answered: there are 2053 projects without DDA date, how to handle missing information\n",
    "\n",
    "def ck_director_approal_date(df):\n",
    "    \n",
    "    if pd.isnull(df['Dist Dir Appr']):\n",
    "        return 'OK'\n",
    "    elif(datetime.strptime(df['Dist Dir Appr'], '%m/%d/%y') > datetime.strptime(TARGETDATE, \"%m-%d-%Y\")): \n",
    "        return 'This project has a future District Director Approval Date ({}) that needs to be removed or corrected. AMT_ID: {}'.format(df['Dist Dir Appr'],df['AMT_ID'])\n",
    "    else: \n",
    "        return \"OK\"\n",
    "\n",
    "df_SHOPP_raw_data['Is District Director Approval Date in the Future?'] = df_SHOPP_raw_data.apply(ck_director_approal_date, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "completed-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['Is District Director Approval Date in the Future?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-vanilla",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "strange-grain",
   "metadata": {},
   "source": [
    "<a id='Is_the_EA_or_Project_ID_repeated_in_the_AM_tool'></a>\n",
    "\n",
    "### Is the EA or Project ID repeated in the AM tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is the EA or Project ID repeated in the AM tool?\n",
    "\n",
    "\n",
    "#mara: if EA or EFIS is null, does not check it for duplication\n",
    "#mara: for this check, we need to for all SHOPP, Minor and HM, ?where is the dataset\n",
    "\n",
    "# cal ='''\n",
    "# If [Count Unique EFIS]>1 or [Count Unique EA]>1 then \"More than one record in the AM tool has the same EA or same Project ID. Please review.\"\n",
    "# Else \"OK\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "\n",
    "\n",
    "#how to deal with the HM and Minor bad data quality (EFIS is not numeric)\n",
    "\n",
    "\n",
    "\n",
    "# df_SHOPP_raw_data['Is the EA or Project ID repeated in the AM tool?'] = (((df_SHOPP_raw_data['Unique EA']!='') & (df_SHOPP_raw_data['Unique EA'].duplicated())) \n",
    "#                                                                          | ((~df_SHOPP_raw_data['EFIS'].isna()) & (df_SHOPP_raw_data['EFIS'].duplicated())))\n",
    "\n",
    "# df_SHOPP_raw_data['Is the EA or Project ID repeated in the AM tool?'] = df_SHOPP_raw_data['Is the EA or Project ID repeated in the AM tool?'].apply(lambda x: \"More than one record in the AM tool has the same EA or same Project ID. Please review.\" if x else \"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-expert",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-marina",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-account",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# skip the check if the EA or EFIS ID is missing\n",
    "\n",
    "df_Minor_raw_data_filtered = df_Minor_raw_data[(df_Minor_raw_data['Status'].isna()) \n",
    "#                                             &(df_Minor_raw_data['EA'] != 'TBD') \n",
    "                                               &(df_Minor_raw_data['EA'] != '')\n",
    "#                                             & (df_Minor_raw_data['EFIS'] != 0)                                              \n",
    "                                            ][['Unique EA','EA','EFIS','AMT_ID','District',]]\n",
    "\n",
    "df_HM_raw_data_filtered = df_HM_raw_data[(df_HM_raw_data['Status'].isna())\n",
    "#                                       &(df_HM_raw_data['EA'] != 'TBD')  \n",
    "                                         &(df_HM_raw_data['EA'] != '') \n",
    "#                                             & (df_HM_raw_data['EFIS'] !='0000000000') \n",
    "#                                             & (df_HM_raw_data['EFIS'] !='TBD0000000')    \n",
    "                                      ][['Unique EA','EFIS','AMT_ID','District',]]\n",
    "\n",
    "df_SHOPP_raw_data_filtered = df_SHOPP_raw_data[df_SHOPP_raw_data['Unique EA'] != ''][['Unique EA','EFIS','AMT_ID','District',]]\n",
    "df_SHOPP_HM_MB = df_SHOPP_raw_data_filtered[['Unique EA','EFIS','AMT_ID','District',]].append(df_Minor_raw_data_filtered, ignore_index=True) \n",
    "df_SHOPP_HM_MB = df_SHOPP_HM_MB.append(df_HM_raw_data_filtered, ignore_index=True) \n",
    "\n",
    "# temp['EFIS'] = temp['EFIS'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-carter",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "duplicate_EA = df_SHOPP_HM_MB.groupby(['Unique EA'])['AMT_ID'].agg(list).reset_index(name = 'AMT_ID List_repeated Unique EA')\n",
    "duplicate_EA['Count_Repeated Unique EA'] = duplicate_EA['AMT_ID List_repeated Unique EA'].apply(lambda x: len(x))\n",
    "duplicate_EA = duplicate_EA[(duplicate_EA['Unique EA'].str.len() > 6) & (duplicate_EA['Count_Repeated Unique EA'] > 1)]\n",
    "\n",
    "duplicate_EA['AMT_ID List_repeated Unique EA'] = duplicate_EA['AMT_ID List_repeated Unique EA'].apply(lambda x: ';'.join(map(str, x)))\n",
    "# duplicate_EA.columns = ['Unique EA','AMT_ID List_repeated UniqueEA'] \n",
    "# duplicate_EA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-ability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_EFIS = df_SHOPP_HM_MB.groupby('EFIS')['AMT_ID'].agg(list).reset_index(name = 'AMT_ID List_repeated EFIS')\n",
    "duplicate_EFIS['Count_Repeated EFIS'] = duplicate_EFIS['AMT_ID List_repeated EFIS'].apply(lambda x: len(x))\n",
    "duplicate_EFIS = duplicate_EFIS[(duplicate_EFIS['EFIS'] != '') & (duplicate_EFIS['Count_Repeated EFIS'] > 1)]\n",
    "# duplicate_EFIS.columns = ['EFIS','AMT_ID List_repeated EFIS'] \n",
    "duplicate_EFIS['AMT_ID List_repeated EFIS']=duplicate_EFIS['AMT_ID List_repeated EFIS'].apply(lambda x: ';'.join(map(str, x)))\n",
    "# duplicate_EFIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data.drop(['Count_Repeated Unique EA','AMT_ID List_repeated Unique EA','Count_Repeated EFIS','AMT_ID List_repeated EFIS'], errors = 'ignore')\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, duplicate_EA, how ='left', left_on = 'Unique EA', right_on ='Unique EA' )\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, duplicate_EFIS, how ='left', left_on = 'EFIS', right_on ='EFIS' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ck_duplicate_EA_EFIS(df):\n",
    "    if pd.isnull(df['Count_Repeated Unique EA']) and pd.isnull(df['Count_Repeated EFIS']):\n",
    "        return 'OK'\n",
    "    elif (\"Repeated EA\" in df['Type of Exception']) or (\"Repeated EFIS\" in df['Type of Exception']): \n",
    "        return 'OK'\n",
    "    else:\n",
    "        comments = '''For {} section in project {}: More than one record in the AM tool has the same EA and/or same Project ID.'''.format(df['Section'],df['AMT_ID'],)\n",
    "        if df['Count_Repeated Unique EA'] > 1: \n",
    "            comments = comments + ' The IDs with repeated EA ({}) are: {}.'.format(df['Unique EA'], duplicate_EA[duplicate_EA['Unique EA'] == df['Unique EA']]['AMT_ID List_repeated Unique EA'].iloc[0])\n",
    "        \n",
    "        if df['Count_Repeated EFIS']> 1: \n",
    "            comments = comments + ' The IDs with repeated EFIS ({}) are: {}.'.format(df['EFIS'], duplicate_EFIS[duplicate_EFIS['EFIS'] ==df['EFIS']]['AMT_ID List_repeated EFIS'].iloc[0])\n",
    "\n",
    "        return comments\n",
    "#         return 'More than one record in the AM tool has the same EA or same Project ID. Please review.'\n",
    "\n",
    "ck_name = 'Is the EA or Project ID repeated in the AM tool?'\n",
    "\n",
    "df_SHOPP_raw_data[ck_name] =df_SHOPP_raw_data.apply(ck_duplicate_EA_EFIS, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data[ck_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-exception",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Unit Test#####\n",
    "\n",
    "#reconcile: for 22934, the duplicated EA or EFIS should not be flagged since it is in expcetion.\n",
    "\n",
    "AMT_ID = 22934\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID][['AMT_ID','EA','EFIS','Unique EA','Count_Repeated Unique EA','Count_Repeated EFIS','Is the EA or Project ID repeated in the AM tool?']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-swing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-funds",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fitted-swing",
   "metadata": {},
   "source": [
    "<a id='Does_project_include_performance_related_to_each_location'></a>\n",
    "\n",
    "### Does project include performance related to each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compact-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does project include performance related to each location?\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# If([Will this project be included in the Project Book?]=\"Yes\" and [Multiple Loc]=\"Y\") Then\n",
    "#     IF([Count Location Performance Raw Data]=[Loc Count]) Then \"OK\"\n",
    "#     Else \"Detail the performance for each location listed in the performance tab.\"\n",
    "#     End\n",
    "# Else \"OK\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "def ck_multiple_location_performance(df):\n",
    "    if(df['Will this project be included in the Project Book?']==\"Yes\" and df['Multiple Loc'] == \"Yes\"):\n",
    "        if df['perf_entry_count'] == df['Loc Count'] : \n",
    "            return \"OK\"\n",
    "        elif \"Cost or RTL not matching CTIPS\" in df['Type of Exception']: \n",
    "            return  \"OK\"\n",
    "        else: \n",
    "            return 'Please update the Performance Tab corresponding to each project location limit to report at least one activity for each of the {} location(s). AMT_ID: {}'.format(df['perf_entry_count'],df['AMT_ID'])\n",
    "        \n",
    "    else: \n",
    "        return \"OK\"\n",
    "ck_name = 'Does project include performance for each location?'\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_multiple_location_performance, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###Unit Test#####\n",
    "\n",
    "# AMT_ID = 19158\n",
    "# # STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID][['AMT_ID','EA','EFIS','Section','Multiple Loc','perf_entry_count','Loc Count',\n",
    "#                                                           ck_name,\n",
    "#                                                           ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data[(df_perf_raw_data['AMT_ID'] == AMT_ID) & (df_perf_raw_data['Section'] ==STU)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-train",
   "metadata": {},
   "source": [
    "<a id='Is_Performance_tab_Complete'></a>\n",
    "\n",
    "### Is Performance tab Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-kitty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is Performance tab Complete?\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# If [Ten-Year Plan RD]=9999 then \"OK\"\n",
    "# ElseIf(isnull([Section])) then\n",
    "# \"Please Complete the Performance Tab\"\n",
    "# Elseif({Fixed [SHOPP ID], [Date]: Max([Include in Performance? Numeric])})=1 Then\"OK\"\n",
    "# Else \"Please Complete the Performance Tab\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "def ck_performance_tab_completed(df):\n",
    "    if df['Ten-Year Plan RD'] == 9999 : \n",
    "        return \"OK\"\n",
    "    elif df['perf_entry_count']>0:\n",
    "        return \"OK\"\n",
    "    else: \n",
    "        return 'Please complete the Performance Tab and include at least one activity for each project location. AMT_ID: {}'.format(df['AMT_ID'])\n",
    "\n",
    "df_SHOPP_raw_data['Is Performance tab Complete?'] = df_SHOPP_raw_data.apply(ck_performance_tab_completed, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['Is Performance tab Complete?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Unit Test#####\n",
    "\n",
    "AMT_ID = 21255\n",
    "#manpaul: agree with this logic\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID][['AMT_ID','Section','Ten-Year Plan RD',\n",
    "                                                            'perf_entry_count',\n",
    "                                                           'Is Performance tab Complete?']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-acquisition",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_perf_raw_data[(df_perf_raw_data['AMT_ID'] == AMT_ID) & (df_perf_raw_data['Section'] == 'PPC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-specific",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "earlier-concord",
   "metadata": {},
   "source": [
    "<a id='Is_at_least_one_performance_activities_related_to_the_Activity_Category_of_planned_project'></a>\n",
    "\n",
    "### Is at least one performance activities related to the Activity Category of planned project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is at least one performance activities related to the Activity Category of planned project?\n",
    "\n",
    "# cal ='''\n",
    "# If [Planning or Post-Planning]=\"Post-Planning\" then \"OK\" \n",
    "# Elseif [Section to Use]=[Section Programming Summary] then \n",
    "#      IF { FIXED [SHOPP ID], [Date], [Section Programming Summary]:SUM([Programming Summary Performance Value (No Nulls)])}=0\n",
    "#                     Then \"Please review activities in the performance tab, or the Activity Category of project profile. The performance measure to be reported to CTC is 0.\"\n",
    "#                 Else \"OK\"\n",
    "#                 END\n",
    "# Else \"OK\"\n",
    "#                 END\n",
    "# '''\n",
    "\n",
    "\n",
    "\n",
    "def ck_active_category_performance(df):\n",
    "    \n",
    "    '''\n",
    "    if project is in post-planning, SKIP the check\n",
    "    if in program summary, same project, same section, no performance value entries is found, FLAG\n",
    "    else ok\n",
    "    '''\n",
    "    \n",
    "    if df['Ten-Year Plan RD'] == 9999 : \n",
    "        return \"OK\"\n",
    "    if (\"Complete Street\" in df['Type of Exception']) or (\"Proactive Safety\" in df['Type of Exception']): \n",
    "        #if the check exception has Complete Street, or Proactive Safety, skip the check.\n",
    "        return  \"OK\" \n",
    "    elif df['Planning or Post-Planning'] == \"Post-Planning\" : \n",
    "        #question: manpaul: we should check the project performance, regardless planning or post-planning.\n",
    "        #Loren: we will do this just to fit for purpose to accomodate legacy data\n",
    "        return  \"OK\" \n",
    "\n",
    "    elif pd.isnull(df['Performance Value Sum']) or df['Performance Value Sum'] == 0:\n",
    "        return 'Please update activities or quantities in the Performance Tab to match the main Activity Category of project in TYP section.  Performance reported in the Programming Summary is missing or equal to 0. AMT_ID: {}'.format(df['AMT_ID'])\n",
    "    else: \n",
    "        return \"OK\"\n",
    "\n",
    "ck_name = 'Does the performance tab include one or more activities related to the Activity Category?' \n",
    "    \n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_active_category_performance, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-africa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data[ck_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-explosion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "# 21182\n",
    "# 22715\n",
    "# 22817\n",
    "# 22972\n",
    "\n",
    "AMT_ID = 23004\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID][['AMT_ID','Section','Planning or Post-Planning','Performance Value Sum',\n",
    "                                                           ck_name]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-press",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program_summary[(df_program_summary['AMT_ID']==AMT_ID)\n",
    "#                        & (df_program_summary['Section']== 'TYP')][['AMT_ID','EA','Section','Performance Value']]\n",
    "                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-medicaid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program_summary[(df_program_summary['AMT_ID']==AMT_ID)\n",
    "#                        & (df_program_summary['Section']== 'TYP')\n",
    "#                       ]['Performance Value'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-concentrate",
   "metadata": {},
   "source": [
    "<a id='Is_Long_Lead_Project_Cost_and_RTL_completed_and_consistent'></a>\n",
    "\n",
    "### Is Long Lead Project Cost and RTL completed and consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Are the Long Lead Project Cost and RTL fields complete and consistent?\n",
    "\n",
    "# cal ='''\n",
    "\n",
    "# IF[Long Lead]=\"Y\" and [Ten-Year Plan RD]<>9999 Then\n",
    "# If [Type of Exception]=\"Long Lead schedule\" then \"OK\" \n",
    "# ElseIF [Section to Use]=\"TYP\" then \n",
    "#     If [Last Year of Fiscal Year]-FLOAT(Right([Target RTL FY],2))<4 or isnull ([Last Year of Fiscal Year])\n",
    "#     or isnull([Target RTL FY]) or isnull([LL PAED Cost ($K)]) or isnull ([TYP Total Project Cost ($K)])\n",
    "#     Then \"Please review PA&ED cost, Long-Lead Cost or PA&ED allocation year and/or Long-Lead RTL. PA&ED allocation year must be 4 or more years before RTL\" \n",
    "#     Else \"OK\"\n",
    "#     End\n",
    "# ElseIf [Section to Use]=\"PRG\" and Isnull([SHOPP Amendment Date]) then \n",
    "#     If [Last Year of Fiscal Year]-Float(Right([Requested RTL FY],2))<4  or isnull ([Last Year of Fiscal Year]) \n",
    "#     or isnull([Requested RTL FY])or isnull([PAED ($K)]) or isnull ([Total LL Prog ($K)])\n",
    "#     Then \"Please review PA&ED cost, Long-Lead Cost or PA&ED allocation year and/or Long-Lead RTL. PA&ED allocation year must be 4 or more years before RTL\" \n",
    "#     Else \"OK\"\n",
    "#     End\n",
    "# Else\"OK\"\n",
    "# End\n",
    "# Else\"OK\"\n",
    "# End\n",
    "# '''\n",
    "\n",
    "def ck_longlead(df):\n",
    "    \n",
    "    #the logic has changed a lot based on the meeting with Mara 9-20-2021\n",
    "    \n",
    "    if df['Long Lead'] != \"Y\" or df['Ten-Year Plan RD']==9999 : # check any long lead project\n",
    "        return \"OK\"\n",
    "    else: #it is a long lead project and not 9999\n",
    "        if \"Long Lead schedule\" in df['Type of Exception'] : \n",
    "            return \"OK\" \n",
    "        elif df['Section'] == \"TYP\" :\n",
    "            initial_comments = 'This project({}) is a long lead project in {} Section. '.format(df['AMT_ID'],df['Section'])\n",
    "            comments = initial_comments\n",
    "            if pd.isnull(df['Last Year of Fiscal Year']):\n",
    "                comments = comments + \"The Last Year of Fiscal Year is missing. \"\n",
    "            if pd.isnull(df['Target RTL FY']):\n",
    "                comments = comments + \"The Target RTL Fiscal Year is missing. \"\n",
    "            if df['TYP Total Project Cost ($K)'] == 0:\n",
    "                comments = comments + \"The TYP Total Project Cost ($K) is missing. \"\n",
    "                \n",
    "            if df['Last Year of Fiscal Year']-df['Target RTL FY Number']<4 :\n",
    "                comments = comments + \"The Last Year of Fiscal Year ({}) is less than 4 year from Target RTL Fiscal Year.({}). PA&ED allocation year must be 4 or more years before RTL. \".format(df['Last Year of Fiscal Year'], df['Target RTL FY Number'])\n",
    "            \n",
    "            if df['Const Cost ($K)']== 0 :\n",
    "                comments = comments + \"The LL construction capital cost is missing. \"\n",
    "                \n",
    "            if df['LL PAED Cost ($K)']== 0 :\n",
    "                comments = comments + \"The PA&ED  cost is missing. \"\n",
    "                \n",
    "            if comments == initial_comments: \n",
    "                return \"OK\"\n",
    "            else:\n",
    "                return comments\n",
    "\n",
    "        elif df['Section'] == \"PRG\": \n",
    "\n",
    "            initial_comments = 'This project({}) is in {} Section. '.format(df['AMT_ID'],df['Section'])\n",
    "            comments = initial_comments\n",
    "            if pd.isnull(df['Last Year of Fiscal Year']):\n",
    "                comments = comments + \"The Long Lead RTL FY is missing. \"\n",
    "            elif df['Last Year of Fiscal Year'] - df['Requested RTL FY Number'] < 4 : # DEBUG: this should flag for project 13559\n",
    "                comments = comments + \"The Last Year of Fiscal Year ({}) is less than 4 year from Target RTL Fiscal Year.({}). PA&ED allocation year must be 4 or more years before RTL. \".format(df['Last Year of Fiscal Year'], df['Requested RTL FY Number'])\n",
    "            if pd.isnull(df['Requested RTL FY']):\n",
    "                comments = comments + \"The PAED Allocation FY is missing. \"\n",
    "                \n",
    "            if df['Total LL Prog ($K)'] == 0:\n",
    "                comments = comments + \"The LL RTL FY Total Cost ($K) is missing. \"\n",
    "                \n",
    "            if df['LL CONS Cap ($K)']== 0 :\n",
    "                comments = comments + \"The LL construction capital cost is missing. \"\n",
    "            \n",
    "            if df['PAED ($K)']== 0 :\n",
    "                comments = comments + \"The PA&ED cost is missing. \"\n",
    "                \n",
    "            if comments == initial_comments: \n",
    "                return \"OK\"\n",
    "            else:\n",
    "                return comments\n",
    "            \n",
    "        else:\n",
    "            return \"OK\"\n",
    "\n",
    "        \n",
    "ck_name = 'Are the Long Lead Project Cost and RTL fields complete and consistent?'\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_longlead, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data[ck_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['Active Long Lead'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "AMT_ID = 13559\n",
    "# AMT_ID = 21862\n",
    "# AMT_ID = 16268\n",
    "\n",
    "AMT_ID = 11296\n",
    "\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "df_SHOPP_raw_data[(df_SHOPP_raw_data['AMT_ID'] == AMT_ID)][['AMT_ID','Section','Active Long Lead' ,  'Type of Exception',                                      \n",
    "'Ten-Year Plan RD' , 'Total LL Prog ($K)' ,\n",
    "'LL CONS Cap ($K)' ,\n",
    "'Requested RTL FY Number',\n",
    "'Const Cost ($K)' ,\n",
    "'Section' ,\n",
    "'LL PAED Cost ($K)' ,\n",
    "'PAED ($K)' ,\n",
    "'Last Year of Fiscal Year' ,\n",
    "'TYP Total Project Cost ($K)' ,\n",
    "'Requested RTL FY' ,\n",
    "\n",
    "'Target RTL FY Number' ,\n",
    "ck_name\n",
    "                                                            ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "AMT_ID = 13559\n",
    "# AMT_ID = 21862\n",
    "# AMT_ID = 16268\n",
    "\n",
    "AMT_ID = 19939\n",
    "\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "df_SHOPP_raw_data[(df_SHOPP_raw_data['AMT_ID'] == AMT_ID)][['AMT_ID','Section','Active Long Lead' ,  'Type of Exception',                                      \n",
    "'Ten-Year Plan RD' , 'Total LL Prog ($K)' ,\n",
    "'LL CONS Cap ($K)' ,\n",
    "'Requested RTL FY Number',\n",
    "'Const Cost ($K)' ,\n",
    "'Section' ,\n",
    "'LL PAED Cost ($K)' ,\n",
    "'PAED ($K)' ,\n",
    "'Last Year of Fiscal Year' ,\n",
    "'TYP Total Project Cost ($K)' ,\n",
    "'Requested RTL FY' ,\n",
    "\n",
    "'Target RTL FY Number' ,\n",
    "ck_name\n",
    "                                                            ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-president",
   "metadata": {},
   "source": [
    "<a id='Are_all_Project_Locations_(PM)_Valid'></a>\n",
    "\n",
    "### Are all Project Locations (PM) Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Are all Project Locations (PM) Valid?\n",
    "\n",
    "# cal ='''\n",
    "# IF [Count of invalid PM]=0 then \"OK\"\n",
    "# Else \"Please review the project locations. One or more project location in this project is invalid.\"\n",
    "# End\n",
    "# '''\n",
    "\n",
    "\n",
    "def ck_Valid_PM(df):\n",
    "    if df['Ten-Year Plan RD'] == 9999:\n",
    "        return 'OK'\n",
    "    elif df['PM_Check'] != 'Has Invalid PM' :\n",
    "        return 'OK'\n",
    "    else:\n",
    "        return 'One or more project locations, primary and/or secondary, in this project in the {} section are invalid. AMT_ID: {}'.format(df['Section'], df['AMT_ID'])\n",
    "\n",
    "ck_name = 'Are all Project Locations and Postmiles Valid?'\n",
    "\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_Valid_PM, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data[ck_name].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-contemporary",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "\n",
    "AMT_ID = 9263\n",
    "\n",
    "df_pm_check[(df_pm_check['AMT_ID'] == AMT_ID)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID']==AMT_ID][['AMT_ID','Section','Section In Use','PM_Check', ck_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-montreal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pm_check[df_pm_check['AMT_ID']==AMT_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-basketball",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "integral-illustration",
   "metadata": {},
   "source": [
    "<a id='Is_Drainage_Worksheet_Complete'></a>\n",
    "\n",
    "### Is Drainage Worksheet Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is Drainage Worksheet Complete (2024/25 RTL and after)? \n",
    "\n",
    "#Question: what does this mean ['Include in Worksheet']\n",
    "\n",
    "# cal ='''\n",
    "# If [Include in Worksheet]=\"Yes\" then\n",
    "# If(isnull([Drainage ID])and [Last Year of Fiscal Year]>24 \n",
    "# and [Include in Performance?]=\"Yes\" \n",
    "# and Left([Performance Objective],15)=\"Drainage System\")\n",
    "# Then 'Please complete drainage worksheet'\n",
    "# Else'OK'\n",
    "# END\n",
    "# Else'OK'\n",
    "# END\n",
    "# '''\n",
    "\n",
    "    \n",
    "\n",
    "def ck_drainage_ws_complete(df):\n",
    "   \n",
    "    #check if \"Last Year of Fiscal Year\" is more than 3 years from now, \"OK\" if no, check further if yes.\n",
    "    #check if Performance raw dataset as has an entry with the same ID and STU, \n",
    "    #        and performance objective that starts with \"Drainage system\"\n",
    "    #if no, \"OK\"\n",
    "    #if yes, check if drainage worksheet dataset has an entry with the same ID and STU, FLAG if no, \"OK\" if yes.\n",
    "    \n",
    "    if df['Ten-Year Plan RD'] == 9999:\n",
    "        return 'OK'\n",
    "    if df['Last Year of Fiscal Year'] <= 2024 or df['Last Year of Fiscal Year'] >= TARGET_FY + 11 : # the drainage worksheet is only available after 2022 SHOPP or later\n",
    "        return 'OK'\n",
    "    else:\n",
    "        if not df['drainage_in_performance']:\n",
    "            return 'OK'\n",
    "        else: \n",
    "\n",
    "            if df['No of Drainage Entries'] > 0:\n",
    "                return 'OK'\n",
    "            else:\n",
    "                return 'The Drainage Worksheet is missing or has not been completed for the PRG section of the project.  AMT_ID: {}'.format(df['AMT_ID'])\n",
    "\n",
    "ck_name = 'Is Drainage Worksheet Complete (2024/25 RTL and after)?'          \n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_drainage_ws_complete, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data[ck_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "\n",
    "AMT_ID = 20234\n",
    "# AMT_ID = 21679\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "df_SHOPP_raw_data[(df_SHOPP_raw_data['AMT_ID'] == AMT_ID)][['AMT_ID','Section',\n",
    "                                                            'drainage_in_performance','No of Drainage Entries',\n",
    "                                                            'Last Year of Fiscal Year',\n",
    "                                                           ck_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_perf_raw_data[(df_perf_raw_data['AMT_ID'] == AMT_ID) & (df_perf_raw_data['Section'] ==STU)][['AMT_ID','Section','Performance Objective']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-saint",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-importance",
   "metadata": {},
   "source": [
    "<a id='Are_all_conditions_selected_for_bridge_replacements'></a>\n",
    "\n",
    "### Are all conditions selected for bridge replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Are all conditions selected for bridge replacements?\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# IF [Ten-Year Plan RD]<> 9999 and [Last Year of Fiscal Year]>24 then\n",
    "#     IF [Section Bridge worksheet]=[Section to Use] Then\n",
    "#     IF isnull([Bridge №]) then \"OK\"\n",
    "#         ElseIF [Work Type]=\"Replacement\" Then \n",
    "#             If Isnull ([Conditions Bridge / Tunnel Health Pre]) Or Isnull([Conditions Bridge Gds Mvmt Pre]) or isnull([Conditions Bridge Scour Pre]) or Isnull ([Conditions Bridge Seismic Pre]) \n",
    "# or (ifnull([Bridge Rail Good (lf)],0)+IFNULL([Bridge Rail Fair (lf)],0)+ifnull([Bridge Rail Poor (lf)],0))=0 \n",
    "#             Then \"Please review Bridge Worksheet. Bridge Replacement should select the condition for Health, Scour, Seismic, Goods and Rail\"\n",
    "#             Else \"OK\"\n",
    "#             END\n",
    "# Else \"OK\"\n",
    "#    END\n",
    "# Else \"OK\"\n",
    "# END\n",
    "# Else \"OK\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "df_SHOPP_raw_data_filtered = df_SHOPP_raw_data[(df_SHOPP_raw_data['Ten-Year Plan RD'] != 9999) \n",
    "                                               & (df_SHOPP_raw_data['Last Year of Fiscal Year'] > 2024)]  # bridge replacement check implemeted in 2022 SHOPP\n",
    "\n",
    "df_brg_raw_data_filtered = df_brg_raw_data[(df_brg_raw_data['WorkType'] == 'Replacement')][[\n",
    "    'AMT_ID','Section','BridgeNo',\n",
    "                'Health Pre',\n",
    "                'Scour_Pre',\n",
    "                'Seismic_Pre', \n",
    "                'GdsMvmt_Pre',\n",
    "                'Rail_Total(lf)']]\n",
    "\n",
    "\n",
    "temp = pd.merge(df_brg_raw_data_filtered, df_SHOPP_raw_data_filtered[['AMT_ID', 'Section']], how='inner', \n",
    "                left_on = ['AMT_ID', 'Section'],\n",
    "                right_on = ['AMT_ID', 'Section'],)\n",
    "\n",
    "def generate_comments(df):\n",
    "    initial_comments = 'For BridgeNo ({}): '.format(df['BridgeNo'])\n",
    "    comments = initial_comments\n",
    "    if pd.isnull(df['Health Pre']): \n",
    "        comments = comments + 'The bridge pre health information is missing. '\n",
    "    if pd.isnull(df['Scour_Pre']): \n",
    "        comments = comments + 'The bridge pre scour information is missing. '        \n",
    "    if pd.isnull(df['Seismic_Pre']): \n",
    "        comments = comments + 'The bridge pre seismic information is missing. '         \n",
    "    if pd.isnull(df['GdsMvmt_Pre']): \n",
    "        comments = comments + 'The bridge pre goods movement information is missing. '        \n",
    "    if pd.isnull(df['Rail_Total(lf)']): \n",
    "        comments = comments + 'The bridge pre total rail quantity is zero. '        \n",
    "    if comments == initial_comments:\n",
    "        return 'OK'\n",
    "    else:\n",
    "        return comments\n",
    "\n",
    "temp['ck_comment_bridge_replacement'] = temp.apply(generate_comments, axis = 1)\n",
    "\n",
    "temp_filtered = temp[temp['ck_comment_bridge_replacement']!= 'OK']\n",
    "#combine comments from mulitple bridges within the same project\n",
    "temp_filtered = temp_filtered.groupby('AMT_ID')['ck_comment_bridge_replacement'].agg(';'.join).reset_index()\n",
    "\n",
    "\n",
    "df_SHOPP_raw_data.drop(['ck_comment_bridge_replacement'], axis=1, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data,temp_filtered,\n",
    "                             how = 'left',\n",
    "                             left_on = 'AMT_ID', right_on = 'AMT_ID')\n",
    "df_SHOPP_raw_data['Are all conditions selected for bridge replacements?'] = df_SHOPP_raw_data.apply(\n",
    "                                                                    lambda df: \"OK\" if pd.isnull(df['ck_comment_bridge_replacement']) \n",
    "                                                                    else '{} AMT_ID: {}, {} Section.'.format(df['ck_comment_bridge_replacement'],df['AMT_ID'],df['Section']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['Are all conditions selected for bridge replacements?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "AMT_ID = 22816\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "temp = df_brg_raw_data[(df_brg_raw_data['AMT_ID'] == AMT_ID) \n",
    "                & (df_brg_raw_data['Section'] == STU)\n",
    "                & (df_brg_raw_data['WorkType'] == 'Replacement')][[\n",
    "                'Health Pre',\n",
    "                'Scour_Pre',\n",
    "                'Seismic_Pre', \n",
    "                'GdsMvmt_Pre',\n",
    "                'Rail_Total(lf)']]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-representation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "grand-return",
   "metadata": {},
   "source": [
    "<a id='Does_Bridge_Worksheet_need_updates'></a>\n",
    "\n",
    "### Does Bridge Worksheet need updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does Bridge Worksheet need updates?\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# if isnull([SHOPP Amendment Date])then \n",
    "#   IF ([Bridge Health Worksheet check])>0  \n",
    "#   Or ([Bridge Goods Worksheet check])>0 \n",
    "#   or ([Bridge Scour Worksheet check])>0 \n",
    "#   or ([Bridge Seismic Worksheet check])>0 \n",
    "#   or([BridgeRail Worksheet check ])>0 \n",
    "#     Then \"Please review Bridge Worksheet to match March 2020 data\"\n",
    "# Else \"OK\"\n",
    " \n",
    "# End\n",
    "# Else \"OK\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "\n",
    "#Bridge Health Worksheet check\n",
    "# BridgeWorksheet_Bridge № \t df_brg_raw_data\n",
    "# Bridge # \t df_bridge_inventory\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# IF [Ten-Year Plan RD]<> 9999 then\n",
    "# IF [Section Bridge worksheet]=[Section to Use] Then\n",
    "# IF isnull([Bridge №]) then 0\n",
    "# ElseIF [Bridge №]= [Bridge #] Then \n",
    "#     If Isnull ([Conditions Bridge / Tunnel Health Pre]) Then 0\n",
    "#     ELSeIF[Conditions Bridge / Tunnel Health Pre]<>[Bridge Health] Or [Deck Areas Exist (sf)]<> [Deck Area, SF] Then 1\n",
    "#     Else 0\n",
    "#     End\n",
    "# Else 0\n",
    "# END\n",
    "# Else 0\n",
    "# End\n",
    "# Else 0\n",
    "# END\n",
    "# '''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ck_bridge_rail_ws(df):\n",
    "    \n",
    "    if (df['Rail_Poor(lf)'] != 0  \n",
    "        and df['Rail_Poor(lf)'] != df['Bridge Rail Upgrade_Poor, LF']):\n",
    "        return 1\n",
    "    elif ((df['Rail_Fair(lf)'] != 0)\n",
    "        and df['Rail_Fair(lf)'] != df['Bridge Rail Upgrade_Fair, LF']):\n",
    "        return 1\n",
    "    elif ((df['Rail_Good(lf)'] != 0) \n",
    "        and df['Rail_Good(lf)'] != df['Bridge Rail Upgrade_Good, LF']):\n",
    "         return 1\n",
    "    else:\n",
    "         return 0\n",
    "\n",
    "\n",
    "def comment_bridge_rail_ws(df):\n",
    "    comment = ''\n",
    "    if (df['Rail_Poor(lf)'] == 0 or df['Rail_Poor(lf)'] == df['Bridge Rail Upgrade_Poor, LF']):\n",
    "        pass\n",
    "    else:\n",
    "        comment += 'Rail_Poor Pre quanity ({}) does not match bridge inventory quantity ({})'.format(\n",
    "            df['Rail_Poor(lf)'] , df['Bridge Rail Upgrade_Poor, LF'])\n",
    "    if (df['Rail_Fair(lf)'] == 0 \n",
    "        or df['Rail_Fair(lf)'] == df['Bridge Rail Upgrade_Fair, LF']):\n",
    "        pass\n",
    "    else:\n",
    "        comment += 'Rail_Fair Pre quanity ({}) does not match bridge inventory quantity ({})'.format(\n",
    "            df['Rail_Fair(lf)'] , df['Bridge Rail Upgrade_Fair, LF'])\n",
    "        \n",
    "    if (df['Rail_Good(lf)'] == 0 \n",
    "        or df['Rail_Good(lf)'] == df['Bridge Rail Upgrade_Good, LF']) :\n",
    "        pass\n",
    "    else:\n",
    "        comment += 'Rail_Good Pre quanity ({}) does not match bridge inventory quantity ({})'.format(\n",
    "            df['Rail_Good(lf)'] , df['Bridge Rail Upgrade_Good, LF'])\n",
    "    return comment\n",
    "\n",
    "#add bridge with mismatch\n",
    "temp = pd.merge(df_brg_raw_data,df_bridge_inventory, how= 'left', left_on = 'BridgeNo', right_on = 'BridgeNo')\n",
    "\n",
    "temp = temp[['AMT_ID', 'Section', 'BridgeNo',\n",
    "    'Health Pre', 'Bridge Health',\n",
    "       'Scour_Pre', 'Bridge Scour',\n",
    "       'Seismic_Pre','Bridge Seismic',\n",
    "       'GdsMvmt_Pre',  'Bridge Goods Movement_Overall',\n",
    "             'Rail_Poor(lf)','Bridge Rail Upgrade_Poor, LF',\n",
    "             'Rail_Fair(lf)','Bridge Rail Upgrade_Fair, LF',\n",
    "            'Rail_Good(lf)', 'Bridge Rail Upgrade_Good, LF',             \n",
    "       'Deck_Exist(sf)', 'Deck Area, SF',  #question to be answered: the logic does not chek deck area, Loren: we leave it to discuss later.\n",
    "            ]]\n",
    "        \n",
    "\n",
    "temp['Bridge Deck Area Mismatch'] = temp.apply(\n",
    "                                        lambda x: (1 if (pd.notnull(x['Deck_Exist(sf)']))\n",
    "#                                                    and (not pd.isnull(x['Deck Area, SF']))\n",
    "                                                   and (x['Deck_Exist(sf)'] != x['Deck Area, SF']) else 0)\n",
    "                                        ,axis = 1)\n",
    "\n",
    "temp['Bridge Health Worksheet Mismatch'] = temp.apply(\n",
    "                                        lambda x: (1 if (pd.notnull(x['Health Pre'])) \n",
    "#                                                    and (not pd.isnull(x['Bridge Health']))\n",
    "                                                   and (x['Health Pre'] != x['Bridge Health'])\n",
    "                                                    else 0)\n",
    "                                        ,axis = 1)\n",
    "temp['Bridge Scour Worksheet Mismatch'] = temp.apply(\n",
    "                                        lambda x: (1 if (pd.notnull(x['Scour_Pre'])) \n",
    "#                                                    and (not pd.isnull(x['Bridge Scour']))\n",
    "                                                   and (x['Scour_Pre'] != x['Bridge Scour'])  else 0)\n",
    "                                        ,axis = 1)\n",
    "\n",
    "temp['Bridge Seismic Worksheet Mismatch'] = temp.apply(\n",
    "                                        lambda x: (1 if (pd.notnull(x['Seismic_Pre'])) \n",
    "#                                                    and (not pd.isnull(x['Bridge Seismic']))\n",
    "                                                   and x['Seismic_Pre'] != x['Bridge Seismic'] else 0)\n",
    "                                        ,axis = 1)\n",
    "temp['Bridge Goods Movement Worksheet Mismatch'] = temp.apply(\n",
    "                                        lambda x: (1 if (pd.notnull(x['GdsMvmt_Pre']))\n",
    "#                                                    and (not pd.isnull(x['Bridge Goods Movement_Overall']))\n",
    "                                                   and (x['GdsMvmt_Pre'] != x['Bridge Goods Movement_Overall'])\n",
    "                                                   else 0 )\n",
    "                                        ,axis = 1)\n",
    "temp['Bridge Rail Worksheet Mismatch'] = temp.apply(\n",
    "                                        ck_bridge_rail_ws\n",
    "                                        ,axis = 1)\n",
    "\n",
    "temp['Bridge Rail Worksheet Comment'] = temp.apply(\n",
    "                                        comment_bridge_rail_ws\n",
    "                                        ,axis = 1)\n",
    "\n",
    "temp['brg_ws_mismatch_counts'] = (\n",
    "    temp['Bridge Deck Area Mismatch']\n",
    "    + temp['Bridge Rail Worksheet Mismatch']\n",
    "     + temp['Bridge Health Worksheet Mismatch']\n",
    "     + temp['Bridge Scour Worksheet Mismatch']\n",
    "     + temp['Bridge Seismic Worksheet Mismatch']\n",
    "     + temp['Bridge Goods Movement Worksheet Mismatch'])\n",
    "\n",
    "\n",
    "        \n",
    "def generate_brg_ws_ck_comment(df):\n",
    "    if df['brg_ws_mismatch_counts'] == 0:\n",
    "        return 'OK'\n",
    "    \n",
    "    initial_comments = 'For {} section in project {}: for bridge No ({}):'.format(df['Section'],df['AMT_ID'], df['BridgeNo'])\n",
    "    comments =  initial_comments\n",
    "    if df['Bridge Deck Area Mismatch'] > 0: \n",
    "        comments = comments + 'Bridge Pre Deck Area ({}) does not match bridge inventory ({})'.format(df['Deck_Exist(sf)'], df['Deck Area, SF'])\n",
    "    if df['Bridge Rail Worksheet Mismatch'] > 0: \n",
    "        comments = comments + '{} '.format(df['Bridge Rail Worksheet Comment'])\n",
    "    if df['Bridge Health Worksheet Mismatch'] > 0: \n",
    "        comments = comments + 'Bridge Pre health quantity ({}) does not match bridge inventory ({})'.format(df['Health Pre'], df['Bridge Health'])\n",
    "    if df['Bridge Scour Worksheet Mismatch'] > 0:\n",
    "        comments = comments + 'Bridge Pre scour quantity ({}) does not match bridge inventory ({})'.format(df['Scour_Pre'] , df['Bridge Scour']) \n",
    "    if df['Bridge Seismic Worksheet Mismatch'] > 0: \n",
    "        comments = comments + 'Bridge Pre seismic quantity ({}) does not match bridge inventory ({})'.format(df['Seismic_Pre'], df['Bridge Seismic'])\n",
    "    if df['Bridge Goods Movement Worksheet Mismatch'] > 0: \n",
    "        comments = comments + 'Bridge Pre Goods Movement quantity ({}) does not match bridge inventory ({})'.format(df['GdsMvmt_Pre'], df['Bridge Goods Movement_Overall'])\n",
    "    \n",
    "    return comments\n",
    "#     if comments == initial_comments: \n",
    "#         return 'OK'\n",
    "#     else:\n",
    "#         return comments\n",
    "\n",
    "    \n",
    "temp['brg_ws_mismatch_comment'] = temp.apply(generate_brg_ws_ck_comment, axis = 1)\n",
    "\n",
    "temp_filtered = temp[temp['brg_ws_mismatch_counts']> 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_grouped = temp_filtered.groupby(['AMT_ID','Section'])['brg_ws_mismatch_comment'].apply(';'.join).reset_index(name = 'brg_ws_mismatch_comments')\n",
    "\n",
    "df_SHOPP_raw_data.drop(['brg_ws_mismatch_comments'], axis=1, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp_grouped[['AMT_ID','Section','brg_ws_mismatch_comments']], \n",
    "                             how ='left', left_on = ['AMT_ID','Section'], right_on = ['AMT_ID','Section'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ck_bridge_ws_against_inventory(df):\n",
    "    \n",
    "#Suggestion: loren: only check project in the project book ,in PPC section (has PCR submitted after bridge WS is implemented), after FY 24\n",
    "    \n",
    "#     AMT_ID = df['AMT_ID']\n",
    "#     STU = df['Section']\n",
    "    \n",
    "    if df['Ten-Year Plan RD'] == 9999 or pd.notnull(df['SHOPP Amendment Date']):\n",
    "        return 'OK'\n",
    "\n",
    "    else:\n",
    "        if pd.isnull(df['brg_ws_mismatch_comments']) :\n",
    "            return 'OK'\n",
    "        else:\n",
    "            return df['brg_ws_mismatch_comments']      \n",
    "ck_name = 'Does Bridge Worksheet need updates?'\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_bridge_ws_against_inventory, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "AMT_ID = 18944\n",
    "\n",
    "\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','Ten-Year Plan RD','AM Tool RTL (Section in Use)',\n",
    "                                                         'SHOPP Amendment Date','Will this project be included in the Project Book?',\n",
    "                                                         'brg_ws_mismatch_comments',ck_name]]\n",
    "\n",
    "# temp[(temp['AMT_ID'] ==AMT_ID) & (temp['Section'] == STU)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp[(temp['AMT_ID'] ==AMT_ID) & (temp['Section'] == STU)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-header",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_brg_raw_data[(df_brg_raw_data['AMT_ID'] ==AMT_ID) & (df_brg_raw_data['Section'] == STU)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-sculpture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-principle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "everyday-domain",
   "metadata": {},
   "source": [
    "<a id='Does_the_Plan_Year_in_the_Pavement_Worksheet_match_the_Project_RTL'></a>\n",
    "\n",
    "### Does the Plan Year in the Pavement Worksheet match the Project RTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does the Plan Year in the Pavement Worksheet match the Project RTL?\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# If [Include in Worksheet]=\"Yes\" and [Last Year of Fiscal Year]<30 then\n",
    "# If(isnull([Plan Year])) then \"OK\"\n",
    "# ElseIf [Last Year of Fiscal Year]+2000<2016 and [Plan Year]=2016 then \"OK\"\n",
    "# ElseIf([Plan Year]=[Last Year of Fiscal Year]+2000 and [Pavement Section]=[Section to Use])\n",
    "# Then \"OK\"\n",
    "# Else \"Please review Pavement Worksheet Plan Year. It does not match the project RTL.\"\n",
    "# END\n",
    "# ElSe \"OK\"\n",
    "# End\n",
    "# '''\n",
    "\n",
    "\n",
    "def ck_pavement_ws_plan_year_RTL(df):\n",
    "    if df['Ten-Year Plan RD'] == 9999:\n",
    "        return  'OK'\n",
    "    \n",
    "    if df['Last Year of Fiscal Year'] < TARGET_FY + 11 : # we decided to check only the 10-year plan projects   \n",
    "        if(pd.isnull(df['Pavement_PlanYear'])) : \n",
    "            return  \"OK\"\n",
    "        \n",
    "        #TODO: discuss this logic with Mara for multiple pavement plan years\n",
    "        elif df['No_Pavement_PlanYear'] > 1: \n",
    "            return \"Please review Pavement Worksheet Plan Year for all locations. Only one pavement worksheet plan year is allowed for all locations and the plan year needs to match the project fiscal year ({}). AMT_ID: {}\".format(df['Last Year of Fiscal Year'],df['AMT_ID'])\n",
    "        \n",
    "        elif df['Last Year of Fiscal Year']<2016 and df['Pavement_PlanYear'] == 2016: #static number, because the first set of data we have is 2016 APCS data \n",
    "            return  \"OK\"\n",
    "        \n",
    "        elif(df['Last Year of Fiscal Year'] == df['Pavement_PlanYear'] ): \n",
    "            return  \"OK\"\n",
    "        \n",
    "        else: \n",
    "            return 'Please update the pavement worksheet Plan Year to match the Project RTL. The pavement worksheet plan year ({:.0f}) does not match the AM Tool project RTL FY ({}). AMT_ID: {}'.format(df['Pavement_PlanYear'], df['Last Year of Fiscal Year'],df['AMT_ID'])\n",
    "    else: \n",
    "        return \"OK\"\n",
    "\n",
    "df_SHOPP_raw_data['Does the Plan Year in the Pavement Worksheet match the Project RTL?'] = df_SHOPP_raw_data.apply(ck_pavement_ws_plan_year_RTL, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-ensemble",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['Does the Plan Year in the Pavement Worksheet match the Project RTL?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-commerce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "AMT_ID = 18437 #\n",
    "AMT_ID = 15971\n",
    "AMT_ID= 18495\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID][['AMT_ID','Section',\n",
    "                                                           'Last Year of Fiscal Year',\n",
    "                                                           'Pavement_PlanYear','No_Pavement_PlanYear',\n",
    "                                                           'Does the Plan Year in the Pavement Worksheet match the Project RTL?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pav_raw_data[df_pav_raw_data['AMT_ID'] == AMT_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['Does the Plan Year in the Pavement Worksheet match the Project RTL?'] != 'OK'][['AMT_ID','Section',\n",
    "#                                                            'Last Year of Fiscal Year',\n",
    "#                                                            'Pavement_PlanYear',\n",
    "#                                                            'Does the Plan Year in the Pavement Worksheet match the Project RTL?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-supervision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-committee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-directive",
   "metadata": {},
   "source": [
    "<a id='Is_Pavement_Worksheet_Complete'></a>\n",
    "\n",
    "### Is Pavement Worksheet Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is Pavement Worksheet Complete (2024/25 RTL and after)?\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# If [Include in Worksheet]=\"Yes\" then\n",
    "# If(isnull([Plan Year])and [Last Year of Fiscal Year]>24 and [Last Year of Fiscal Year]<31 \n",
    "#and [Include in Performance?]=\"Yes\" and Left([Performance Objective],8)=\"Pavement\")\n",
    "# Then 'Please complete pavement worksheet'\n",
    "# Else'OK'\n",
    "# END\n",
    "# Else'OK'\n",
    "# END\n",
    "# '''\n",
    "\n",
    "\n",
    "def ck_pav_ws_complete(df):\n",
    "    \n",
    "    #updated logic\n",
    "    # if ['Performance Objective has Pavement'] == 'Yes', \n",
    "    #if last fiscal year is exclusively between 24 and 31, \n",
    "    # and pavement worksheet has a plan year matching last fiscal year, \n",
    "    # and performance worksheet has an entry of same project ID and section, and Performance Objective starting with 'Pavement'\n",
    "    # Question: can we make the logic more specific and reading friendly \n",
    "    # df_perf_raw_data['Performance Objective'] in ['Pavement Class I','Pavement Class II','Pavement Class III']\n",
    "    \n",
    "    if df['Ten-Year Plan RD'] == 9999:\n",
    "        return 'OK'\n",
    "    if (df['Last Year of Fiscal Year']> 2024 and df['Last Year of Fiscal Year']<TARGET_FY + 11 \n",
    "        and df['Performance Objective has Pavement'] == 'Yes'): #2024 static since pavement worksheet is required after 2024.\n",
    "        \n",
    "        \n",
    "        if(pd.isnull(df['Pavement_PlanYear'])) : #if found plan year, means there is an entry in pavement worksheet\n",
    "            return 'The Pavement Worksheet is missing or has not been completed for the {} section of the project. AMT_ID: {}'.format(df['Section'],df['AMT_ID'])\n",
    "\n",
    "#         elif not (df['Last Year of Fiscal Year'] == df['Pavement_PlanYear']):   #this is checked elsewhere.\n",
    "#             return  'Please correct the pavement worksheet. The plan year ({}) in the pavement worksheet in {} Section does not match the last year of fiscal year of the project ({}).'.format(df['Pavement_PlanYear'], df['Section'], df['Last Year of Fiscal Year'])\n",
    "        else: \n",
    "            return 'OK'\n",
    "        \n",
    "    else: \n",
    "        return 'OK'\n",
    "\n",
    "ck_name = 'Is Pavement Worksheet Complete (2024/25 RTL and after)?'\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_pav_ws_complete, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data[ck_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "AMT_ID = 11365\n",
    "AMT_ID = 20231\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section',\n",
    "                                                           'Last Year of Fiscal Year',\n",
    "                                                           'Pavement_PlanYear',\n",
    "                                                           'Performance Objective has Pavement',\n",
    "                                                           ck_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-staff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-internship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "educational-vancouver",
   "metadata": {},
   "source": [
    "<a id='Is_the_Pavement_Work_Limits_Direction_in_the_Pavement_Worksheet_complete'></a>\n",
    "\n",
    "### Is the Pavement Work Limits Direction in the Pavement Worksheet complete (Obselete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is the Pavement Work Limits Direction in the Pavement Worksheet complete? \n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# If [Include in Worksheet]=\"Yes\" then\n",
    "# IF isnull([Plan Year]) then \"OK\"\n",
    "# ElseIf (isnull([Direction])) then \"Please review Pavement Worksheet Work Limits direction. It is blank and it is probably using a worksheet logic that was not in aligment with PAVEM.\"\n",
    "# Else \"OK\"\n",
    "# END\n",
    "# ElSe \"OK\"\n",
    "# End\n",
    "# '''\n",
    "\n",
    "\n",
    "def ck_pavement_worklimit_direction(df):\n",
    "    if pd.isnull(df['Pavement_PlanYear']) : \n",
    "        return  \"OK\"\n",
    "    elif (df['Direction_check']=='Direction Info Missing') : \n",
    "        return  \"For {} section in project {}: Please review Pavement Worksheet Work Limits direction. It is blank and it is probably using a worksheet logic that was not in aligment with PAVEM.\".format(df['Section'],df['AMT_ID'],)\n",
    "    else: \n",
    "        return \"OK\"\n",
    "\n",
    "df_SHOPP_raw_data['Is the Pavement Work Limits Direction in the Pavement Worksheet complete?'] = df_SHOPP_raw_data.apply(ck_pavement_worklimit_direction, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-rugby",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['Is the Pavement Work Limits Direction in the Pavement Worksheet complete?'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-omega",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "AMT_ID = 16724\n",
    "AMT_ID = 20342\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section',\n",
    "                                                           'Pavement_PlanYear',\n",
    "                                                           'Direction_check',\n",
    "                                                           'Is the Pavement Work Limits Direction in the Pavement Worksheet complete?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pav_raw_data[(df_pav_raw_data['AMT_ID'] == AMT_ID ) & (df_pav_raw_data['Section'] == STU)\n",
    "#                  ][['AMT_ID', \n",
    "#        'Section', 'Direction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-letters",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-navigator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "opposed-spring",
   "metadata": {},
   "source": [
    "<a id='Is_TMS_Worksheet_Complete'></a>\n",
    "\n",
    "### Is TMS Worksheet Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is TMS Worksheet Complete (2024/25 RTL and after)? \n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# If [Include in Worksheet]=\"Yes\" then\n",
    "# If(isnull([TMS RTL Plan Year])and [Last Year of Fiscal Year]>24 and [Last Year of Fiscal Year]<31 and \n",
    "# [Include in Performance?]=\"Yes\" and [Performance Objective]=\"Transportation Management Systems\")\n",
    "# Then 'Please complete TMS worksheet'\n",
    "# Else'OK'\n",
    "# END\n",
    "# Else'OK'\n",
    "# END\n",
    "# '''\n",
    "\n",
    "# def ck_TMS_ws_complete(df):\n",
    "    \n",
    "#     if(pd.isnull(df['RTL Plan Year'])) : #if found plan year, means there is an entry in TMS worksheet\n",
    "#         return  \"OK\"\n",
    "#     else:\n",
    "#         if(df['Last Year of Fiscal Year']>24 \n",
    "#            and df['Last Year of Fiscal Year']<31 \n",
    "#            and df['Last Year of Fiscal Year'] != df['RTL Plan Year']%100\n",
    "#            and df['Performance Objective has TMS'] == 'Yes'): \n",
    "#             return  'Please complete TMS worksheet'\n",
    "#         else: return 'OK'\n",
    "        \n",
    "def ck_TMS_ws_complete(df):\n",
    "    if df['Ten-Year Plan RD'] == 9999:\n",
    "        return 'OK'\n",
    "#     if(pd.isnull(df['TMS_PlanYear'])) : #if found plan year, means there is an entry in TMS worksheet\n",
    "#         return  \"OK\"\n",
    "    else:\n",
    "        if(df['Last Year of Fiscal Year']>2024 and df['Last Year of Fiscal Year']<TARGET_FY + 11 \n",
    "#            and not df['Last Year of Fiscal Year'] == df['TMS_PlanYear']\n",
    "            and (pd.isnull(df['TMS_PlanYear']))\n",
    "           and df['Performance Objective has TMS'] == 'Yes'): \n",
    "            return  'Please complete the TMS worksheet in the TYP section. AMT_ID: {}'.format(df['AMT_ID'])\n",
    "        else: return 'OK'\n",
    "        \n",
    "        \n",
    "ck_name =  'Is TMS Worksheet Complete (2024/25 RTL and after)?'\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_TMS_ws_complete, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-inspector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data[ck_name].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-decision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "# AMT_ID = 20078\n",
    "AMT_ID = 16365\n",
    "AMT_ID = 19546\n",
    "\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','Ten-Year Plan RD',\n",
    "                                                           'Last Year of Fiscal Year',\n",
    "                                                           'TMS_PlanYear','Performance Objective has TMS',\n",
    "                                                           ck_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tms_raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tms_raw_data[(df_tms_raw_data['AMT_ID']==AMT_ID) & (df_tms_raw_data['Section'] == STU)][['AMT_ID','Section','RTL Plan Year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-invitation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-edgar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-clearance",
   "metadata": {},
   "source": [
    "<a id='Does_the_RTL_Plan_Year_in_the_TMS_Worksheet_match_the_Project_RTL'></a>\n",
    "\n",
    "### Does the RTL Plan Year in the TMS Worksheet match the Project RTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Does the RTL Plan Year in the TMS Worksheet match the Project RTL?\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# If [Include in Worksheet]=\"Yes\" and [Last Year of Fiscal Year]<30 then\n",
    "# If(isnull([TMS RTL Plan Year])) then \"OK\"\n",
    "# ElseIf [Last Year of Fiscal Year]+2000<2020 and [TMS RTL Plan Year]=2020 then \"OK\"\n",
    "# ElseIf([TMS RTL Plan Year]=[Last Year of Fiscal Year]+2000 and [TMS Section]=[Section to Use])\n",
    "# Then \"OK\"\n",
    "# Else \"Please review TMS Worksheet RTL Plan Year. It does not match the project RTL.\"\n",
    "# END\n",
    "# ElSe \"OK\"\n",
    "# End\n",
    "# '''\n",
    "\n",
    "\n",
    "# def ck_TMS_ws_plan_year_RTL(df):\n",
    "    \n",
    "#     if df['Last Year of Fiscal Year']<30 :    \n",
    "#         if(pd.isnull(df['RTL Plan Year'])) : \n",
    "#             return  \"OK\"\n",
    "#         elif df['Last Year of Fiscal Year']+2000<2020 and df['RTL Plan Year'] == 2020 : \n",
    "#             #question why 2016 for pavement , 2020 for TMS \n",
    "#             #Mara: due to the availablibility of the pavement and TMS data\n",
    "#             return  \"OK\"\n",
    "#         elif(df['RTL Plan Year'] == df['Last Year of Fiscal Year']+2000): \n",
    "#             return  \"OK\"\n",
    "#         else: \n",
    "#             return \"Please review TMS Worksheet RTL Plan Year. It does not match the project RTL.\"\n",
    "#     else: \n",
    "#         return \"OK\"\n",
    "    \n",
    "def ck_TMS_ws_plan_year_RTL(df):\n",
    "    if df['Ten-Year Plan RD'] == 9999:\n",
    "        return  'OK'\n",
    "    if df['Last Year of Fiscal Year'] < TARGET_FY + 11 :    \n",
    "        if(pd.isnull(df['TMS_PlanYear'])) : \n",
    "            return  \"OK\"\n",
    "        elif df['No_TMS_PlanYear'] > 1: \n",
    "            return \"Please review TMS Worksheet Plan Year for all locations. Only one TMS worksheet plan year is allowed for all locations and the plan year needs to match the project fiscal year ({}). AMT_ID: {}\".format(df['Last Year of Fiscal Year'],df['AMT_ID'])\n",
    "            \n",
    "        elif df['Last Year of Fiscal Year']< 2020 and (2020 == df['TMS_PlanYear']) : \n",
    "            #2020 for TMS  due to the availablibility of the pavement and TMS data\n",
    "            return  \"OK\"\n",
    "        elif(df['Last Year of Fiscal Year'] == df['TMS_PlanYear']): \n",
    "            return  \"OK\"\n",
    "        else: \n",
    "            \n",
    "            return 'Please update the TMS Worksheet RTL Plan Year to match the Project RTL.  The TMS Worksheet RTL Plan Year ({:.0f}) does not match the AM Tool project RTL ({}). AMT_ID: {}'.format(df['TMS_PlanYear'], df['Last Year of Fiscal Year'],df['AMT_ID'])\n",
    "    else: \n",
    "        return \"OK\"\n",
    "\n",
    "    \n",
    "ck_name = 'Does the RTL Plan Year in the TMS Worksheet match the Project RTL?'\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_TMS_ws_plan_year_RTL, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['Does the RTL Plan Year in the TMS Worksheet match the Project RTL?'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-civilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####Unit Test#####\n",
    "\n",
    "AMT_ID = 19941\n",
    "\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section',\n",
    "                                                           'Last Year of Fiscal Year',\n",
    "                                                           'TMS_PlanYear',\n",
    "                                                           'Does the RTL Plan Year in the TMS Worksheet match the Project RTL?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMT_ID = 18611\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section',\n",
    "                                                           'Last Year of Fiscal Year',\n",
    "                                                           'TMS_PlanYear',\n",
    "                                                           'Does the RTL Plan Year in the TMS Worksheet match the Project RTL?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-check",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "blocked-mattress",
   "metadata": {},
   "source": [
    "<a id='Do_SHOPP_project_data_in_the_AM_Tool_match_CTIPS_data'></a>\n",
    "\n",
    "### Do SHOPP project data in the AM Tool match CTIPS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do SHOPP project data in the AM Tool match CTIPS data (RTL and/or Cost)?\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# If isnull([TOOL ID to obsolete])then\n",
    "# IF [Type of Exception] = \"Cost or RTL not matching CTIPS\" then \"OK\"\n",
    "# ElseIf([Planning or Post-Planning]=\"Post-Planning\") Then\n",
    "#     If(abs([Shopp Tool Cost to use]-[Total Capital & Support Cost])<.9 and [AM Tool RTL (Section in Use)]=[FY]) then \"OK\"\n",
    "#     Else \"Please check cost and RTL data in the PRG or PPC section. Data is not matching CTIPs data (RTL and Cost)\"\n",
    "#     END\n",
    "# Else \"OK\"\n",
    "# END\n",
    "# Else \"Please Obsolete Proejct in the AM Tool. Project was un-pared, split or combined with another project.\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "\n",
    "def ck_CTIPS(df):\n",
    "    \n",
    "    if df['Ten-Year Plan RD'] == 9999:\n",
    "        return  'OK'\n",
    "    \n",
    "    if df['AMT_ID to obsolete']== 'Obselete':  \n",
    "        return \"Please Obsolete Proejct in the AM Tool. Project was un-pared, split or combined with another project.\"\n",
    "    \n",
    "    elif df['SHOPP Amendment Date'] == '04/01/23': #Needs attention \n",
    "#         ''' if the shopp amendment date is 04/01/23, check the candidate list, \n",
    "#         if in candidate list, check the cost and RTL against the candidate list (Advertised Year\tProject Cost ($K) )\n",
    "#         '''\n",
    "    \n",
    "        initial_comment = 'For the {} section of project ID({}), '.format(df['Section'], df['AMT_ID'])\n",
    "        comments = initial_comment \n",
    "        if(abs(df['Shopp Tool Cost to use']-df['Project Cost ($K)_Candidate'])>=.9): #if the difference is no less than 900 dollars\n",
    "            comments += 'The AM Tool Total Cost (${:.0f}k) does not match SHOPP Candidate Project Cost (${:.0f}k).'.format(df['Shopp Tool Cost to use'], df['Project Cost ($K)_Candidate'])\n",
    "        if df['AM Tool RTL (Section in Use)'] != df['Advertised Year_Candidate'] : \n",
    "            comments += 'The AM Tool RTL FY ({}) does not match SHOPP Candidate RTL FY ({}).'.format(df['AM Tool RTL (Section in Use)'], df['Advertised Year_Candidate'])\n",
    "\n",
    "        if comments == initial_comment :\n",
    "            return 'OK'\n",
    "        else:\n",
    "            return comments + ' If the project should be in another section, and if a PCR or PCD was approved and the PPC section updated, please notify your liaison for review and entering amendment date. If you have a pending PCR please notify your district liaison to add to the book checks exception.'\n",
    "    else:\n",
    "        if \"Cost or RTL not matching CTIPS\" in df['Type of Exception']: \n",
    "            return  \"OK\"\n",
    "\n",
    "        elif(df['Planning or Post-Planning'] == \"Post-Planning\") :  \n",
    "            initial_comment = 'For the {} section of project ID({}), '.format(df['Section'], df['AMT_ID'])\n",
    "            comments = initial_comment \n",
    "            if(abs(df['Shopp Tool Cost to use']-df['Total Capital & Support Cost'])>=.9): #if the difference is no less than 900 dollars\n",
    "                comments += 'The AM Tool Total Cost (${:.0f}k) does not match CTIPS Total Cost (${:.0f}k).'.format(df['Shopp Tool Cost to use'], df['Total Capital & Support Cost'])\n",
    "            if df['AM Tool RTL (Section in Use)'] != df['FY'] : \n",
    "                comments += 'The AM Tool RTL FY ({}) does not match SHOPP Candidate RTL FY ({}).'.format(df['AM Tool RTL (Section in Use)'], df['FY'])\n",
    "\n",
    "            if comments == initial_comment :\n",
    "                return 'OK'\n",
    "            else:\n",
    "                return comments + ' If the project should be in another section, and if a PCR or PCD was approved and the PPC section updated, please notify your liaison for review and entering amendment date. If you have a pending PCR please notify your district liaison to add to the book checks exception.'\n",
    "        else: \n",
    "            return \"OK\"\n",
    "\n",
    "\n",
    "ck_name = 'Does SHOPP project data in the AM Tool data match CTIPS (RTL and/or Cost)?'\n",
    "\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_CTIPS, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-genealogy",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data[ck_name].value_counts(dropna = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-likelihood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data[df_SHOPP_raw_data[ck_name] != 'OK'][['AMT_ID','Section','AMT_ID to obsolete','Planning or Post-Planning',\n",
    "#                                                            'Shopp Tool Cost to use','Total Capital & Support Cost',\n",
    "#                                                            'AM Tool RTL (Section in Use)','FY',ck_name]].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ####Unit Test#####\n",
    "\n",
    "# AMT_ID = 22124\n",
    "# ck_name = 'Does SHOPP project data in the AM Tool data match CTIPS (RTL and/or Cost)?'\n",
    "\n",
    "# STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','SHOPP Amendment Date','Long Lead','AMT_ID to obsolete','Planning or Post-Planning',\n",
    "#                                                            'Shopp Tool Cost to use','Total Capital & Support Cost',\n",
    "#                                                            'AM Tool RTL (Section in Use)','FY','Prog Total Project Cost ($K)',\n",
    "#                                                            ck_name]]\n",
    "                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][ck_name].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-special",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_shopp_candidate[df_shopp_candidate['AMT_ID'] ==AMT_ID]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-sunglasses",
   "metadata": {},
   "source": [
    "<a id='Is_PID_completed_and_uploaded_for_current_SHOPP_Candidates'></a>\n",
    "\n",
    "### Is PID completed and uploaded for current SHOPP Candidates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is PID completed and uploaded for current SHOPP Candidates?\n",
    "\n",
    "\n",
    "# cal ='''\n",
    "# If([Planning or Post-Planning]=\"Planning\") and [Will this project be included in the Project Book?]=\"Yes\" Then\n",
    "#     IF [Last Year of Fiscal Year]=25 or [Last Year of Fiscal Year]=26 then\n",
    "#         If  [Section to Use]=\"PRG\" Then \"OK\"\n",
    "#         Else \"For 2022 SHOPP candidates, planned projects with RTL FY 2025/26 and 2026/27, District should complete the PRG section information and upload a completed PID.\"\n",
    "#         END   \n",
    "#     ELSEIF [PA&ED FY]=23  then\n",
    "#         If[Section to Use]=\"PRG\" Then \"OK\"\n",
    "#         Else \"For 2022 Long Lead SHOPP candidates, Long Lead projects with PA&ED FY 2022/23, District should complete the PRG section information and upload a completed PID.\"\n",
    "#         END  \n",
    "# Else \"OK\"\n",
    "# End\n",
    "# Else \"OK\"\n",
    "# End\n",
    "# '''\n",
    "\n",
    "\n",
    "def ck_PID_status(df):\n",
    "    \n",
    "    if(df['Planning or Post-Planning'] == \"Planning\"\n",
    "        and df['Will this project be included in the Project Book?'] == \"Yes\"\n",
    "        and df['Activity (group)'] != 'Reservation') : \n",
    "        if df['Last Year of Fiscal Year'] in [TARGET_FY + 6, TARGET_FY + 7] :  \n",
    "            if  df['Section'] == \"PRG\" : \n",
    "                return  \"OK\"\n",
    "            else: \n",
    "                return \"For {} section in project {}: For {} SHOPP candidates, planned projects with RTL FY {} or {}, District should complete the PRG section information and upload a completed PID.\".format(df['Section'],df['AMT_ID'],TARGET_FY + 3, TARGET_FY + 6, TARGET_FY + 7)\n",
    "\n",
    "        elif df['PA&ED FY Number'] == TARGET_FY + 4: \n",
    "            if df['Section'] == \"PRG\" : \n",
    "                return  \"OK\"\n",
    "            else: \n",
    "                return \"For {} section in project {}: For {} Long Lead SHOPP candidates, Long Lead projects with PA&ED FY {}, District should complete the PRG section information and upload a completed PID.\".format(df['Section'],df['AMT_ID'],TARGET_FY + 3, df['PA&ED FY Number'])\n",
    "\n",
    "        else: \n",
    "            return \"OK\"\n",
    "\n",
    "    else: \n",
    "        return \"OK\"\n",
    "\n",
    "df_SHOPP_raw_data['Is PID completed and uploaded for current SHOPP Candidates?'] = df_SHOPP_raw_data.apply(ck_PID_status, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['Is PID completed and uploaded for current SHOPP Candidates?'].value_counts(dropna =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-facial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-medline",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Unit Test#####\n",
    "\n",
    "AMT_ID = 20220\n",
    "\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section',\n",
    "                                                           'Planning or Post-Planning',\n",
    "                                                           'Will this project be included in the Project Book?',\n",
    "                                                           'PA&ED FY Number',\n",
    "                                                           'Is PID completed and uploaded for current SHOPP Candidates?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-theme",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['AMT_ID'].value_counts().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-rotation",
   "metadata": {},
   "source": [
    "<a id='Is_CCE_uploaded'></a>\n",
    "\n",
    "### Is CCE uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is CCE uploaded? (5-year POR)\n",
    "\n",
    "# cal ='''\n",
    "# If [Activity (group)]=\"Reservation\" Then \"OK\"\n",
    "# ElseIf([Include 5-year POR?]=\"Yes\" and ISnull ([PID Uploaded])and  Isnull([CCE Uploaded]) and Isnull ([PIP Uploaded])and [Section to Use]=\"TYP\") Then \"CCE needs to be uploaded\"\n",
    "#     Else \"OK\"\n",
    "#     End\n",
    "# '''\n",
    "\n",
    "ck_name = 'Is the Conceptual Cost Estimate (CCE) uploaded? (Applies to all project in the 5-year POR)'\n",
    "\n",
    "def ck_CCE_upload(df):\n",
    "    if df['Activity (group)'] == \"Reservation\" : \n",
    "        return  \"OK\"\n",
    "    elif(df['Include 5-year POR?'] == \"Yes\" \n",
    "         and pd.isnull(df['PID Uploaded'])\n",
    "         and pd.isnull(df['CCE Uploaded']) \n",
    "         and pd.isnull(df['PIP Uploaded'])\n",
    "         and df['Section'] == \"TYP\") :\n",
    "        return 'Please upload the appropriate supporting document (PIP or CCE). AMT_ID: {}'.format(df['AMT_ID'])\n",
    "#         return  \"This project ({}) is in {} section. But none of the PID/PIP/CCE was uploaded. At least one needs to be uploaded.\".format(df['AMT_ID'], df['Section'])\n",
    "    else: \n",
    "        return \"OK\"\n",
    "\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_CCE_upload, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-outside",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data[ck_name].value_counts(dropna =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###Unit Test#####\n",
    "\n",
    "AMT_ID = 19149\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][[\n",
    "    'AMT_ID','PIP Uploaded','CCE Uploaded','PID Uploaded','District',ck_name]]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID']== 20748][['AMT_ID','PIP Uploaded','CCE Uploaded','PID Uploaded','District']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # <a id='LL_not_in_POR'></a>\n",
    "\n",
    "# ### LL not in POR\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def LLinPOR(df):\n",
    "#     if (df['Active Long Lead'] == \"Yes\"\n",
    "#             and df['Last Year of Fiscal Year']< TARGET_FY + 11\n",
    "#             and df['Planning or Post-Planning'] == \"Post-Planning\" \n",
    "#             and df['Include 5-year POR?'] == \"No\"): \n",
    "        \n",
    "#         return \"Check data. This long lead Project is programmed but not in the POR.\"\n",
    "#     else: \n",
    "#         return \"OK\"\n",
    "\n",
    "# df_SHOPP_raw_data['LL not in POR']= df_SHOPP_raw_data.apply(LLinPOR, axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# ###Unit Test#####\n",
    "\n",
    "# AMT_ID = 19149\n",
    "\n",
    "\n",
    "# STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','Last Year of Fiscal Year' ,\n",
    "# 'Active Long Lead' ,\n",
    "# 'Planning or Post-Planning' ,\n",
    "# 'Include 5-year POR?','Ten-Year Plan RD' ,\n",
    "# 'SHOPP Amendment Date' ,\n",
    "# 'Long Lead' ,\n",
    "# 'Last Year FY POR' ,\n",
    "# 'Activity (group)' ,]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-portuguese",
   "metadata": {},
   "source": [
    "<a id='Is_Pavement_limits_repeating_in_the_same_project'></a>\n",
    "\n",
    "## Is Pavement limits repeating in the same project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-skirt",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Include in the Analysis?\n",
    "\n",
    "# cal = \n",
    "# '''\n",
    "# IF [Section In Use]=[Pavement Section] Then\n",
    "# \"Yes\"\n",
    "# Else \"No\"\n",
    "# End\n",
    "\n",
    "# '''\n",
    "\n",
    "# # Is Pavement limits repeating in the same project?\n",
    "\n",
    "# cal = '''\n",
    "# IF [count unique pave limits in same project]>1\n",
    "# Then \"Please review Pavement Worksheet. One or more pavement limits are repeating in this project\"\n",
    "# Else \"OK\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "# # count unique pave limits in same project\n",
    "\n",
    "# cal = '''\n",
    "# {Fixed [Pavement Section],[Section In Use],[UNIQUE ID PAvement],[Date]:Count ([UNIQUE ID PAvement])}\n",
    "# '''\n",
    "\n",
    "# # UNIQUE ID PAvement\n",
    "\n",
    "# cal = '''\n",
    "# str([ID Pavement])+str(([County Pavement]))+[Work Limits Rout]+ ifnull(BPP,\"\")+str([Beg PM])+ifnull([BPS],\"\")+ifnull([EPP],\"\")+str([End PM])+ifnull([EPS],\"_\")+ifnull([Direction],\"\")+ str(ifnull([Lane],0))+STR([Roadway Class])\n",
    "# '''\n",
    "\n",
    "\n",
    "# def calc_unique_pave_ID(df):\n",
    "    \n",
    "#     return (df['County']\n",
    "#      + '_'+ df['Route']\n",
    "#      + '_'+ (\"\" if pd.isnull(df['BPP']) else df['BPP'])\n",
    "#      + '_'+ str(df['Beg PM'])\n",
    "#      + '_'+ (\"\" if pd.isnull(df['BPS']) else df['BPS'])\n",
    "#      + '_'+ (\"\" if pd.isnull(df['EPP']) else df['EPP'])\n",
    "#      + '_'+ str(df['End PM'])\n",
    "#      + '_'+ (\"\" if pd.isnull(df['EPS']) else df['EPS'])\n",
    "#      + '_'+ (\"\" if pd.isnull(df['Direction']) else df['Direction'])\n",
    "#      + '_'+ str(0 if pd.isnull(df['Lane']) else df['Lane'])\n",
    "#      + '_'+ str(df['RoadwayClass'])\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "df_pav_raw_data['Unique_Pave_Limits'] = df_pav_raw_data.apply(uf.calc_unique_pave_ID, axis = 1)\n",
    "\n",
    "temp = df_pav_raw_data.groupby(['AMT_ID','Section','Unique_Pave_Limits'])['Program'].agg('count').reset_index(name = 'Count of Unique_Pave_Limits')\n",
    "\n",
    "temp_filtered = temp[temp['Count of Unique_Pave_Limits'] > 1]\n",
    "\n",
    "\n",
    "if temp_filtered.empty:\n",
    "    df_SHOPP_raw_data['Is Pavement limits repeating in the same project?'] = 'OK'\n",
    "\n",
    "else:\n",
    "    temp_out = temp_filtered.groupby(['AMT_ID','Section'])['Unique_Pave_Limits'].agg(';'.join)\n",
    "    temp_out = temp_out.reset_index(name = 'Repeated Unique_Pave_Limits')\n",
    "\n",
    "\n",
    "    df_SHOPP_raw_data.drop(columns =['Repeated Unique_Pave_Limits'], inplace=True, errors='ignore')\n",
    "\n",
    "    df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp_out, how= 'left', \n",
    "                                 left_on = ['AMT_ID','Section'],\n",
    "                                 right_on = ['AMT_ID','Section'])\n",
    "\n",
    "    def ck_repeated_pavement(df):\n",
    "        if pd.isnull(df['Repeated Unique_Pave_Limits']):\n",
    "            return 'OK'\n",
    "        else:\n",
    "            return 'Please review Pavement Worksheet for repeating segments ({}) in {} Section. AMT_ID: {}'.format(df['Repeated Unique_Pave_Limits'], df['Section'],df['AMT_ID'])\n",
    "\n",
    "    df_SHOPP_raw_data['Is Pavement limits repeating in the same project?'] = df_SHOPP_raw_data.apply(ck_repeated_pavement, axis = 1)\n",
    "\n",
    "    # df_SHOPP_raw_data['Is Pavement limits repeating in the same project?'].value_counts()\n",
    "\n",
    "\n",
    "    # df_SHOPP_raw_data.drop(columns =['List of Repeated Unique_Pave_Limits','Count of Repeated Unique_Pave_Limits'], inplace=True, errors='ignore')\n",
    "\n",
    "    # df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp_out, how= 'left', \n",
    "    #                              left_on = ['AMT_ID','Section'],\n",
    "    #                              right_on = ['AMT_ID','Section'])\n",
    "\n",
    "    # df_SHOPP_raw_data['Is Pavement limits repeating in the same project?'] = df_SHOPP_raw_data['Count of Repeated Unique_Pave_Limits'].apply(\n",
    "    #                                                                         lambda x: 'OK' if (pd.isnull(x) or x < 2 )\n",
    "    #     else 'Please review Pavement Worksheet. One or more pavement limits are repeating in this project')\n",
    "\n",
    "    # df_out = df_SHOPP_raw_data[df_SHOPP_raw_data['Is Pavement limits repeating in the same project?'] != 'OK'][['District','AMT_ID','Section','List of Repeated Unique_Pave_Limits','Count of Repeated Unique_Pave_Limits']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['Is Pavement limits repeating in the same project?'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###Unit Test#####\n",
    "\n",
    "AMT_ID = 20044\n",
    "AMT_ID= 22913\n",
    "\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section',\n",
    "                                                           'Is Pavement limits repeating in the same project?']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp[(temp['AMT_ID'] ==AMT_ID) & (temp['Section'] ==STU)]['Unique_Pave_Limits'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-joining",
   "metadata": {},
   "source": [
    "\n",
    "<a id='Repeated_Bridge_within_the_same_project'></a>\n",
    "\n",
    "## Repeated Bridge within the same project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-composer",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Include in the analysis (section in use only)?\n",
    "\n",
    "# cal = '''\n",
    "# IF [Section In Use]=[Section Bridge worksheet] Then\n",
    "# \"Yes\"\n",
    "# Else \"No\"\n",
    "# End\n",
    "\n",
    "# '''\n",
    "\n",
    "\n",
    "\n",
    "# # Is Bridge # repeating in the same project?\n",
    "# cal = '''\n",
    "# IF [Count Unique bridge same project]>1\n",
    "# Then \"Please review Bridge Worksheet. One or more bridges are repeating in this project\"\n",
    "# Else \"OK\"\n",
    "# END\n",
    "\n",
    "# '''\n",
    "\n",
    "\n",
    "# # Count Unique bridge same project\n",
    "\n",
    "\n",
    "# cal = '''\n",
    "\n",
    "# If [Bridge №]=\"NYA\" then 0 else\n",
    "# {Fixed [Section Bridge worksheet],[Section In Use],[Unique Bridge], [Date]:Count ([Unique Bridge])}\n",
    "# END\n",
    "# '''\n",
    "\n",
    "# # Unique Bridge\n",
    "\n",
    "# cal = '''\n",
    "# Str([ID bridge worksheet])+[Bridge №]+[Section Bridge worksheet]\n",
    "# '''\n",
    "\n",
    "ck_name = 'Is Bridge repeating in the same project?'\n",
    "temp = df_brg_raw_data[df_brg_raw_data['BridgeNo'] != 'NYA']\n",
    "\n",
    "temp_count = temp.groupby(['AMT_ID','Section','BridgeNo'])['Program'].count().reset_index(name = 'Count of BridgeNo')\n",
    "# temp_count.columns = ['AMT_ID','Section','BridgeNo','Count of Duplicated Bridge']\n",
    "temp_count_filtered = temp_count[temp_count['Count of BridgeNo'] > 1]\n",
    "\n",
    "\n",
    "if temp_count_filtered.empty:\n",
    "    df_SHOPP_raw_data[ck_name] = 'OK'\n",
    "\n",
    "else:\n",
    "    temp_count_filtered = temp_count_filtered.groupby(['AMT_ID', 'Section'])['BridgeNo'].agg(lambda x: ';'.join(x)).reset_index(name = 'repeated BirdgeNos')\n",
    "\n",
    "\n",
    "    df_SHOPP_raw_data.drop(columns =['repeated BirdgeNos'], inplace=True, errors='ignore')\n",
    "\n",
    "    df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp_count_filtered[['AMT_ID','Section','repeated BirdgeNos']], how= 'left', \n",
    "                                 left_on = ['AMT_ID', 'Section'], \n",
    "                                 right_on = ['AMT_ID','Section'])\n",
    "\n",
    "    def ck_repeated_bridge(df):\n",
    "        if pd.isnull(df['repeated BirdgeNos']):\n",
    "            return 'OK'\n",
    "        else:\n",
    "            return 'This project has repeating bridges ({}) in {} Section. AMT_ID: {}'.format(df['repeated BirdgeNos'], df['Section'], df['AMT_ID'])\n",
    "\n",
    "    df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_repeated_bridge, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['Is Bridge repeating in the same project?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Unit Test#####\n",
    "\n",
    "\n",
    "# AMT_ID = 21966\n",
    "AMT_ID = 16157\n",
    "AMT_ID = 18046\n",
    "AMT_ID = 20004\n",
    "\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "# temp[temp['AMT_ID'] == AMT_ID][['AMT_ID','Section','BridgeNo']]\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','repeated BirdgeNos',\n",
    "'Is Bridge repeating in the same project?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[(temp['AMT_ID'] ==AMT_ID) & (temp['Section'] ==STU)]['BridgeNo'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-travel",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id='Is_TMS_Asset_repeating_in_the_same_project'></a>\n",
    "\n",
    "## Repeated TMS within the same project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Is TMS Asset repeating in the same project?\n",
    "\n",
    "# cal = '''\n",
    "# IF [Count unique ID in a same project]>1\n",
    "# Then \"Please review TMS Worksheet. One or more TMS assets are repeating in this project\"\n",
    "# Else \"OK\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "\n",
    "# # Count unique ID in a same project\n",
    "# cal = '''\n",
    "# If [TMSUnique ID]=\"New\" then 0 else\n",
    "# {Fixed [TMS Section],[Section In Use],[TMS_same_project_unique ID], [Date]:Count ([TMS_same_project_unique ID])}\n",
    "# END\n",
    "# '''\n",
    "\n",
    "# #  TMS_same_project_unique ID\n",
    "\n",
    "# cal = '''Str([SHOPP ID])+[TMSUnique ID]'''\n",
    "\n",
    "ck_name = 'Is TMS Asset repeating in the same project?'\n",
    "\n",
    "\n",
    "#TODO need to create unique TMS ID including column name of \"TMS Structural or Technology\"\n",
    "\n",
    "\n",
    "temp = df_tms_raw_data[df_tms_raw_data.TMSID != 'New']\n",
    "\n",
    "temp_count = temp.groupby(['AMT_ID','Section','TMSID','TMS Structural or Technology'])['Program'].count().reset_index(name = 'Count of TMSID')\n",
    "# temp_count.columns = ['AMT_ID','Section','BridgeNo','Count of Duplicated Bridge']\n",
    "\n",
    "\n",
    "temp_count_filtered = temp_count[temp_count['Count of TMSID'] > 1]\n",
    "\n",
    "if temp_count_filtered.empty:\n",
    "    df_SHOPP_raw_data[ck_name] = 'OK'\n",
    "\n",
    "else:\n",
    "    temp_count_filtered = temp_count_filtered.groupby(['AMT_ID', 'Section'])['TMSID'].agg(lambda x: ';'.join(x)).reset_index(name = 'repeated TMSIDs')\n",
    "\n",
    "    df_SHOPP_raw_data.drop(columns =['repeated TMSIDs'], inplace=True, errors='ignore')\n",
    "\n",
    "    df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp_count_filtered[['AMT_ID','Section','repeated TMSIDs']], how= 'left', \n",
    "                                 left_on = ['AMT_ID', 'Section'], \n",
    "                                 right_on = ['AMT_ID', 'Section'])\n",
    "\n",
    "\n",
    "    def ck_repeated_TMS(df):\n",
    "        if pd.isnull(df['repeated TMSIDs']):\n",
    "            return 'OK'\n",
    "        else:\n",
    "            return 'For {} section in project {}: Please review TMS Worksheet for repeating TMS ({}) in {} Section.'.format(df['Section'],df['AMT_ID'],df['repeated TMSIDs'], df['Section'])\n",
    "\n",
    "    df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_repeated_TMS, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data[ck_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###Unit Test#####\n",
    "\n",
    "\n",
    "# AMT_ID = 21966\n",
    "AMT_ID = 16157\n",
    "AMT_ID = 18046\n",
    "# AMT_ID = 23023\n",
    "\n",
    "\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','repeated TMSIDs' ,\n",
    "ck_name]]\n",
    "\n",
    "# df_out = df_SHOPP_raw_data[df_SHOPP_raw_data[ck_name]!= 'OK'][['AMT_ID','Section','repeated Culverts']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[(temp['AMT_ID'] ==AMT_ID) & (temp['Section'] ==STU)]['TMSID'].value_counts().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-administration",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id='Repeated_Culvert_within_the_same_project'></a>\n",
    "\n",
    "## Repeated Culvert within the same project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# #Is Culvert repeating in the same project?\n",
    "\n",
    "# cal = '''\n",
    "# IF [Count unique culverts in same project]>1\n",
    "# Then \"Please review Drainage Worksheet. One or more culverts are repeating in this project\"\n",
    "# Else \"OK\"\n",
    "# END\n",
    "# '''\n",
    "\n",
    "# # Count unique culverts in same project\n",
    "# cal = '''\n",
    "# If[Activity Description]=\"New Culvert\" then 0 else\n",
    "# {Fixed [Section Drainage],[Section In Use],[Unique Culvert+ID], [Date]:Count ([Unique Culvert+ID])}\n",
    "# End\n",
    "# '''\n",
    "\n",
    "# # Unique Culvert+ID\n",
    "\n",
    "# cal ='''\n",
    "# str([Drainage ID])+[SYSNO]+[INETNO]+[OUTETNO]\n",
    "# '''\n",
    "\n",
    "ck_name = 'Is Culvert repeating in the same project?'\n",
    "\n",
    "temp = df_drain_raw_data[df_drain_raw_data['Activity Description'] !=\"New Culvert\"]\n",
    "\n",
    "temp_count = temp.groupby(['AMT_ID','Section','Unique Culvert ID'])['Program'].count().reset_index(name = 'Count of Unique Culvert ID')\n",
    "\n",
    "temp_count_filtered = temp_count[temp_count['Count of Unique Culvert ID'] > 1]\n",
    "\n",
    "\n",
    "if temp_count_filtered.empty:\n",
    "    df_SHOPP_raw_data[ck_name] = 'OK'\n",
    "\n",
    "else:\n",
    "    temp_count_filtered = temp_count_filtered.groupby(['AMT_ID', 'Section'])['Unique Culvert ID'].agg(lambda x: ';'.join(x)).reset_index(name = 'repeated Culverts')\n",
    "\n",
    "    df_SHOPP_raw_data.drop(columns =['repeated Culverts'], inplace=True, errors='ignore')\n",
    "\n",
    "    df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp_count_filtered[['AMT_ID','Section','repeated Culverts']], how= 'left', \n",
    "                                 left_on = ['AMT_ID', 'Section'], \n",
    "                                 right_on = ['AMT_ID', 'Section'])\n",
    "\n",
    "    def ck_repeated_culvert(df):\n",
    "        if pd.isnull(df['repeated Culverts']):\n",
    "            return 'OK'\n",
    "        else:\n",
    "            return 'This project has repeating culverts ({}) in the {} Section. AMT_ID: {}'.format(df['repeated Culverts'], df['Section'], df['AMT_ID']) \n",
    "\n",
    "    df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_repeated_culvert, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data[ck_name].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###Unit Test#####\n",
    "\n",
    "\n",
    "AMT_ID = 21966\n",
    "AMT_ID = 22780\n",
    "\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','repeated Culverts' ,\n",
    "ck_name]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-antibody",
   "metadata": {},
   "source": [
    "## Check duplicate asset within project book and flag only programmed projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-description",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_SHOPP_raw_data_filtered = df_SHOPP_raw_data[\n",
    "    (df_SHOPP_raw_data['Will this project be included in the Project Book?']=='Yes')\n",
    "    ][['AMT_ID', 'Section','District','EA' ,'Nickname','Advertised Year','Last Year of Fiscal Year','Planning or Post-Planning','Project Cost ($K)','Type of Exception']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-wrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_AMT_ID_Section(df):\n",
    "    return 'Section ' + df['Section'] + ' of ' + str(df['AMT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filtered_comments(df, comment_name, exception):\n",
    "    if pd.isnull(df[comment_name]):\n",
    "        return 'OK'\n",
    "    else:\n",
    "        if df['Planning or Post-Planning'] != 'Planning': #only flag the project that is in planning\n",
    "            return 'OK'\n",
    "        elif exception in df['Type of Exception']: # if it is in exception, do not flag\n",
    "            return 'OK'\n",
    "        else:\n",
    "            return df[comment_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is the bridge repeated in the Project Book?\n",
    "\n",
    "# cal ='''\n",
    "# If isnull([Bridge #]) then \"OK\"\n",
    "# ElseIf [Will this project be included in the Project Book?]=\"Yes\" and  [Include in Performance?]=\"Yes\" and [Section to Use]=[Section Bridge worksheet] then\n",
    "#   IF {Fixed Date, [Will this project be included in the Project Book?], [Bridge #],[Include in Performance?]: COUNTD([ID bridge worksheet])}>1\n",
    "# Then \"This bridge is repeating in your Ten-Year Plan. Please review\"\n",
    "# Else \"OK\"\n",
    "# End\n",
    "# Else \"OK\"\n",
    "# End\n",
    "# '''\n",
    "\n",
    "\n",
    "# get a list of duplicate Bridges with a list of projects  \n",
    "\n",
    "ck_name = 'Is a bridge in a planned project repeated in another project in the Project Book?'\n",
    "comment_col_name = 'Comment for Duplicated Bridge'\n",
    "\n",
    "\n",
    "temp = pd.merge(df_brg_raw_data[['AMT_ID', 'Section','BridgeNo']], df_SHOPP_raw_data_filtered, how='inner', \n",
    "                left_on = ['AMT_ID', 'Section'],\n",
    "                right_on = ['AMT_ID', 'Section'],)\n",
    "\n",
    "# temp_filtered = temp[(temp['Type of Exception'] != 'Repeated Bridge') & (temp['BridgeNo'] != 'NYA')]\n",
    "temp_filtered = temp[(temp['BridgeNo'] != 'NYA')]\n",
    "\n",
    "unique_bridge_group = temp_filtered.groupby(['BridgeNo'])['AMT_ID'].nunique().reset_index(\n",
    "    name = 'Count of AMT_IDs for Same BridgeNo')\n",
    "\n",
    "duplicate_bridge = unique_bridge_group[unique_bridge_group['Count of AMT_IDs for Same BridgeNo']> 1]\n",
    "\n",
    "\n",
    "if duplicate_bridge.empty:\n",
    "    df_SHOPP_raw_data[ck_name] = 'OK'\n",
    "\n",
    "else:\n",
    "    duplicate_bridge_out = pd.merge(duplicate_bridge, temp_filtered, how='left', \n",
    "                    left_on = ['BridgeNo'],\n",
    "                    right_on = ['BridgeNo'],)\n",
    "\n",
    "    duplicate_bridge_out['AMT_ID+Section'] = duplicate_bridge_out.apply(combined_AMT_ID_Section, axis = 1)\n",
    "\n",
    "\n",
    "    temp = duplicate_bridge_out.groupby(['BridgeNo'])['AMT_ID+Section'].apply(','.join).reset_index()\n",
    "\n",
    "    temp[comment_col_name] = temp.apply(lambda df: 'Bridge ({}) is repeated in: {}'.format(df['BridgeNo'], df['AMT_ID+Section'] ), axis = 1)\n",
    "\n",
    "    duplicate_bridge_2 = pd.merge(temp, temp_filtered, how='left', \n",
    "                    left_on = ['BridgeNo'],\n",
    "                    right_on = ['BridgeNo'],)\n",
    "\n",
    "\n",
    "    #cumulate duplicate asset table\n",
    "    duplicate_bridge_3  = duplicate_bridge_2[['BridgeNo','AMT_ID',\n",
    "           'Section', 'District', 'EA','Last Year of Fiscal Year', 'Type of Exception']]\n",
    "\n",
    "    dict_rename = {\n",
    "    'BridgeNo' : 'Asset_ID'\n",
    "                          }\n",
    "    duplicate_bridge_3= duplicate_bridge_3.rename(dict_rename, axis = 1)\n",
    "    \n",
    "    duplicate_bridge_3['Asset_Type'] = 'Bridge'\n",
    "    df_duplicated_asset = duplicate_bridge_3\n",
    "    \n",
    "\n",
    "    \n",
    "    duplicate_bridge_out= duplicate_bridge_2.groupby(['AMT_ID', 'Section'])[comment_col_name].apply(';'.join).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "    df_SHOPP_raw_data.drop(columns=[comment_col_name],inplace=True , errors='ignore')\n",
    "\n",
    "    df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, duplicate_bridge_out, how = 'left', \n",
    "                      left_on = ['AMT_ID','Section'], \n",
    "                      right_on = ['AMT_ID','Section'])\n",
    "\n",
    "\n",
    "    df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(lambda df: 'OK' if pd.isnull(df[comment_col_name]) else '{} Please review and reconcile with the HQ Program.'.format(df[comment_col_name]), axis = 1)\n",
    "    \n",
    "    df_SHOPP_raw_data[ck_name] =  df_SHOPP_raw_data.apply(create_filtered_comments, args=(comment_col_name,'Repeated Bridge'), axis = 1) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-farming",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "AMT_ID = 23072\n",
    "# AMT_ID = 23073\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section',ck_name, comment_col_name, 'Type of Exception']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_out = duplicate_bridge_out\n",
    "# filename = 'duplicate_bridges_all'\n",
    "\n",
    "# df_out.to_csv('.\\output\\{}.csv'.format(filename), index= False)\n",
    "# shutil.copy('.\\output\\{}.csv'.format(filename), 'C:\\inetpub\\wwwroot\\DataLake\\ProjectBookCheck\\{}.csv'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is the Culvert repeated in the Project Book?\n",
    "# cal = '''\n",
    "# If isnull([SYSNO]) or [ActID Drainage]=\"C13\" then \"OK\"\n",
    "# ElseIf [Will this project be included in the Project Book?]=\"Yes\" and  [Include in Performance?]=\"Yes\" and [Section to Use]=[Section Drainage] then\n",
    "#   IF {Fixed Date, [Will this project be included in the Project Book?], [Unique Culvert ID],[Include in Performance?]: COUNTD([Drainage ID])}>1\n",
    "# Then \"This Culvert element is repeating in your Ten-Year Plan. Please review\"\n",
    "# Else \"OK\"\n",
    "# End\n",
    "# Else \"OK\"\n",
    "# End\n",
    "# '''\n",
    "\n",
    "\n",
    "# get a list of duplicate culverts with a list of projects  \n",
    "\n",
    "ck_name = 'Is a culvert in a planned project repeated in another project in the Project Book?'\n",
    "comment_col_name = 'Comment for Duplicated Culvert'\n",
    "\n",
    "temp = pd.merge(df_drain_raw_data[['AMT_ID', 'Section','SYSNO','INETNO','OUTETNO','Activity Description','Unique Culvert ID','ActID']], df_SHOPP_raw_data_filtered, how='inner', \n",
    "                left_on = ['AMT_ID', 'Section'],\n",
    "                right_on = ['AMT_ID', 'Section'],)\n",
    "\n",
    "\n",
    "temp_filtered = temp[(temp['ActID'] != 'C13') & (~temp['SYSNO'].isna())]\n",
    "\n",
    "\n",
    "unique_culvert_group = temp_filtered.groupby(['Unique Culvert ID'])['AMT_ID'].nunique().reset_index(\n",
    "    name = 'Count of AMT_IDs for Same Culvert ID')\n",
    "\n",
    "# unique_bridge_group['Count of AMT_IDs for Same BridgeNo'] = unique_bridge_group['AMT_ID list for same BridgeNo'].apply(lambda x: len(x))\n",
    "\n",
    "duplicate_culvert= unique_culvert_group [unique_culvert_group ['Count of AMT_IDs for Same Culvert ID']> 1]\n",
    "\n",
    "if duplicate_culvert.empty:\n",
    "    df_SHOPP_raw_data[ck_name] = 'OK'\n",
    "\n",
    "else:\n",
    "\n",
    "    duplicate_culvert_out = pd.merge(duplicate_culvert, temp_filtered, how='left', \n",
    "                    left_on = ['Unique Culvert ID'],\n",
    "                    right_on = ['Unique Culvert ID'],)\n",
    "\n",
    "    duplicate_culvert_out['AMT_ID+Section'] = duplicate_culvert_out.apply(combined_AMT_ID_Section, axis = 1)\n",
    "\n",
    "    temp_1 = duplicate_culvert_out.groupby(['Unique Culvert ID'])['AMT_ID+Section'].apply(','.join).reset_index()\n",
    "\n",
    "    temp_1[comment_col_name] = temp_1.apply(lambda df: 'Culvert ({}) is repeated in: {}'.format(df['Unique Culvert ID'], df['AMT_ID+Section'] ), axis = 1)\n",
    "\n",
    "    temp_2 = pd.merge(temp_1, temp_filtered, how='left', \n",
    "                    left_on = ['Unique Culvert ID'],\n",
    "                    right_on = ['Unique Culvert ID'],)\n",
    "\n",
    "    #cumulate duplicate asset table\n",
    "    temp_3  = temp_2[['Unique Culvert ID','AMT_ID',\n",
    "           'Section', 'District', 'EA','Last Year of Fiscal Year', 'Type of Exception']]\n",
    "\n",
    "    dict_rename = {\n",
    "    'Unique Culvert ID' : 'Asset_ID'\n",
    "                          }\n",
    "    temp_3= temp_3.rename(dict_rename, axis = 1)\n",
    "    \n",
    "    temp_3['Asset_Type'] = 'Culvert'\n",
    "    df_duplicated_asset =  pd.concat([df_duplicated_asset,  temp_3], axis = 0)\n",
    "    \n",
    "\n",
    "    temp_out= temp_2.groupby(['AMT_ID', 'Section'])[comment_col_name].apply(';'.join).reset_index()\n",
    "\n",
    "    df_SHOPP_raw_data.drop(columns=[comment_col_name],inplace=True , errors='ignore')\n",
    "\n",
    "    df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp_out, how = 'left', \n",
    "                      left_on = ['AMT_ID','Section'], \n",
    "                      right_on = ['AMT_ID','Section'])\n",
    "\n",
    "    df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(lambda df: 'OK' if pd.isnull(df[comment_col_name]) else '{} Please review and reconcile with the HQ Program.'.format(df[comment_col_name]), axis = 1)\n",
    "    df_SHOPP_raw_data[ck_name] =  df_SHOPP_raw_data.apply(create_filtered_comments, args=(comment_col_name,'Repeated Culvert'), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###Unit Test#####\n",
    "\n",
    "AMT_ID = 21966\n",
    "AMT_ID = 23072\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section',ck_name,]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-omega",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-burner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-daughter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-hospital",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-healthcare",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stable-burlington",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_tms_raw_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3b83a9520f4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mcomment_col_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Comment for Duplicated TMS'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m temp = pd.merge(df_tms_raw_data[['TMSID','AMT_ID', 'Section','TMS Structural or Technology']], df_SHOPP_raw_data_filtered, how='inner', \n\u001b[0m\u001b[0;32m     19\u001b[0m                 \u001b[0mleft_on\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'AMT_ID'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Section'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 right_on = ['AMT_ID', 'Section'],)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_tms_raw_data' is not defined"
     ]
    }
   ],
   "source": [
    "#Is the TMS repeated in the Project Book?\n",
    "\n",
    "# cal ='''\n",
    "# If isnull([TMSUnique ID]) or [TMSUnique ID]=\"New\" then \"OK\"\n",
    "# ElseIf [Will this project be included in the Project Book?]=\"Yes\" and [Section to Use]=[TMS Section] then\n",
    "#   IF {Fixed Date, [Will this project be included in the Project Book?], [TMSUnique ID], [Include in Performance?]: COUNTD([SHOPP ID])}>1\n",
    "# Then \"This TMS element is repeating in your Ten-Year Plan. Please review\"\n",
    "# Else \"OK\"\n",
    "# End\n",
    "# Else \"OK\"\n",
    "# End\n",
    "# '''\n",
    "\n",
    "# get a list of duplicate TMS with a list of projects  \n",
    "ck_name = 'Is a TMS in a planned project repeated in another project in the Project Book?'\n",
    "comment_col_name = 'Comment for Duplicated TMS'\n",
    "\n",
    "temp = pd.merge(df_tms_raw_data[['TMSID','AMT_ID', 'Section','TMS Structural or Technology']], df_SHOPP_raw_data_filtered, how='inner', \n",
    "                left_on = ['AMT_ID', 'Section'],\n",
    "                right_on = ['AMT_ID', 'Section'],)\n",
    "\n",
    "\n",
    "temp_filtered = temp[(temp['TMSID'] != 'New') & (~temp['TMSID'].isna())]\n",
    "\n",
    "unique_TMS_group = temp_filtered.groupby(['TMSID','TMS Structural or Technology'])['AMT_ID'].nunique().reset_index(\n",
    "    name = 'Count of AMT_IDs for Same TMS ID')\n",
    "\n",
    "# unique_bridge_group['Count of AMT_IDs for Same BridgeNo'] = unique_bridge_group['AMT_ID list for same BridgeNo'].apply(lambda x: len(x))\n",
    "\n",
    "duplicate_TMS= unique_TMS_group[unique_TMS_group['Count of AMT_IDs for Same TMS ID']> 1]\n",
    "\n",
    "#cumulate duplicate asset table\n",
    "# duplicate_culvert['Asset_Type'] = 'TMS'\n",
    "# df_duplicated_asset =  pd.concat([df_duplicated_asset, duplicate_culvert], axis = 0)\n",
    "# df_duplicated_asset =  pd.concat([df_duplicated_asset, duplicated_TMS[]], axis = 0)\n",
    "\n",
    "\n",
    "if duplicate_TMS.empty:\n",
    "    df_SHOPP_raw_data[ck_name] = 'OK'\n",
    "\n",
    "else:\n",
    "    duplicate_TMS_out = pd.merge(duplicate_TMS, temp_filtered, how='left', \n",
    "                    left_on = ['TMSID'],\n",
    "                    right_on = ['TMSID'],)\n",
    "\n",
    "    duplicate_TMS_out['AMT_ID+Section'] = duplicate_TMS_out.apply(combined_AMT_ID_Section, axis = 1)\n",
    "\n",
    "    temp_1 = duplicate_TMS_out.groupby(['TMSID'])['AMT_ID+Section'].apply(','.join).reset_index()\n",
    "\n",
    "    temp_1[comment_col_name] = temp_1.apply(lambda df: 'TMS ({}) is repeated in: {}'.format(df['TMSID'], df['AMT_ID+Section'] ), axis = 1)\n",
    "\n",
    "    temp_2 = pd.merge(temp_1, temp_filtered, how='left', \n",
    "                    left_on = ['TMSID'],\n",
    "                    right_on = ['TMSID'],)\n",
    "\n",
    "    #cumulate duplicate asset table\n",
    "    temp_3  = temp_2[['TMSID','AMT_ID',\n",
    "           'Section', 'District', 'EA','Last Year of Fiscal Year', 'Type of Exception']]\n",
    "\n",
    "    dict_rename = {\n",
    "    'TMSID' : 'Asset_ID'\n",
    "                          }\n",
    "    temp_3= temp_3.rename(dict_rename, axis = 1)\n",
    "    \n",
    "    temp_3['Asset_Type'] = 'TMS'\n",
    "    df_duplicated_asset =  pd.concat([df_duplicated_asset,  temp_3], axis = 0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    temp_out= temp_2.groupby(['AMT_ID', 'Section'])[comment_col_name].apply(';'.join).reset_index()\n",
    "\n",
    "\n",
    "    df_SHOPP_raw_data.drop(columns=[comment_col_name],inplace=True , errors='ignore')\n",
    "\n",
    "    df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp_out, how = 'left', \n",
    "                      left_on = ['AMT_ID','Section'], \n",
    "                      right_on = ['AMT_ID','Section'])\n",
    "\n",
    "\n",
    "\n",
    "    df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(lambda df: 'OK' if pd.isnull(df[comment_col_name]) else '{} Please review and reconcile with the HQ Program. AMT_ID: {}'.format(df[comment_col_name], df['AMT_ID']), axis = 1)\n",
    "    df_SHOPP_raw_data[ck_name] =  df_SHOPP_raw_data.apply(create_filtered_comments, args=(comment_col_name,'Repeated TMS'), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###Unit Test#####\n",
    "\n",
    "\n",
    "AMT_ID = 21966\n",
    "AMT_ID = 18046\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section',ck_name ,]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-string",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-draft",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is the Pavement Segment repeated in the Project Book?\n",
    "ck_name = 'Is a pavement segment in a planned project repeated in another project in the Project Book?'\n",
    "comment_col_name = 'Comment for Duplicated Pavement'\n",
    "\n",
    "temp = pd.merge(df_pav_raw_data[['Unique_Pave_Limits','AMT_ID', 'Section']], df_SHOPP_raw_data_filtered, how='inner', \n",
    "                left_on = ['AMT_ID', 'Section'],\n",
    "                right_on = ['AMT_ID', 'Section'],)\n",
    "\n",
    "temp_filtered = temp[(~temp['Unique_Pave_Limits'].isna())]\n",
    "\n",
    "unique_pavement_group = temp_filtered.groupby(['Unique_Pave_Limits'])['AMT_ID'].nunique().reset_index(\n",
    "    name = 'Count of AMT_IDs for Same Pavement Segment')\n",
    "\n",
    "# unique_bridge_group['Count of AMT_IDs for Same BridgeNo'] = unique_bridge_group['AMT_ID list for same BridgeNo'].apply(lambda x: len(x))\n",
    "\n",
    "duplicate_pavement= unique_pavement_group[unique_pavement_group['Count of AMT_IDs for Same Pavement Segment']> 1]\n",
    "\n",
    "if duplicate_pavement.empty:\n",
    "    df_SHOPP_raw_data[ck_name] = 'OK'\n",
    "\n",
    "else:\n",
    "\n",
    "    duplicate_pavement_out = pd.merge(duplicate_pavement, temp_filtered, how='left', \n",
    "                    left_on = ['Unique_Pave_Limits'],\n",
    "                    right_on = ['Unique_Pave_Limits'],)\n",
    "\n",
    "    duplicate_pavement_out['AMT_ID+Section'] = duplicate_pavement_out.apply(combined_AMT_ID_Section, axis = 1)\n",
    "\n",
    "    temp_1 = duplicate_pavement_out.groupby(['Unique_Pave_Limits'])['AMT_ID+Section'].apply(','.join).reset_index()\n",
    "\n",
    "    temp_1[comment_col_name] = temp_1.apply(lambda df: 'Pavement Segment ({}) is repeated in: {}'.format(df['Unique_Pave_Limits'], df['AMT_ID+Section'] ), axis = 1)\n",
    "\n",
    "    temp_2 = pd.merge(temp_1, temp_filtered, how='left', \n",
    "                    left_on = ['Unique_Pave_Limits'],\n",
    "                    right_on = ['Unique_Pave_Limits'],)\n",
    "    \n",
    "    #cumulate duplicate asset table\n",
    "    temp_3  = temp_2[['Unique_Pave_Limits','AMT_ID',\n",
    "           'Section', 'District', 'EA','Last Year of Fiscal Year', 'Type of Exception']]\n",
    "\n",
    "    dict_rename = {\n",
    "    'Unique_Pave_Limits' : 'Asset_ID'\n",
    "                          }\n",
    "    temp_3= temp_3.rename(dict_rename, axis = 1)\n",
    "    \n",
    "    temp_3['Asset_Type'] = 'Pavement'\n",
    "    df_duplicated_asset =  pd.concat([df_duplicated_asset,  temp_3], axis = 0)\n",
    "    \n",
    "    \n",
    "\n",
    "    temp_out= temp_2.groupby(['AMT_ID', 'Section'])[comment_col_name].apply(';'.join).reset_index()\n",
    "\n",
    "\n",
    "    df_SHOPP_raw_data.drop(columns=[comment_col_name],inplace=True , errors='ignore')\n",
    "\n",
    "    df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp_out, how = 'left', \n",
    "                      left_on = ['AMT_ID','Section'], \n",
    "                      right_on = ['AMT_ID','Section'])\n",
    "\n",
    "    \n",
    "    df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(lambda df: 'OK' if pd.isnull(df[comment_col_name]) else 'For {} section in project {}: {}'.format(df['Section'],df['AMT_ID'], df[comment_col_name]), axis = 1)\n",
    "    \n",
    "    df_SHOPP_raw_data[ck_name] =  df_SHOPP_raw_data.apply(create_filtered_comments, args=(comment_col_name,'Repeated Pavement'), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['AMT_ID'].value_counts().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ###Unit Test#####\n",
    "# ck_name = 'Is a pavement segment in a planned project repeated in another project in the Project Book?'\n",
    "# comment_col_name = 'Comment for Duplicated Pavement'\n",
    "# AMT_ID = 22983\n",
    "# # AMT_ID = 20476\n",
    "\n",
    "\n",
    "# STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section',\n",
    "#                                                         ck_name\n",
    "#                                                         ]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-clone",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Is the Pavement Worksheet using updated inventory and condition data (2019 APCS) for planned projects?'\n",
    "\n",
    "\n",
    "def ck_Pav_APCS(df):\n",
    "    if (df['Planning or Post-Planning']== \"Post-Planning\" \n",
    "        or df['SHOPP Amendment Date'] == DD_Approval_Placeholder_Date\n",
    "        or df['Ten-Year Plan RD'] == 9999 \n",
    "        or pd.isnull(df['Pavement_PlanYear'])\n",
    "        or df['Last Year of Fiscal Year'] > TARGET_FY + 10 # last Year of FY greater than 2031 for SHSMP 2021\n",
    "        ): \n",
    "        return 'OK'\n",
    "    else: \n",
    "        if df['count_PCRScenarioNo'] != 1:\n",
    "            return 'Only one pavement APCR scenario is allowed per project and it needs to match the 2019 APCS data (APCR scenario #3299). Please update the pavement worksheet. AMT_ID: {}'.format(df['AMT_ID'])\n",
    "        elif df['first_PCRScenarioNo'] == '3299':  \n",
    "            #only allow one scenario per project and current valid scenario is 3299\n",
    "            return 'OK'\n",
    "        else: \n",
    "            return 'Please update the pavement worksheet to use 2019 APCS data (APCR scenario #3299). AMT_ID: {}'.format(df['AMT_ID'])\n",
    "    \n",
    "ck_name = 'Is the Pavement Worksheet using updated inventory and condition data (2019 APCS) for planned projects?'\n",
    "\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_Pav_APCS, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Unit Test\n",
    "\n",
    "\n",
    "AMT_ID = 21966\n",
    "AMT_ID = 13668\n",
    "\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','Planning or Post-Planning' ,\n",
    "'Ten-Year Plan RD' ,\n",
    "'first_PCRScenarioNo' ,\n",
    "'Pavement_PlanYear' ,\n",
    "'Is the Pavement Worksheet using updated inventory and condition data (2019 APCS) for planned projects?']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-quantum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-sheet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Is the Drainage Worksheet using updated inventory and condition data (Sept 2021 or later) for planned projects?'\n",
    "\n",
    "#Attention: update this check every time we change SHSMP plan \n",
    "\n",
    "def ck_drain_data(df):\n",
    "    if (df['Planning or Post-Planning']== \"Post-Planning\" \n",
    "        or df['SHOPP Amendment Date'] == DD_Approval_Placeholder_Date\n",
    "        or  df['Ten-Year Plan RD'] == 9999 \n",
    "        or df['No of Drainage Entries'] == 0\n",
    "        or df['Last Year of Fiscal Year'] > TARGET_FY + 10): # last Year of FY greater than 2031 for SHSMP 2021 \n",
    "        return 'OK'\n",
    "    else: \n",
    "        if datetime.strptime(df['Data Date_Drainage'], \"%m-%d-%Y\")  > datetime.strptime('09-01-2021', \"%m-%d-%Y\"):  \n",
    "            return 'OK'\n",
    "        else: \n",
    "            return 'Please update the drainage worksheet to use the current CIP data (09-01-2021 or later). AMT_ID: {}'.format(df['AMT_ID'])\n",
    "\n",
    "ck_name = 'Is the Drainage Worksheet using updated inventory and condition data (Sept 2021 or later) for planned projects?'\n",
    "\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_drain_data, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Unit Test#####\n",
    "\n",
    "\n",
    "AMT_IDs = [13827]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'].isin(AMT_IDs)][['AMT_ID','Section','Planning or Post-Planning','SHOPP Amendment Date','Ten-Year Plan RD','No of Drainage Entries','Data Date_Drainage',\n",
    "'Is the Drainage Worksheet using updated inventory and condition data (Sept 2021 or later) for planned projects?'\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-cover",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Is the TMS Worksheet using updated inventory and condition data (June 2021 or later) for planned projects?'\n",
    "\n",
    "#Attention: update this check every time we change SHSMP plan \n",
    "\n",
    "def ck_TMS_data(df):\n",
    "    if (df['Planning or Post-Planning']== \"Post-Planning\" \n",
    "        or df['SHOPP Amendment Date'] == DD_Approval_Placeholder_Date\n",
    "        or  df['Ten-Year Plan RD'] == 9999 \n",
    "        or pd.isnull(df['TMS_PlanYear'])\n",
    "         or df['Last Year of Fiscal Year'] > TARGET_FY + 10 # last Year of FY greater than 2031 for SHSMP 2021 \n",
    "       ): \n",
    "        return 'OK'\n",
    "    else: \n",
    "        if datetime.strptime(df['Data Date_TMS'], \"%m-%d-%Y\")  > datetime.strptime('06-01-2021', \"%m-%d-%Y\"):  \n",
    "            return 'OK'\n",
    "        else: \n",
    "            return 'Please update the TMS worksheet to use the current TMS inventory and condition data (06-01-2021 or later). AMT_ID: {}'.format(df['AMT_ID'])\n",
    "\n",
    "        \n",
    "ck_name = 'Is the TMS Worksheet using updated inventory and condition data (June 2021 or later) for planned projects?'\n",
    "\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_TMS_data, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepting-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unit Test\n",
    "\n",
    "\n",
    "# AMT_ID = 14058\n",
    "\n",
    "# STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','Data Date_TMS' ,\n",
    "# 'Planning or Post-Planning' ,\n",
    "# 'Ten-Year Plan RD' ,\n",
    "# 'TMS_PlanYear'\n",
    "# ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-stephen",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ck_project_in_PID_WP(df):\n",
    "    if (df['Planning or Post-Planning']== \"Post-Planning\" \n",
    "        or  df['Ten-Year Plan RD'] == 9999 \n",
    "        or df['Activity (group)'] == \"Reservation\"): \n",
    "        return 'OK'\n",
    "    elif df['Long Lead'] != 'Y':# it is not a long lead\n",
    "        if (df['Last Year of Fiscal Year'] in [TARGET_FY + 6, TARGET_FY + 7] \n",
    "            and pd.isnull(df['PID Status'])):\n",
    "            return 'This project is in year 6 or 7 and must be added to the PID workplan.  Please work with the PID Unit to resolve. AMT_ID: {}'.format(df['AMT_ID'])\n",
    "        else: \n",
    "            return 'OK'\n",
    "    else: # it is a long lead\n",
    "        if df['PA&ED FY Number'] == TARGET_FY + 4 and pd.isnull(df['PID Status']):\n",
    "            return 'This long lead project has PA&ED allocation in year 4 and must be in the PID workplan. Please work with the PID Unit to resolve. AMT_ID: {}'.format(df['AMT_ID'])\n",
    "        else: \n",
    "            return 'OK'\n",
    "        \n",
    "        \n",
    "ck_name = 'Is this project in the Project Book but not in the PID Workplan?'\n",
    "\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_project_in_PID_WP, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Unit Test#####\n",
    "\n",
    "\n",
    "AMT_ID = 20830\n",
    "\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','Data Date_TMS' ,\n",
    "'Planning or Post-Planning' ,\n",
    "'Ten-Year Plan RD' ,\n",
    "'TMS_PlanYear'\n",
    "]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-authorization",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ck_project_in_PID_WP_2(df):\n",
    "    if pd.isnull(df['PID Status']):\n",
    "        #it is not in the PID workplan, do not check\n",
    "        return 'OK'\n",
    "    elif df['Will this project be included in the Project Book?'] == 'No':\n",
    "        #it is in the PID workplan, but it is not in the project book, flag right away\n",
    "        return 'This project is in the PID workplan and must be added to year 6 or 7 in the Project Book. AMT_ID: {}'.format(df['AMT_ID'])\n",
    "    else:\n",
    "\n",
    "        #it is in the PID workplan and the project book\n",
    "        if (df['Planning or Post-Planning']== \"Post-Planning\"   #it is already programmed, not need to check further\n",
    "            or df['Activity (group)'] == \"Reservation\"  #it is a reservation project, it can be any fiscal year\n",
    "            or df['Last Year of Fiscal Year'] in [TARGET_FY + 6, TARGET_FY + 7] ## for non-reservation project, it has to be within 6/7 fiscal year of the 10 year plan\n",
    "            or (df['PA&ED FY Number'] == TARGET_FY + 4)): #if it is a long lead, the PA&ED FY needs to be the 4th year of the 10 year plan\n",
    "            return 'OK'\n",
    "        else: # \n",
    "            return 'This project is in the PID workplan and it is not in year 6 or 7. Please work with PID group to resolve this conflict. AMT_ID: {}'.format(df['AMT_ID'])\n",
    "        \n",
    "        \n",
    "ck_name = 'Is this project in the PID Workplan but not in the Project Book?'\n",
    "\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_project_in_PID_WP_2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Unit Test#####\n",
    "\n",
    "\n",
    "AMT_ID = 20830\n",
    "AMT_ID = 22887\n",
    "AMT_ID = 22809\n",
    "\n",
    "\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','PID Status' ,'Last Year of Fiscal Year' ,\n",
    "'Planning or Post-Planning' ,'PA&ED FY Number' ,'Will this project be included in the Project Book?' ,\n",
    "ck_name]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-black",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-classic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ck_ped_bike_accessible(df):\n",
    "    if (df['Planning or Post-Planning']== \"Post-Planning\" \n",
    "        or  df['Ten-Year Plan RD']  == 9999 \n",
    "        or df['ck_ActID_H32'] == 'OK'\n",
    "       ):  \n",
    "        return 'OK'\n",
    "    else: \n",
    "        return 'Please add activity H32 (Is any Location Within the Project Limits Ped/Bike Accessible?) to the Performance Tab for the primary location with appropriate response. AMT_ID: {}'.format(df['AMT_ID'])\n",
    "        \n",
    "ck_name = 'For all planned projects, does the Performance Tab indicate If any location within the project limits Ped/Bike accessible?'\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_ped_bike_accessible, axis = 1)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-level",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Does the CCA date in the AM Tool match PRSM?'\n",
    "\n",
    "def ck_CCA_date(df):\n",
    "    if (df['Section'] == \"CCA\") and (df['CCA Date Miilestone (M600)'] != df['Cca Finish Date']):\n",
    "        return 'For {} section in project {}: The CCA date does not match PRSM data. Please correct in AMTool or PRSM.'.format(df['Section'],df['AMT_ID'],)\n",
    "    else:\n",
    "        return 'OK'\n",
    "\n",
    "ck_name = 'Does the CCA date in the AM Tool match PRSM?'\n",
    "\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_CCA_date, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-terrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Unit Test#####\n",
    "\n",
    "AMT_ID = 20830\n",
    "AMT_ID = 9182\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','Cca Finish Date' ,\n",
    "'CCA Date Miilestone (M600)','Does the CCA date in the AM Tool match PRSM?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-pacific",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['Cca Percent Comp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Does the CCA section including performance need to be completed?  (Applies to projects with CCA date on or after July 1, 2020)'\n",
    "\n",
    "def ck_CCA_complete(df):\n",
    "    if pd.isnull(df['Cca Finish Date']):\n",
    "        return 'OK'\n",
    "    \n",
    "    if df['Cca Percent Comp'] == 100:\n",
    "        if (datetime.strptime(df['Cca Finish Date'], \"%m-%d-%Y\")  > datetime.strptime('06-30-2020' , \"%m-%d-%Y\")\n",
    "            and datetime.strptime(df['Cca Finish Date'], \"%m-%d-%Y\") < datetime.strptime(TARGETDATE, \"%m-%d-%Y\")\n",
    "           ):\n",
    "            if df['Section'] == \"CCA\" and df['perf_entry_count']>0:\n",
    "                return 'OK'\n",
    "            else: \n",
    "                return 'Please complete the CCA band or the Performance Tab or both. AMT_ID: {}'.format(df['AMT_ID'])\n",
    "        else:\n",
    "            return 'OK'\n",
    "    elif df['Section'] == \"CCA\":\n",
    "        return 'Please remove CCA Date. According to PRSM CCA is not Complete. AMT_ID: {}'.format(df['AMT_ID'])\n",
    "    else:\n",
    "        return 'OK'\n",
    "\n",
    "    \n",
    "ck_name = 'Does the CCA section including performance need to be completed?  (Applies to projects with CCA date on or after July 1, 2020)'\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_CCA_complete, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ###Unit Test#####\n",
    "\n",
    "\n",
    "# AMT_ID = 20830\n",
    "# # AMT_ID = 17266\n",
    "\n",
    "# AMT_ID = 13529\n",
    "\n",
    "# STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','Cca Finish Date' ,'perf_entry_count',\n",
    "#                                                          ck_name\n",
    "# ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-tattoo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-intelligence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Does the primary asset performance match CTIPS?  (Applies to projects with RTL 25/26 and later)'\n",
    "\n",
    "ck_name = 'Does the primary asset performance match CTIPS?  (Applies to projects with RTL 25/26 and later)'\n",
    "\n",
    "df_SHOPP_raw_data[ck_name] = 'OK'   #df_SHOPP_raw_data.apply(ck_add_PPC_amendment_date, axis = 1) \n",
    "# TODO discuss this with mara later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data.shape\n",
    "\n",
    "df_SHOPP_raw_data_copy = df_SHOPP_raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrative-publicity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data = df_SHOPP_raw_data_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-advantage",
   "metadata": {},
   "source": [
    "<a id='InteralChecks'></a>\n",
    "## Internal Checks\n",
    "* Does amendment date needs to be removed?\n",
    "* Need to add Resource in the PID workplan?\n",
    "* PRG section needs amendment date?\n",
    "* PPC section needs amendment date?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-cruise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does amendment date needs to be removed?\n",
    "\n",
    "def ck_remove_PRG_amendment_date(df):\n",
    "    if pd.notnull(df['Candidate Type']): #it is on the candidate list, skip check\n",
    "        return 'OK'\n",
    "    if pd.isnull(df['EFIS_Program']) and pd.notnull(df['SHOPP Amendment Date']):\n",
    "        return 'For project {}: Please remove the SHOPP amendment date in PRG section'.format(df['AMT_ID'],)\n",
    "    else:\n",
    "        return 'OK'\n",
    "    \n",
    "ck_name = 'Does amendment date needs to be removed?'\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_remove_PRG_amendment_date, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ###Unit Test#####\n",
    "\n",
    "\n",
    "# AMT_ID = 20830\n",
    "# AMT_ID = 16439\n",
    "\n",
    "\n",
    "\n",
    "# STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','Cca Finish Date' ,'perf_entry_count',\n",
    "# 'Does the CCA section including performance need to be completed?  (Applies to projects with CCA date on or after July 1, 2020)'\n",
    "# ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to add Resource in the PID workplan?\n",
    "\n",
    "def ck_PID_resource_date(df):\n",
    "    if pd.notnull(df['EFIS_Program']) and pd.isnull(df['Resourced In PID WP']):\n",
    "        return 'For project {}: Please enter the resourced PID workplan.'.format(df['AMT_ID'],)\n",
    "    else:\n",
    "        return 'OK'\n",
    "    \n",
    "ck_name = 'Need to add resource in the PID workplan?'\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_PID_resource_date, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-measurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRG section needs amendment date?\n",
    "\n",
    "def ck_add_PRG_amendment_date(df):\n",
    "    if pd.notnull(df['EFIS_Program']) and pd.isnull(df['SHOPP Amendment Date']):\n",
    "        return 'For project {}: Please add the SHOPP amendment date in PRG section'.format(df['AMT_ID'],)\n",
    "    else:\n",
    "        return 'OK'\n",
    "    \n",
    "ck_name = 'PRG section needs amendment date?'\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_add_PRG_amendment_date, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-trinity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPC section needs amendment date?\n",
    "\n",
    "#district should not fill the PPC section if the project is not programmed with future amendment date in PRG section\n",
    "\n",
    "def ck_add_PPC_amendment_date(df):\n",
    "    if (df['PCR Total Cost ($K)'] == df['Total Capital & Support Cost'] \n",
    "        and df['PCR RTL'] ==  df['FY'] \n",
    "       and pd.isnull(df['PCR SHOPP Amendment Date'])\n",
    "       ):\n",
    "        return 'For project {}: Please review the PPC section and add the SHOPP amendment date.'.format(df['AMT_ID'],)\n",
    "    else:\n",
    "        return 'OK'\n",
    "    \n",
    "ck_name = 'PPC section needs amendment date?'\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_add_PPC_amendment_date, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Unit Test#####\n",
    "\n",
    "AMT_ID = 19336\n",
    "\n",
    "STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "\n",
    "df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','FY' ,'PCR RTL' ,\n",
    "'Total Capital & Support Cost' ,'PCR SHOPP Amendment Date' ,'PCR Total Cost ($K)','PPC section needs amendment date?'                                             \n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-facing",
   "metadata": {},
   "source": [
    "<a id='NewChecks'></a>\n",
    "## New checks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-mexican",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-nightlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_summary_ActiveSection = pd.merge(df_program_summary, \n",
    "                                            df_SHOPP_raw_data[['AMT_ID','Section In Use', 'County','Route',  'Begin Postmile', 'End Postmile','Multiple Loc']], \n",
    "                how='inner', left_on=['AMT_ID','Section'], right_on=['AMT_ID','Section In Use'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-elephant",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_AMT = pd.merge(df_programmed_projects, df_program_summary_ActiveSection, \n",
    "                how='left', left_on=['AMT_ID'], right_on=['AMT_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['EFIS_Program', 'County', 'Route', 'Post Miles', \n",
    "       'Performance Value', 'Performance Measure', \n",
    "        'Programming Pre-Good', 'Programming Pre-Fair',\n",
    "       'Programming Pre-Poor', 'Programming Pre-Qty', 'Programming Post_Good',\n",
    "       'Programming Post_Fair', 'Programming Post_Poor',\n",
    "       'Programming Post_Qty', 'Programming Unit',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_combined = pd.merge(df_program_AMT, df_program[cols],\n",
    "                how='left', \n",
    "                left_on=['EFIS'], right_on=['EFIS_Program'], suffixes=['_AMT','_CTIPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-drive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_PM_Begin_End(PM_string):\n",
    "    if PM_string == '':\n",
    "        return '', ''\n",
    "    \n",
    "    PM_string = str(PM_string)\n",
    "    PMs = PM_string.split('/')\n",
    "    if len(PMs) == 1: \n",
    "        return PMs[0], PMs[0]\n",
    "    elif len(PMs) == 2: \n",
    "        return PMs[0], PMs[1] \n",
    "    else:\n",
    "        return 'Error','Error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_PM(PM):\n",
    "    \n",
    "    '''\n",
    "    validate and parse the Post Mile string \n",
    "    \n",
    "    input: PM string\n",
    "    return:\n",
    "        PM prefix\n",
    "        PM digit\n",
    "        PM suffix\n",
    "    \n",
    "    Allowed Prefix\n",
    "    Allowed Suffix\n",
    "    '''\n",
    "    \n",
    "    PM = PM.strip().upper()\n",
    "    \n",
    "#     if type(PM) == float or PM.isnumeric(): return float(PM)\n",
    "    \n",
    "    if PM[0].isalpha(): \n",
    "        prefix = PM[0].upper()\n",
    "        PM = PM[1:]\n",
    "    else:\n",
    "        prefix = ''\n",
    "    if PM[-1].isalpha(): \n",
    "        suffix = PM[-1].upper()\n",
    "        PM = PM[:-1]\n",
    "    else:\n",
    "        suffix = ''\n",
    "        \n",
    "    try:\n",
    "        return  prefix, round(float(PM), 3), suffix\n",
    "    except:\n",
    "        return '', PM , ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_combined['Post Miles'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superior-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_combined['Begin Postmile_CTIPS'], df_program_combined['End Postmile_CTIPS'] = zip(*df_program_combined['Post Miles'].apply(split_PM_Begin_End))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_PMs(pm_AMT, pm_CTIPS, multiple_loc):\n",
    "#do PM clean up before comparison, then only check prefix and digit\n",
    "# if the difference of digits are within 0.1 mile, it is ok\n",
    "\n",
    "    if pm_CTIPS == '' :\n",
    "        if multiple_loc == 'Yes':\n",
    "            return 'OK'\n",
    "        else:\n",
    "            return 'Post mile does not match between AMTool and CTIPS.'\n",
    "    \n",
    "    pm1_p, pm1_d, pm1_s = parse_PM(pm_AMT)\n",
    "    pm2_p, pm2_d, pm2_s = parse_PM(pm_CTIPS)\n",
    "    \n",
    "    if pm1_p != pm2_p or (pm2_s != '' and pm1_s != pm2_s):\n",
    "        return 'Post mile prefix or suffix does not match between AMTool and CTIPS.'\n",
    "    elif abs(round(pm1_d,1) - round(pm2_d,1) ) < 0.0999:\n",
    "        return 'OK'\n",
    "    else:\n",
    "        return 'Post mile values differ by at least 0.1 mile between AMTool and CTIPS.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_combined['Begin Postmile Ck'] = df_program_combined.apply(\n",
    "    lambda df: compare_PMs(df['Begin Postmile'], df['Begin Postmile_CTIPS'], df['Multiple Loc']), axis = 1)\n",
    "\n",
    "df_program_combined['End Postmile Ck'] = df_program_combined.apply(\n",
    "    lambda df: compare_PMs(df['End Postmile'], df['End Postmile_CTIPS'], df['Multiple Loc']), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-documentary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-frequency",
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_col = 'Does Route Match CTIPS?'\n",
    "def ck_Route(df):\n",
    "    if pd.isna(df['Route_CTIPS']) or df['Route_CTIPS'].upper() == 'VAR':\n",
    "        if df['Multiple Loc'] == 'Yes':\n",
    "            return 'OK'\n",
    "        else:\n",
    "            return 'Error'\n",
    "    else:\n",
    "        CTIPS = re.split('(\\d+)',df['Route_CTIPS'])\n",
    "        AMT = re.split('(\\d+)',df['Route_AMT'])\n",
    "        \n",
    "        if len(CTIPS) == 3 and len(AMT) == 3 and int(CTIPS[1]) == int(AMT[1]) and CTIPS[2] == AMT[2]:\n",
    "            return 'OK'\n",
    "        else:\n",
    "            return 'Error'\n",
    "\n",
    "df_program_combined[ck_col] = df_program_combined.apply(ck_Route, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_col = 'Does County Match CTIPS?'\n",
    "\n",
    "def ck_County(df):\n",
    "    if pd.isna(df['County_CTIPS']) or df['County_CTIPS'].upper() == 'VAR':\n",
    "        if df['Multiple Loc'] == 'Yes':\n",
    "            return 'OK'\n",
    "        else:\n",
    "            return 'Error'\n",
    "    elif df['County_CTIPS'] == df['County_AMT']:\n",
    "        return 'OK'\n",
    "    else:\n",
    "        return 'Error'\n",
    "df_program_combined[ck_col] = df_program_combined.apply(ck_County, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-category",
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_col = 'Does Project Location Match CTIPS?'\n",
    "\n",
    "def ck_location(df):\n",
    "    if df['Does Route Match CTIPS?'] != 'OK' or df['Does County Match CTIPS?'] != 'OK' or df['Begin Postmile Ck']!= 'OK' or df['End Postmile Ck'] !='OK':\n",
    "        return 'The Post mile inputs (Route, County, Begin PM, End PM) do not fully match between AMTool and CTIPS.'\n",
    "    else :\n",
    "        return 'OK'\n",
    "    \n",
    "df_program_combined[ck_col] = df_program_combined.apply(ck_location, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unit Test\n",
    "df_program_combined[df_program_combined['AMT_ID'].isin([9133,9188,16368,16166])][['AMT_ID',\n",
    "    'County_AMT','County_CTIPS',\n",
    "    'Route_AMT','Route_CTIPS',\n",
    "    'Does Route Match CTIPS?','Does County Match CTIPS?',\n",
    "    'Begin Postmile','Begin Postmile_CTIPS',\n",
    "    'End Postmile','End Postmile_CTIPS',\n",
    "    'Begin Postmile Ck','End Postmile Ck',\n",
    "    'Does Project Location Match CTIPS?',\n",
    "       ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unit Test\n",
    "df_program_combined[df_program_combined['AMT_ID'].isin([11249,16364\n",
    "])][['AMT_ID',\n",
    "    'Begin Postmile','Begin Postmile_CTIPS',\n",
    "    'End Postmile','End Postmile_CTIPS',\n",
    "    'Begin Postmile Ck','End Postmile Ck',\n",
    "    'Does Project Location Match CTIPS?',\n",
    "       ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pm1_p, pm1_d, pm1_s = parse_PM('0.8')\n",
    "# pm2_p, pm2_d, pm2_s = parse_PM('0.9')\n",
    "\n",
    "# abs(round(pm1_d,2) - round(pm2_d,2))\n",
    "\n",
    "# round(pm2_d,1)\n",
    "\n",
    "# round(pm1_d,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vocational-issue",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_program_combined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8c2a17c198c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m df_program_combined[[    'Pre-Good','Programming Pre-Good',\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;34m'Pre-Fair'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Programming Pre-Fair'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m  \u001b[1;34m'Pre-Poor'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Programming Pre-Poor'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m      \u001b[1;34m'Post Good+New'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Programming Post_Good'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m      \u001b[1;34m'Post-Fair'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Programming Post_Fair'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_program_combined' is not defined"
     ]
    }
   ],
   "source": [
    "df_program_combined[[    'Pre-Good','Programming Pre-Good',\n",
    "'Pre-Fair','Programming Pre-Fair',\n",
    " 'Pre-Poor','Programming Pre-Poor',\n",
    "     'Post Good+New', 'Programming Post_Good',\n",
    "     'Post-Fair', 'Programming Post_Fair',\n",
    "     'Post-Poor', 'Programming Post_Poor']].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "thermal-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_col = 'Does Anchor Performance Match CTIPS?'\n",
    "\n",
    "def ck_Anchor_Performance(df):\n",
    "    if pd.isna(df['Performance Value_AMT']) or abs(round(df['Performance Value_AMT'],1) - round(df['Performance Value_CTIPS'],1))>=0.1:\n",
    "        return 'Project anchor performance value does not match CTIPS.'\n",
    "    \n",
    "    elif pd.isna(df['Performance Measure_AMT']) or df['Performance Measure_AMT'] != df['Performance Measure_CTIPS']:\n",
    "        return 'Project anchor performance measure does not match CTIPS.'\n",
    "    \n",
    "    elif pd.notna(df['Asset Class']):\n",
    "        if abs(df['Pre-Good'] - df['Programming Pre-Good']) >= 0.1:\n",
    "            return 'Project anchor performance \"Pre-Good\" do not match CTIPS.' \n",
    "                                                                              \n",
    "        elif abs(df['Pre-Fair'] - df['Programming Pre-Fair']) >= 0.1:\n",
    "            return 'Project anchor performance \"Pre-Fair\" do not match CTIPS.'    \n",
    "                                                                              \n",
    "        elif abs(df['Pre-Poor'] - df['Programming Pre-Poor']) >= 0.1:\n",
    "            return 'Project anchor performance \"Pre-Poor\" do not match CTIPS.'  \n",
    "                                                                              \n",
    "        elif abs(df['Post Good+New'] - df['Programming Post_Good']) >= 0.1:\n",
    "            return 'Project anchor performance \"Post Good+New\" do not match CTIPS.'     \n",
    "                                                                              \n",
    "        elif abs(df['Post-Fair'] - df['Programming Post_Fair']) >= 0.1:\n",
    "            return 'Project anchor performance \"Post-Fair\" do not match CTIPS.'  \n",
    "                                                                              \n",
    "        elif abs(df['Post-Poor'] - df['Programming Post_Poor']) >= 0.1:\n",
    "            return 'Project anchor performance \"Post-Poor\" do not match CTIPS.'  \n",
    "\n",
    "        else:\n",
    "            return 'OK'\n",
    "    else:\n",
    "        return 'OK'\n",
    "\n",
    "df_program_combined[ck_col] = df_program_combined.apply(ck_Anchor_Performance, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "sensitive-advice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-515-8e6047ba92b8>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_program_combined[df_program_combined['AMT_ID'].isin([9306])][ck_col] = df_program_combined[df_program_combined['AMT_ID'].isin([9306])].apply(ck_Anchor_Performance, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# df_program_combined[df_program_combined['AMT_ID'].isin([9306])][ck_col] = df_program_combined[df_program_combined['AMT_ID'].isin([9306])].apply(ck_Anchor_Performance, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "accredited-census",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMT_ID</th>\n",
       "      <th>Does Anchor Performance Match CTIPS?</th>\n",
       "      <th>Performance Value_AMT</th>\n",
       "      <th>Performance Value_CTIPS</th>\n",
       "      <th>Performance Measure_AMT</th>\n",
       "      <th>Performance Measure_CTIPS</th>\n",
       "      <th>Asset Class</th>\n",
       "      <th>Pre-Good</th>\n",
       "      <th>Programming Pre-Good</th>\n",
       "      <th>Pre-Fair</th>\n",
       "      <th>Programming Pre-Fair</th>\n",
       "      <th>Pre-Poor</th>\n",
       "      <th>Programming Pre-Poor</th>\n",
       "      <th>Post Good+New</th>\n",
       "      <th>Programming Post_Good</th>\n",
       "      <th>Post-Fair</th>\n",
       "      <th>Programming Post_Fair</th>\n",
       "      <th>Post-Poor</th>\n",
       "      <th>Programming Post_Poor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>13351.0</td>\n",
       "      <td>Project anchor performance \"Pre-Good\" do not m...</td>\n",
       "      <td>29.4</td>\n",
       "      <td>29.4</td>\n",
       "      <td>Lane mile(s)</td>\n",
       "      <td>Lane mile(s)</td>\n",
       "      <td>Primary</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.3</td>\n",
       "      <td>21.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>29.4</td>\n",
       "      <td>29.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>16180.0</td>\n",
       "      <td>Project anchor performance \"Pre-Poor\" do not m...</td>\n",
       "      <td>8.9</td>\n",
       "      <td>8.9</td>\n",
       "      <td>Mile(s) of cable</td>\n",
       "      <td>Mile(s) of cable</td>\n",
       "      <td>Primary</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>13733.0</td>\n",
       "      <td>Project anchor performance value does not matc...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>Location(s)</td>\n",
       "      <td>Location(s)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1127</th>\n",
       "      <td>9306.0</td>\n",
       "      <td>Project anchor performance value does not matc...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Location(s)</td>\n",
       "      <td>Location(s)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1165</th>\n",
       "      <td>11367.0</td>\n",
       "      <td>OK</td>\n",
       "      <td>20.4</td>\n",
       "      <td>20.4</td>\n",
       "      <td>Lane mile(s)</td>\n",
       "      <td>Lane mile(s)</td>\n",
       "      <td>Primary</td>\n",
       "      <td>7.9</td>\n",
       "      <td>7.9</td>\n",
       "      <td>12.5</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>20.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AMT_ID               Does Anchor Performance Match CTIPS?  \\\n",
       "271   13351.0  Project anchor performance \"Pre-Good\" do not m...   \n",
       "1038  16180.0  Project anchor performance \"Pre-Poor\" do not m...   \n",
       "1078  13733.0  Project anchor performance value does not matc...   \n",
       "1127   9306.0  Project anchor performance value does not matc...   \n",
       "1165  11367.0                                                 OK   \n",
       "\n",
       "      Performance Value_AMT  Performance Value_CTIPS Performance Measure_AMT  \\\n",
       "271                    29.4                     29.4            Lane mile(s)   \n",
       "1038                    8.9                      8.9        Mile(s) of cable   \n",
       "1078                   28.0                     52.0             Location(s)   \n",
       "1127                   30.0                     40.0             Location(s)   \n",
       "1165                   20.4                     20.4            Lane mile(s)   \n",
       "\n",
       "     Performance Measure_CTIPS Asset Class  Pre-Good  Programming Pre-Good  \\\n",
       "271               Lane mile(s)     Primary       3.4                   3.3   \n",
       "1038          Mile(s) of cable     Primary       0.0                   0.0   \n",
       "1078               Location(s)         NaN       0.0                   NaN   \n",
       "1127               Location(s)         NaN       0.0                   NaN   \n",
       "1165              Lane mile(s)     Primary       7.9                   7.9   \n",
       "\n",
       "      Pre-Fair  Programming Pre-Fair  Pre-Poor  Programming Pre-Poor  \\\n",
       "271       21.2                  21.2       4.9                   4.9   \n",
       "1038       0.0                   0.0       8.9                   1.0   \n",
       "1078       0.0                   NaN      28.0                   NaN   \n",
       "1127       0.0                   NaN      30.0                   NaN   \n",
       "1165      12.5                  12.5       0.0                   0.0   \n",
       "\n",
       "      Post Good+New Programming Post_Good  Post-Fair  Programming Post_Fair  \\\n",
       "271            29.4                  29.4        0.0                    0.0   \n",
       "1038            8.9                   1.0        0.0                    0.0   \n",
       "1078           28.0                   NaN        0.0                    NaN   \n",
       "1127           30.0                   NaN        0.0                    NaN   \n",
       "1165           20.4                  20.4        0.0                    0.0   \n",
       "\n",
       "      Post-Poor  Programming Post_Poor  \n",
       "271         0.0                    0.0  \n",
       "1038        0.0                    0.0  \n",
       "1078        0.0                    NaN  \n",
       "1127        0.0                    NaN  \n",
       "1165        0.0                    0.0  "
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Unit Test\n",
    "df_program_combined[df_program_combined['AMT_ID'].isin([9306,11367,13351, 16180, 13733\n",
    "])][['AMT_ID',\n",
    "       'Does Anchor Performance Match CTIPS?',\n",
    "    'Performance Value_AMT','Performance Value_CTIPS',\n",
    "    \n",
    "    'Performance Measure_AMT','Performance Measure_CTIPS',\n",
    "    \n",
    "    'Asset Class',\n",
    "    'Pre-Good','Programming Pre-Good',\n",
    "    'Pre-Fair','Programming Pre-Fair',\n",
    "    'Pre-Poor','Programming Pre-Poor',\n",
    "    'Post Good+New', 'Programming Post_Good',\n",
    "    'Post-Fair', 'Programming Post_Fair',\n",
    "    'Post-Poor', 'Programming Post_Poor',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "studied-punch",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMT_ID</th>\n",
       "      <th>Does Anchor Performance Match CTIPS?</th>\n",
       "      <th>Performance Value_AMT</th>\n",
       "      <th>Performance Value_CTIPS</th>\n",
       "      <th>Performance Measure_AMT</th>\n",
       "      <th>Performance Measure_CTIPS</th>\n",
       "      <th>Asset Class</th>\n",
       "      <th>Pre-Good</th>\n",
       "      <th>Programming Pre-Good</th>\n",
       "      <th>Pre-Fair</th>\n",
       "      <th>Programming Pre-Fair</th>\n",
       "      <th>Pre-Poor</th>\n",
       "      <th>Programming Pre-Poor</th>\n",
       "      <th>Post Good+New</th>\n",
       "      <th>Programming Post_Good</th>\n",
       "      <th>Post-Fair</th>\n",
       "      <th>Programming Post_Fair</th>\n",
       "      <th>Post-Poor</th>\n",
       "      <th>Programming Post_Poor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>16787.0</td>\n",
       "      <td>Project anchor performance \"Pre-Good\" do not m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Location(s)</td>\n",
       "      <td>Location(s)</td>\n",
       "      <td>Supplementary</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>16792.0</td>\n",
       "      <td>Project anchor performance \"Pre-Good\" do not m...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Location(s)</td>\n",
       "      <td>Location(s)</td>\n",
       "      <td>Supplementary</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>13295.0</td>\n",
       "      <td>Project anchor performance \"Pre-Good\" do not m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Bridge(s)</td>\n",
       "      <td>Bridge(s)</td>\n",
       "      <td>Primary</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AMT_ID               Does Anchor Performance Match CTIPS?  \\\n",
       "712   16787.0  Project anchor performance \"Pre-Good\" do not m...   \n",
       "846   16792.0  Project anchor performance \"Pre-Good\" do not m...   \n",
       "1021  13295.0  Project anchor performance \"Pre-Good\" do not m...   \n",
       "\n",
       "      Performance Value_AMT  Performance Value_CTIPS Performance Measure_AMT  \\\n",
       "712                     1.0                      1.0             Location(s)   \n",
       "846                     2.0                      2.0             Location(s)   \n",
       "1021                    0.0                      0.0               Bridge(s)   \n",
       "\n",
       "     Performance Measure_CTIPS    Asset Class  Pre-Good  Programming Pre-Good  \\\n",
       "712                Location(s)  Supplementary       0.0                   NaN   \n",
       "846                Location(s)  Supplementary       0.0                   NaN   \n",
       "1021                 Bridge(s)        Primary       0.0                   NaN   \n",
       "\n",
       "      Pre-Fair  Programming Pre-Fair  Pre-Poor  Programming Pre-Poor  \\\n",
       "712        0.0                   NaN       1.0                   NaN   \n",
       "846        0.0                   NaN       2.0                   NaN   \n",
       "1021       0.0                   NaN       0.0                   NaN   \n",
       "\n",
       "      Post Good+New Programming Post_Good  Post-Fair  Programming Post_Fair  \\\n",
       "712             1.0                   NaN        0.0                    NaN   \n",
       "846             2.0                   NaN        0.0                    NaN   \n",
       "1021            0.0                   NaN        0.0                    NaN   \n",
       "\n",
       "      Post-Poor  Programming Post_Poor  \n",
       "712         0.0                    NaN  \n",
       "846         0.0                    NaN  \n",
       "1021        0.0                    NaN  "
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Unit Test\n",
    "df_program_combined[df_program_combined['AMT_ID'].isin([13295,16787,\n",
    "16792\n",
    "])][['AMT_ID',\n",
    "       'Does Anchor Performance Match CTIPS?',\n",
    "    'Performance Value_AMT','Performance Value_CTIPS',\n",
    "    \n",
    "    'Performance Measure_AMT','Performance Measure_CTIPS',\n",
    "    \n",
    "    'Asset Class',\n",
    "     \n",
    "    'Pre-Good','Programming Pre-Good',\n",
    "    'Pre-Fair','Programming Pre-Fair',\n",
    "    'Pre-Poor','Programming Pre-Poor',\n",
    "    'Post Good+New', 'Programming Post_Good',\n",
    "    'Post-Fair', 'Programming Post_Fair',\n",
    "    'Post-Poor', 'Programming Post_Poor',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "miniature-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_combined[['AMT_ID','EA','EFIS_Program','EFIS','Section In Use','Does Project Location Match CTIPS?','Does Anchor Performance Match CTIPS?'\n",
    "]].to_csv('unit_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-booking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "biblical-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #Unit Test\n",
    "# df_program_combined[df_program_combined['AMT_ID'].isin([16209,19158 ])][['Performance Value_AMT','Performance Value_CTIPS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "earlier-turkey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30., 40.]])"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_program_combined[df_program_combined['AMT_ID'].isin([9306])][['Performance Value_AMT','Performance Value_CTIPS',]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "arabic-schema",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (round(40,1) - round(30,1)>0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "backed-rings",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMT_ID</th>\n",
       "      <th>Does Anchor Performance Match CTIPS?</th>\n",
       "      <th>Performance Value_AMT</th>\n",
       "      <th>Performance Value_CTIPS</th>\n",
       "      <th>Performance Measure_AMT</th>\n",
       "      <th>Performance Measure_CTIPS</th>\n",
       "      <th>Asset Class</th>\n",
       "      <th>Pre-Good</th>\n",
       "      <th>Programming Pre-Good</th>\n",
       "      <th>Pre-Fair</th>\n",
       "      <th>Programming Pre-Fair</th>\n",
       "      <th>Pre-Poor</th>\n",
       "      <th>Programming Pre-Poor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>22744.0</td>\n",
       "      <td>OK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Location(s)</td>\n",
       "      <td>Location(s)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>18555.0</td>\n",
       "      <td>OK</td>\n",
       "      <td>489.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>Linear feet rail</td>\n",
       "      <td>Linear feet rail</td>\n",
       "      <td>Primary</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>489.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>11249.0</td>\n",
       "      <td>Project anchor performance items do not match ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bridge(s)</td>\n",
       "      <td>Bridge(s)</td>\n",
       "      <td>Primary</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18105.0</td>\n",
       "      <td>18105.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      AMT_ID               Does Anchor Performance Match CTIPS?  \\\n",
       "358  22744.0                                                 OK   \n",
       "444  18555.0                                                 OK   \n",
       "566  11249.0  Project anchor performance items do not match ...   \n",
       "\n",
       "     Performance Value_AMT  Performance Value_CTIPS Performance Measure_AMT  \\\n",
       "358                    1.0                      1.0             Location(s)   \n",
       "444                  489.0                    489.0        Linear feet rail   \n",
       "566                    1.0                      1.0               Bridge(s)   \n",
       "\n",
       "    Performance Measure_CTIPS Asset Class  Pre-Good  Programming Pre-Good  \\\n",
       "358               Location(s)         NaN       0.0                   NaN   \n",
       "444          Linear feet rail     Primary       0.0                   0.0   \n",
       "566                 Bridge(s)     Primary       0.0                   0.0   \n",
       "\n",
       "     Pre-Fair  Programming Pre-Fair  Pre-Poor  Programming Pre-Poor  \n",
       "358       0.0                   NaN       1.0                   NaN  \n",
       "444       0.0                   0.0     489.0                 489.0  \n",
       "566       0.0                   0.0   18105.0               18105.0  "
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Unit Test\n",
    "df_program_combined[df_program_combined['AMT_ID'].isin([14095,16836,17254,17257,18337,19069])][['AMT_ID',\n",
    "       'Does Anchor Performance Match CTIPS?',\n",
    "    'Performance Value_AMT','Performance Value_CTIPS',\n",
    "    \n",
    "    'Performance Measure_AMT','Performance Measure_CTIPS',\n",
    "    \n",
    "    'Asset Class',\n",
    "    'Pre-Good','Programming Pre-Good',\n",
    "'Pre-Fair','Programming Pre-Fair',\n",
    " 'Pre-Poor','Programming Pre-Poor'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "equal-network",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMT_ID</th>\n",
       "      <th>Does Anchor Performance Match CTIPS?</th>\n",
       "      <th>Performance Value_AMT</th>\n",
       "      <th>Performance Value_CTIPS</th>\n",
       "      <th>Performance Measure_AMT</th>\n",
       "      <th>Performance Measure_CTIPS</th>\n",
       "      <th>Asset Class</th>\n",
       "      <th>Pre-Good</th>\n",
       "      <th>Programming Pre-Good</th>\n",
       "      <th>Pre-Fair</th>\n",
       "      <th>Programming Pre-Fair</th>\n",
       "      <th>Pre-Poor</th>\n",
       "      <th>Programming Pre-Poor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>22744.0</td>\n",
       "      <td>OK</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Location(s)</td>\n",
       "      <td>Location(s)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>18555.0</td>\n",
       "      <td>OK</td>\n",
       "      <td>489.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>Linear feet rail</td>\n",
       "      <td>Linear feet rail</td>\n",
       "      <td>Primary</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>489.0</td>\n",
       "      <td>489.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>11249.0</td>\n",
       "      <td>Project anchor performance items do not match ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bridge(s)</td>\n",
       "      <td>Bridge(s)</td>\n",
       "      <td>Primary</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18105.0</td>\n",
       "      <td>18105.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      AMT_ID               Does Anchor Performance Match CTIPS?  \\\n",
       "358  22744.0                                                 OK   \n",
       "444  18555.0                                                 OK   \n",
       "566  11249.0  Project anchor performance items do not match ...   \n",
       "\n",
       "     Performance Value_AMT  Performance Value_CTIPS Performance Measure_AMT  \\\n",
       "358                    1.0                      1.0             Location(s)   \n",
       "444                  489.0                    489.0        Linear feet rail   \n",
       "566                    1.0                      1.0               Bridge(s)   \n",
       "\n",
       "    Performance Measure_CTIPS Asset Class  Pre-Good  Programming Pre-Good  \\\n",
       "358               Location(s)         NaN       0.0                   NaN   \n",
       "444          Linear feet rail     Primary       0.0                   0.0   \n",
       "566                 Bridge(s)     Primary       0.0                   0.0   \n",
       "\n",
       "     Pre-Fair  Programming Pre-Fair  Pre-Poor  Programming Pre-Poor  \n",
       "358       0.0                   NaN       1.0                   NaN  \n",
       "444       0.0                   0.0     489.0                 489.0  \n",
       "566       0.0                   0.0   18105.0               18105.0  "
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#Unit Test\n",
    "df_program_combined[df_program_combined['AMT_ID'].isin([14095,16836,17254,17257,18337,19069])][['AMT_ID',\n",
    "       'Does Anchor Performance Match CTIPS?',\n",
    "    'Performance Value_AMT','Performance Value_CTIPS',\n",
    "    \n",
    "    'Performance Measure_AMT','Performance Measure_CTIPS',\n",
    "    \n",
    "    'Asset Class',\n",
    "    'Pre-Good','Programming Pre-Good',\n",
    "'Pre-Fair','Programming Pre-Fair',\n",
    " 'Pre-Poor','Programming Pre-Poor'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "primary-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_program_combined[df_program_combined['AMT_ID'].isin([14095])]['Asset Class'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "turkish-while",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "stopped-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Unit Test\n",
    "# df_program_combined[df_program_combined['AMT_ID'].isin([13745, 17182])][['County_AMT','County_CTIPS','Route_AMT','Route_CTIPS','Does Project Location Match CTIPS?',\n",
    "#        'Does Anchor Performance Match CTIPS?']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "challenging-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_program_combined['Data_TimeStamp']=Data_TimeStamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "flexible-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cols = [\n",
    "       'AMT_ID', 'District', 'EA', 'EFIS', 'Section','Program Code',\n",
    "       'Activity Category', 'Asset Class', 'Asset', 'Performance Value_AMT',\n",
    "       'Performance Measure_AMT', 'Unit', 'Pre-Good', 'Pre-Fair', 'Pre-Poor',\n",
    "       'Pre-Total', 'Post Good', 'New', 'Post Good+New', 'Post-Fair',\n",
    "       'Post-Poor', 'Post-Total', 'Program', 'Type', 'Status',\n",
    "       'Section In Use', 'County_AMT', 'Route_AMT', 'Begin Postmile',\n",
    "       'End Postmile', 'Multiple Loc', 'EFIS_Program', 'County_CTIPS',\n",
    "       'Route_CTIPS', 'Post Miles', 'Performance Value_CTIPS',\n",
    "       'Performance Measure_CTIPS', 'Programming Pre-Good',\n",
    "       'Programming Pre-Fair', 'Programming Pre-Poor', 'Programming Pre-Qty',\n",
    "       'Programming Post_Good', 'Programming Post_Fair',\n",
    "       'Programming Post_Poor', 'Programming Post_Qty', 'Programming Unit',\n",
    "       'Begin Postmile_CTIPS', 'End Postmile_CTIPS',\n",
    "       'Does Project Location Match CTIPS?', 'Does Anchor Performance Match CTIPS?', \n",
    "    'Data_TimeStamp',\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "transparent-nicaragua",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing table: 1172it [00:00, 5139.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signing into AssetManagement at https://tableau.dot.ca.gov\n",
      "Publishing Programmed_Project_Data_Check.hyper to Sandbox_ProjectBookCheck_Automation...\n"
     ]
    }
   ],
   "source": [
    "uf.export_hyper(df_program_combined[out_cols], 'Programmed_Project_Data_Check', LOG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "together-label",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'03-08-2022 15:10:44'"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data_TimeStamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-tissue",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-debate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-integral",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consecutive-above",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "adjustable-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check comments about ActID of E55 and E58\n",
    "#if ActID == E55 and E58 and the comments include: 'HQ added the activity and District needs to review it.', mark the project\n",
    "\n",
    "def filter_target_comment(df, target_comment):\n",
    "    if target_comment in str(df['Comment']):\n",
    "        return target_comment\n",
    "    else: \n",
    "        return 'OK'\n",
    "\n",
    "target_comment = 'HQ added the activity and District needs to review it.'\n",
    "df_perf_raw_data['Activity Has Safety Comment?'] = df_perf_raw_data.apply(filter_target_comment, args = [target_comment], axis = 1)\n",
    "\n",
    "temp = df_perf_raw_data[df_perf_raw_data['ActID'].isin(['E55','E58'])]\n",
    "\n",
    "\n",
    "temp1 = temp.groupby(['AMT_ID', 'Section'])['Activity Has Safety Comment?'].agg(list).reset_index()\n",
    "\n",
    "\n",
    "def ck_Safety_Comments(df):\n",
    "    if 'HQ added the activity and District needs to review it.' in df['Activity Has Safety Comment?']:\n",
    "        return 'HQ added the activity and District needs to review it.'\n",
    "    else:\n",
    "        return 'OK'\n",
    "    \n",
    "temp1['Check Safety Comment'] = temp1.apply(ck_Safety_Comments,axis = 1)\n",
    "\n",
    "\n",
    "df_SHOPP_raw_data.drop(['Check Safety Comment'],  axis='columns', inplace=True, errors='ignore')\n",
    "\n",
    "df_SHOPP_raw_data = pd.merge(df_SHOPP_raw_data, temp1[['AMT_ID', 'Section', 'Check Safety Comment']], \n",
    "                             how = 'left', left_on = ['AMT_ID', 'Section', ], right_on = ['AMT_ID', 'Section'])\n",
    "\n",
    "\n",
    "#TODO\n",
    "#remove the check comments for TYP=9999\n",
    "df_SHOPP_raw_data['Check Safety Comment'] = df_SHOPP_raw_data.apply(\n",
    "    lambda df: 'OK' if df['Ten-Year Plan RD'] == 9999 else df['Check Safety Comment'], axis = 1)\n",
    "\n",
    "df_SHOPP_raw_data['Check Safety Comment'].fillna('OK', inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-trade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "ordered-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attached projects have TMS IDs that do not match the new data we are uploading the AM tool.\n",
    "# For projects in the 5-year POR, Please create a data check that check if project in the list attaches is using the 11/30/2021 data for the TMS worksheet .\n",
    "ck_name = 'TMS data update needed for the project?'\n",
    "\n",
    "def ck_TMS_dataupdate(df):\n",
    "    if (df['Include 5-year POR?'] == 'Yes' \n",
    "        and df['AMT_ID'] in df_TMS_Datachange['AMT_ID'].values\n",
    "        and pd.notna(df['Data Date_TMS'])\n",
    "        and datetime.strptime(df['Data Date_TMS'], \"%m-%d-%Y\") < datetime.strptime(TMS_Data_Date, \"%m-%d-%Y\")\n",
    "        ):\n",
    "        return 'Please update the TMS worksheet to use the current TMS inventory and condition data ({}). AMT_ID: {}'.format(df['AMT_ID'], TMS_Data_Date)\n",
    "\n",
    "    return 'OK'    \n",
    "\n",
    "df_SHOPP_raw_data[ck_name] = df_SHOPP_raw_data.apply(ck_TMS_dataupdate, axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "straight-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMS_Data_Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "surface-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['Data Date_TMS'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "foster-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['Data Date_TMS'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "sunrise-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###Unit Test#####\n",
    "\n",
    "# # df_SHOPP_raw_data[(df_SHOPP_raw_data['Include 5-year POR?'] == 'Yes') & (df_SHOPP_raw_data['AMT_ID'].isin(df_TMS_Datachange['AMT_ID'].values))]\n",
    "\n",
    "# AMT_ID = 20248\n",
    "\n",
    "# STU = df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID]['Section'].iloc[0]\n",
    "# print(TMS_Data_Date)\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] ==AMT_ID][['AMT_ID','Section','FY' ,'Data Date_TMS','Include 5-year POR?',ck_name                                            \n",
    "# ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-workstation",
   "metadata": {},
   "source": [
    "\n",
    "<a id='Check_Flag'></a>\n",
    "\n",
    "## Check Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "turkish-haiti",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ck_cols = ['Does project cost exceed Minor Program limits ($1,250K)?',\n",
    "#             'Is Major Damage or Mobility Subcategory Identified?',\n",
    "            'Is Planned Project RTL in a FY that can be programmed in future SHOPP cycles?',\n",
    "            'Is the PID cycle consistent with the project status and RTL?',\n",
    "            'Is Project Initiation Proposal (PIP) uploaded? Applies to projects with Active and  Complete PIDs.',\n",
    "            'Is the Conceptual Cost Estimate (CCE) uploaded? (Applies to all project in the 5-year POR)',\n",
    "            'Are the Long Lead Project Cost and RTL fields complete and consistent?',\n",
    "            'Is District Director Approval Date in the Future?',\n",
    "            'Is the EA or Project ID repeated in the AM tool?',\n",
    "            'Does project include performance for each location?',\n",
    "            'Is Performance tab Complete?',\n",
    "            'Does the performance tab include one or more activities related to the Activity Category?',\n",
    "            'Are all Project Locations and Postmiles Valid?',\n",
    "            'Is Drainage Worksheet Complete (2024/25 RTL and after)?',\n",
    "            'Is Pavement Worksheet Complete (2024/25 RTL and after)?',\n",
    "            'Is TMS Worksheet Complete (2024/25 RTL and after)?', \n",
    "                                    \n",
    "            'Are all conditions selected for bridge replacements?',\n",
    "            'Does Bridge Worksheet need updates?',\n",
    "            'Does the Plan Year in the Pavement Worksheet match the Project RTL?',\n",
    "#             'Is the Pavement Work Limits Direction in the Pavement Worksheet complete?',\n",
    "            'Does the RTL Plan Year in the TMS Worksheet match the Project RTL?',\n",
    "            'Does SHOPP project data in the AM Tool data match CTIPS (RTL and/or Cost)?',\n",
    "#             'Is PID completed and uploaded for current SHOPP Candidates?',\n",
    "\n",
    "#             'LL not in POR', obseleted\n",
    "\n",
    "            'Is Pavement limits repeating in the same project?',\n",
    "            'Is Bridge repeating in the same project?',\n",
    "            'Is TMS Asset repeating in the same project?',\n",
    "           'Is Culvert repeating in the same project?',\n",
    "            \n",
    "            #modified checks\n",
    "            'Is a pavement segment in a planned project repeated in another project in the Project Book?',\n",
    "            'Is a bridge in a planned project repeated in another project in the Project Book?',\n",
    "            'Is a TMS in a planned project repeated in another project in the Project Book?',\n",
    "            'Is a culvert in a planned project repeated in another project in the Project Book?',\n",
    "           \n",
    "           \n",
    "            'Is the Pavement Worksheet using updated inventory and condition data (2019 APCS) for planned projects?',\n",
    "            'Is the Drainage Worksheet using updated inventory and condition data (Sept 2021 or later) for planned projects?',\n",
    "            'Is the TMS Worksheet using updated inventory and condition data (June 2021 or later) for planned projects?',\n",
    "            'Is this project in the Project Book but not in the PID Workplan?',\n",
    "            'Is this project in the PID Workplan but not in the Project Book?',\n",
    "            'For all planned projects, does the Performance Tab indicate If any location within the project limits Ped/Bike accessible?',\n",
    "           \n",
    "           #new checks\n",
    "           'Check Safety Comment', #added in 12-2021\n",
    "           'TMS data update needed for the project?',\n",
    "           \n",
    "           'Does the CCA date in the AM Tool match PRSM?',\n",
    "            'Does the CCA section including performance need to be completed?  (Applies to projects with CCA date on or after July 1, 2020)',\n",
    "            #TODO:\n",
    "           # 'Does the primary asset performance match CTIPS?  (Applies to projects with RTL 25/26 and later)',\n",
    "        \n",
    "          ]\n",
    "\n",
    "#filter out the projects without check flags\n",
    "def calc_flag(df, cols):\n",
    "    cnt = 0\n",
    "    for c in cols:\n",
    "        if df[c] !='OK': \n",
    "            cnt += 1\n",
    "    if cnt > 0: \n",
    "        return 'Flagged for Data Check'\n",
    "    else:\n",
    "        return 'Pass Data Check'\n",
    "\n",
    "df_SHOPP_raw_data['DataCheckFlag'] = df_SHOPP_raw_data.apply(calc_flag, args=(ck_cols,), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "external-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AMT_ID = 20004\n",
    "# # AMT_ID = 13667\n",
    "# temo_cols =  ck_cols+ ['DataCheckFlag']\n",
    "\n",
    "# df_SHOPP_raw_data[df_SHOPP_raw_data['AMT_ID'] == AMT_ID ][temo_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "assigned-browse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5261, 215)"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_SHOPP_raw_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-astrology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dominican-winning",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<a id='Project_missing_AMT_ID'></a>\n",
    "\n",
    "### Project missing AMT_ID (SHOPP ID) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "guilty-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp = pd.merge(df_program, df_SHOPP_raw_data[['EFIS']],\n",
    "               how ='left', left_on= 'EFIS_Program', right_on='EFIS')\n",
    "\n",
    "temp['FY_Program'] = temp['FY'].str[-2:].astype(int)+2000\n",
    "\n",
    "temp['Check Description'] = 'Does the District need to create a SHOPP ID in the AM Tool?'\n",
    "\n",
    "def ck_missing_EFIS(df):\n",
    "    if pd.isnull(df['EFIS']): \n",
    "        return 'Please create a SHOPP Project Record in AMT for Project ID ({}), including appropriate activities in the performance tab.'.format(df['EFIS_Program']) \n",
    "    else:\n",
    "        return 'OK' \n",
    "    \n",
    "    \n",
    "temp['Check Summary'] = temp.apply(ck_missing_EFIS, axis = 1)\n",
    "\n",
    "\n",
    "out_cols = ['Dist', 'EA','County','EFIS_Program', 'FY_Program', 'Check Description', 'Check Summary']\n",
    "\n",
    "project_missing_AMT_ID = temp[(temp['FY_Program']> 2019) & (temp['EFIS'].isna())][out_cols] # please change the logic for hard code all project with after RTL 18/19 and programmed should be in the tool. We missed this project in Q4\n",
    "\n",
    "project_missing_AMT_ID.columns = ['District', 'EA', 'County', 'EFIS', 'Last Year of Fiscal Year', 'Check Description', 'Check Summary']\n",
    "\n",
    "dict_rename = {\n",
    "    'Dist':'District',\n",
    "    'EFIS_Program':'EFIS', \n",
    "    'FY_Program': 'Last Year of Fiscal Year', \n",
    "}\n",
    "\n",
    "\n",
    "project_missing_AMT_ID = project_missing_AMT_ID.rename(dict_rename, axis = 1)\n",
    "\n",
    "project_missing_AMT_ID['Will this project be included in the Project Book?'] = 'Yes'\n",
    "project_missing_AMT_ID['Planning or Post-Planning'] = 'Planning'\n",
    "project_missing_AMT_ID['Section'] = 'TYP'\n",
    "project_missing_AMT_ID['AMT_ID'] = 0\n",
    "\n",
    "project_missing_AMT_ID['Data_TimeStamp'] = Data_TimeStamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "ultimate-elevation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ###Unit Test#####\n",
    "\n",
    "# EFIS_Program = 1021000191\n",
    "\n",
    "\n",
    "# temp[temp['EFIS_Program'] ==EFIS_Program][['EFIS','EFIS_Program','Dist', 'EA','County', 'FY_Program', 'Check Description', 'Check Summary']]\n",
    "\n",
    "# # temp[(temp['FY_Program']> TARGET_FY + 1) & (temp['EFIS'].isna())][out_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-anniversary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-storm",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-harrison",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "front-might",
   "metadata": {},
   "source": [
    "<a id='Export_Table1'></a>\n",
    "# Export Check Summary and Project Missing SHOPP ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "color-container",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_export_log = open(LOG_FILE, \"a\")  # append mode\n",
    "file_export_log.write(\"#####ProjectBook Data Check: {} \\n\".format(Data_TimeStamp))\n",
    "file_export_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "registered-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SHOPP_raw_data['Data_TimeStamp'] = Data_TimeStamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "robust-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export all projects with all checks in matrix\n",
    "out_cols = ['AMT_ID','Section', \n",
    "            'District',\n",
    "            'County',\n",
    "            'EA','EFIS',\n",
    "            'Planning or Post-Planning',\n",
    "            'Last Year of Fiscal Year',\n",
    "            'AM Tool RTL (Section in Use)',\n",
    "            'Advertised Year',\n",
    "            'Project Cost ($K)', \n",
    "            'Will this project be included in the Project Book?',\n",
    "            'DataCheckFlag',\n",
    "            'Ten-Year Plan RD',\n",
    "            \n",
    "            #internal check columns\n",
    "            'Does amendment date needs to be removed?',\n",
    "            'Need to add resource in the PID workplan?',\n",
    "            'PRG section needs amendment date?',\n",
    "            'PPC section needs amendment date?',\n",
    "            \n",
    "            'Data_TimeStamp',\n",
    "                    ]\n",
    "\n",
    "out_cols.extend(ck_cols)\n",
    "\n",
    "filename = 'projectbook_datachecks_matrix'\n",
    "df_out = df_SHOPP_raw_data[out_cols]\n",
    "uf.export_csv(df_out, filename, PROJECTBOOKCHECK_HTTPSERVER_FOLDER, LOG_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "split-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "#table 1\n",
    "\n",
    "df_melted = pd.melt(df_SHOPP_raw_data, \n",
    "                    id_vars=['AMT_ID'], \n",
    "                    value_vars=ck_cols, var_name = 'Check Description')\n",
    "\n",
    "df_melted.columns = ['AMT_ID','Check Description','Check Summary']\n",
    "df_melted_filtered = df_melted[df_melted['Check Summary']!= 'OK']\n",
    "\n",
    "\n",
    "df_out = pd.merge(df_melted_filtered, df_SHOPP_raw_data[['AMT_ID','Section', \n",
    "            'District',\n",
    "            'County',\n",
    "            'EA','EFIS',\n",
    "            'Planning or Post-Planning',\n",
    "            'Advertised Year',\n",
    "            'AM Tool RTL (Section in Use)', \n",
    "            'Project Cost ($K)', \n",
    "            'Will this project be included in the Project Book?',\n",
    "            'Data_TimeStamp',                                           \n",
    "            ]],\n",
    "        how = 'left', left_on = 'AMT_ID', right_on = 'AMT_ID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-reasoning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "exterior-catering",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing table: 430it [00:00, 13029.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signing into AssetManagement at https://tableau.dot.ca.gov\n",
      "Publishing projectbook_datachecks_punchlist.hyper to Sandbox_ProjectBookCheck_Automation...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#combine check tables\n",
    "df_out =  pd.concat([df_out, project_missing_AMT_ID], axis = 0)\n",
    "\n",
    "## Export to csv\n",
    "filename = 'projectbook_datachecks_punchlist'\n",
    "\n",
    "uf.export_csv(df_out, filename, PROJECTBOOKCHECK_HTTPSERVER_FOLDER, LOG_FILE)\n",
    "\n",
    "#table 1\n",
    "\n",
    "uf.export_hyper(df_out, filename,  LOG_FILE)\n",
    "\n",
    "\n",
    "#table 1a\n",
    "filename = 'projectlist_tobecreatedinAMT'\n",
    "\n",
    "uf.export_csv(project_missing_AMT_ID, filename, PROJECTBOOKCHECK_HTTPSERVER_FOLDER, LOG_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-provincial",
   "metadata": {},
   "source": [
    "<a id='Export_internal_check_summary'></a>\n",
    "\n",
    "### export internal check summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "solar-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export internal check summary\n",
    "out_cols = ['AMT_ID','Section', \n",
    "            'District',\n",
    "            'County',\n",
    "            'EA_',\n",
    "            \n",
    "            #internal check columns\n",
    "            'Does amendment date needs to be removed?',\n",
    "            'Need to add resource in the PID workplan?',\n",
    "            'PRG section needs amendment date?',\n",
    "            'PPC section needs amendment date?',\n",
    "\n",
    "            'Data_TimeStamp',\n",
    "                    ]\n",
    "\n",
    "filename = 'HQ_TAM_actionitems'\n",
    "\n",
    "uf.export_csv(df_SHOPP_raw_data[out_cols], filename, PROJECTBOOKCHECK_OUTPUT_FOLDER, LOG_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "north-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_col = 'New SHOPP Candidate?'\n",
    "\n",
    "df_SHOPP_raw_data[ck_col] = df_SHOPP_raw_data['SHOPP Amendment Date'].apply(\n",
    "    lambda x: 'Yes' if x == DD_Approval_Placeholder_Date else 'No'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "sorted-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['SHOPP Amendment Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "civilian-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-freeze",
   "metadata": {},
   "source": [
    "<a id='Export_ProjectBook'></a>\n",
    "\n",
    "# Export Project Book\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "metropolitan-springer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing table: 1971it [00:00, 11261.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signing into AssetManagement at https://tableau.dot.ca.gov\n",
      "Publishing projectbook_draft.hyper to Sandbox_ProjectBookCheck_Automation...\n"
     ]
    }
   ],
   "source": [
    "out_cols = [\n",
    "            'AMT_ID',\n",
    "            'District',\n",
    "            'County Name',\n",
    "            'Route',\n",
    "            'Begin Postmile',\n",
    "            'End Postmile',\n",
    "\n",
    "            'Activity',\n",
    "            'Planning or Post-Planning',\n",
    "            'Advertised Year',\n",
    "            'Total Project Cost ($K)',\n",
    "            'SB-1 Priority',\n",
    "            'EFIS',\n",
    "            'EA_',\n",
    "            'MPO/RTPA',\n",
    "            'Section',\n",
    "            'Active Long Lead',\n",
    "\n",
    "            'Nickname',\n",
    "            'New SHOPP Candidate?',\n",
    "            'Data_TimeStamp',\n",
    "           ]\n",
    "\n",
    "df_projectbook_filtered = df_SHOPP_raw_data[df_SHOPP_raw_data['Will this project be included in the Project Book?']=='Yes'][out_cols]\n",
    "\n",
    "\n",
    "filename = 'projectbook_draft'\n",
    "\n",
    "uf.export_csv(df_projectbook_filtered, filename, PROJECTBOOKCHECK_HTTPSERVER_FOLDER, LOG_FILE)\n",
    "\n",
    "uf.export_hyper(df_projectbook_filtered, filename, LOG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "aboriginal-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in  df_SHOPP_raw_data.columns:\n",
    "#     print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "furnished-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_SHOPP_raw_data['PID Status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-bryan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-family",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "native-salmon",
   "metadata": {},
   "source": [
    "\n",
    "## Export Project Book With Detailed Info For Internal Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "joint-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_projectbook_filtered = df_SHOPP_raw_data[df_SHOPP_raw_data['Will this project be included in the Project Book?']=='Yes'][out_cols]\n",
    "\n",
    "temp = df_pm_check[df_pm_check['Location']=='Primary'][[\n",
    "    'AMT_ID','Section','BackPMLatitude', 'BackPMLongitude',\n",
    "       'BackPMAssemblyDistrict', 'BackPMCongressDistrict',\n",
    "       'BackPMSenateDistrict']]\n",
    "\n",
    "df_projectbook_internal = pd.merge(df_projectbook_filtered, temp, \n",
    "                                   how='left', left_on = ['AMT_ID','Section'], right_on = ['AMT_ID','Section'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "rocky-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_projectbook_filtered['AMT_ID'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "furnished-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_activity(df):\n",
    "    group_dict = {\n",
    "        'Bridge':'Bridge',\n",
    "        'Bridge - Deck':'Bridge',\n",
    "        'Bridge - Health':'Bridge',\n",
    "        'Proactive Safety':'Proactive Safety',\n",
    "        'Safety - Collision Reduction':'Proactive Safety',\n",
    "        'Reactive Safety':'Reactive Safety',\n",
    "        'Safety - Monitoring':'Reactive Safety',\n",
    "        'Safety - SI':'Reactive Safety',\n",
    "        'Sustainability/Climate Change':'Sustainability',\n",
    "        'Facilities':'Facilities',\n",
    "        'Facilities - Office Buildings':'Facilities',\n",
    "        'Advance Mitigation/Mitigation':'Advance Mitigation',        \n",
    "    }\n",
    "    if df['Activity'] in group_dict.keys():\n",
    "        return group_dict[df['Activity']]\n",
    "    else:\n",
    "        return df['Activity']\n",
    "df_projectbook_internal['Activity_InternalProjectBook'] = df_projectbook_internal.apply(group_activity, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "cloudy-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_cols = ['AMT_ID', 'District', 'County Name', 'Route',\n",
    "       'Begin Postmile', 'End Postmile', 'Activity_InternalProjectBook',\n",
    "       'Planning or Post-Planning', 'Advertised Year',\n",
    "       'Total Project Cost ($K)', 'SB-1 Priority', 'EFIS', 'EA_', 'MPO/RTPA',\n",
    "       'Section', 'Active Long Lead', 'Nickname', 'BackPMLatitude',\n",
    "       'BackPMLongitude', 'BackPMAssemblyDistrict', 'BackPMCongressDistrict',\n",
    "       'BackPMSenateDistrict','Data_TimeStamp',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "lightweight-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_projectbook_internal[out_cols]\n",
    "filename = 'projectbook_internal'\n",
    "uf.export_csv(df_out, filename, PROJECTBOOKCHECK_OUTPUT_FOLDER, LOG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-arctic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "increasing-grill",
   "metadata": {},
   "source": [
    "<a id='Export_repated_EA_EFIS'></a>\n",
    "\n",
    "### Export repeated EA and EFIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "iraqi-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "duplicate_EA['Data_TimeStamp'] = Data_TimeStamp\n",
    "\n",
    "temp1 = pd.merge(duplicate_EA, df_SHOPP_HM_MB[['Unique EA','District']].drop_duplicates(), how = 'left', left_on = 'Unique EA', right_on = 'Unique EA')\n",
    "temp1 = temp1.sort_values(by = ['District','Unique EA'])[['District','Unique EA','AMT_ID List_repeated Unique EA']]\n",
    "# temp1.shape\n",
    "\n",
    "df_out = temp1\n",
    "filename = 'repeated_EA'\n",
    "\n",
    "uf.export_csv(df_out, filename, PROJECTBOOKCHECK_OUTPUT_FOLDER, LOG_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "freelance-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = pd.merge(duplicate_EFIS, df_SHOPP_HM_MB[['EFIS','District']].drop_duplicates(), how = 'left', left_on = 'EFIS', right_on = 'EFIS')\n",
    "temp1 = temp1.sort_values(by = ['District','EFIS'])[['District','EFIS','AMT_ID List_repeated EFIS']]\n",
    "\n",
    "\n",
    "# duplicate_EFIS.to_csv('.\\output\\info_p3t2_duplicate_EFIS.csv')\n",
    "\n",
    "df_out = temp1\n",
    "filename = 'repeated_EFIS'\n",
    "uf.export_csv(df_out, filename, PROJECTBOOKCHECK_OUTPUT_FOLDER, LOG_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-surveillance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "balanced-republican",
   "metadata": {},
   "source": [
    "<a id='Export_repeated_assets'></a>\n",
    "\n",
    "### Export repeated assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "competitive-siemens",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing table: 46it [00:00, 15339.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signing into AssetManagement at https://tableau.dot.ca.gov\n",
      "Publishing repeated_assets.hyper to Sandbox_ProjectBookCheck_Automation...\n"
     ]
    }
   ],
   "source": [
    "df_duplicated_asset['Data_TimeStamp'] = Data_TimeStamp\n",
    "\n",
    "out_col =['Asset_Type','Asset_ID', 'AMT_ID', 'Section', 'District', 'EA',\n",
    "       'Last Year of Fiscal Year', 'Type of Exception','Data_TimeStamp']\n",
    "\n",
    "filename = 'repeatedassets_inprojectbook'\n",
    "df_out = df_duplicated_asset[out_col]\n",
    "\n",
    "uf.export_csv(df_out, filename, PROJECTBOOKCHECK_HTTPSERVER_FOLDER, LOG_FILE)\n",
    "\n",
    "\n",
    "hyper_name = 'repeated_assets'\n",
    "uf.export_hyper(df_out, hyper_name, LOG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-patrick",
   "metadata": {},
   "source": [
    "<a id='Export_KeyDates'></a>\n",
    "\n",
    "### Export Key Dates for Project Book Check Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "painful-overview",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing table: 1it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signing into AssetManagement at https://tableau.dot.ca.gov\n",
      "Publishing KeyDates.hyper to Sandbox_ProjectBookCheck_Automation...\n"
     ]
    }
   ],
   "source": [
    "df_out = df_GlobalParameters[['PID Workplan Data Date', 'QMRS Data Date', 'CTC Meeting Date',]]\n",
    "\n",
    "hyper_name = 'KeyDates'\n",
    "uf.export_hyper(df_out, hyper_name, LOG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-check",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-mistress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-contents",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-vatican",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "resistant-isaac",
   "metadata": {},
   "source": [
    "\n",
    "<a id='FinalCleanUp'></a>\n",
    "## Final Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "understanding-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up tableau publishing log file\n",
    "\n",
    "import os\n",
    "import glob\n",
    "# get a recursive list of file paths that matches pattern\n",
    "fileList = glob.glob('./*.log')\n",
    "# Iterate over the list of filepaths & remove each file.\n",
    "for filePath in fileList:\n",
    "    try:\n",
    "        os.remove(filePath)\n",
    "    except OSError:\n",
    "        print(\"Error while deleting file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "durable-sponsorship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed : 71.35344529151917 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time =  time.time()\n",
    "elapsed = end_time - start_time\n",
    "print('time elapsed : {} seconds'.format(elapsed))\n",
    "\n",
    "file_export_log = open(LOG_FILE, \"a\")  # append mode\n",
    "file_export_log.write('#####time elapsed : {} seconds \\n'.format(elapsed))\n",
    "file_export_log.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
